{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch_geometric.data import Data\n",
    "import numpy as np\n",
    "from dateutil.parser import parse\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pittsburgh Steelers' 'San Francisco 49ers' 'Denver Broncos'\n",
      " 'Las Vegas Raiders' 'Washington Commanders' 'Los Angeles Chargers'\n",
      " 'Seattle Seahawks' 'Cincinnati Bengals' 'Cleveland Browns'\n",
      " 'Detroit Lions' 'Houston Texans' 'Jacksonville Jaguars' 'Miami Dolphins'\n",
      " 'New England Patriots' 'Tampa Bay Buccaneers' 'Los Angeles Rams'\n",
      " 'Arizona Cardinals' 'Atlanta Falcons' 'Buffalo Bills' 'Green Bay Packers'\n",
      " 'Indianapolis Colts' 'Minnesota Vikings' 'New York Giants'\n",
      " 'New Orleans Saints' 'New York Jets' 'Carolina Panthers' 'Dallas Cowboys'\n",
      " 'Baltimore Ravens' 'Philadelphia Eagles' 'Tennessee Titans'\n",
      " 'Chicago Bears' 'Kansas City Chiefs']\n",
      "Data(edge_index=[2, 246], edge_attr=[246, 2], num_nodes=32)\n"
     ]
    }
   ],
   "source": [
    "# process data\n",
    "# each node is a team (for a year), edges\n",
    "game_info = pd.read_csv(\"nfl-game-info.csv\")\n",
    "# team_info = pd.read_csv(\"team_stats_2003_2023.csv\")\n",
    "\n",
    "teams = pd.concat([game_info['Home Team'], game_info['Away Team']]).unique()\n",
    "teams = list(teams)\n",
    "teams.remove('San Diego Chargers')\n",
    "teams.remove('Washington Redskins')\n",
    "teams.remove('St. Louis Rams')\n",
    "teams.remove('Oakland Raiders')\n",
    "teams.remove('Washington Football Team')\n",
    "teams = np.array(teams)\n",
    "print(teams)\n",
    "team_to_idx = {team: i for i, team in enumerate(teams)}\n",
    "\n",
    "edge_index = []\n",
    "edge_attr = []\n",
    "\n",
    "for _, row in game_info.iterrows():\n",
    "  date = parse(row[\"Date\"])\n",
    "  if date < parse('2024-9-5'):\n",
    "    continue\n",
    "  # create an edge between two teams,\n",
    "  team1_idx = team_to_idx[row['Home Team']]\n",
    "  team2_idx = team_to_idx[row['Away Team']]\n",
    "\n",
    "  # Add an undirected edge (team1 -> team2 and team2 -> team1)\n",
    "  edge_index.append([team1_idx, team2_idx])\n",
    "  edge_index.append([team2_idx, team1_idx])\n",
    "\n",
    "  edge_attr.append([row[\"Home Score\"], row[\"Away Score\"]])\n",
    "  edge_attr.append([row[\"Away Score\"], row[\"Home Score\"]])\n",
    "\n",
    "edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "\n",
    "data = Data(edge_index=edge_index, edge_attr=edge_attr, num_nodes=len(teams))\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.utils import to_networkx\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAT Embeddings:\n",
      "tensor([[-7.7690e+00, -1.1343e+01,  2.5720e+01, -1.2655e+01,  5.5458e+00,\n",
      "          1.5980e+01, -1.2176e+00,  2.0703e+00, -3.0421e+00, -1.0410e+01,\n",
      "          5.9335e+00,  2.1580e+01,  5.1752e+00,  6.5979e+00,  2.1606e+01,\n",
      "          3.8228e+00],\n",
      "        [ 3.4300e-01, -1.6680e+00, -7.7003e-01, -6.0018e-01,  4.1553e-01,\n",
      "         -3.0598e-01,  6.4938e-01, -1.3300e-01, -6.2258e-01, -4.0410e-01,\n",
      "         -1.2769e+00, -8.5998e-02,  4.1683e-01,  2.0594e-01,  8.4491e-02,\n",
      "          5.6659e-01],\n",
      "        [-6.4114e+00,  7.3263e+00, -1.2643e+01, -9.3496e+00,  2.5017e+01,\n",
      "         -1.6920e+01,  1.3468e+01, -3.3060e+00, -1.6164e+00, -1.0911e+00,\n",
      "         -3.1940e+00,  5.5168e+00, -1.3516e+00, -4.1200e+00,  3.7319e+00,\n",
      "          4.6974e+00],\n",
      "        [-8.3395e-02, -3.1126e-01,  3.0398e-01,  1.7405e-01,  2.3598e-01,\n",
      "         -1.1304e-02, -2.4101e-01, -4.8060e-03, -3.7143e-01, -1.5961e-01,\n",
      "          4.8529e-02,  2.7109e-03, -1.1525e-01,  2.1554e-01, -2.1282e-01,\n",
      "          2.5780e-01],\n",
      "        [-1.9342e-03, -2.6245e-03,  1.9919e-03,  1.2377e-03,  1.1686e-03,\n",
      "          4.6952e-04, -5.6047e-04,  6.9299e-05, -2.3758e-05, -2.3126e-03,\n",
      "          4.1353e-04,  1.2493e-03, -1.5059e-03, -4.0494e-04, -2.2484e-03,\n",
      "          2.0787e-03],\n",
      "        [-1.9217e+00,  2.0813e+00, -6.8724e+00, -5.5364e+00, -1.6019e+01,\n",
      "          1.3236e+01, -1.1885e+01, -1.4986e+01,  3.2414e+01, -1.6600e+01,\n",
      "          5.5288e+00,  1.3458e+01, -1.9514e+00,  5.1989e+00,  1.7096e+01,\n",
      "         -7.6621e+00],\n",
      "        [-7.6535e+00, -1.0923e+01,  2.4756e+01, -1.2588e+01,  5.9272e+00,\n",
      "          1.5148e+01, -8.5183e-01,  1.9392e+00, -3.0053e+00, -1.0142e+01,\n",
      "          5.6746e+00,  2.1061e+01,  4.9372e+00,  6.2669e+00,  2.1007e+01,\n",
      "          3.8806e+00],\n",
      "        [-4.7664e+00, -4.0307e+00,  2.7323e+00, -2.7946e+00,  1.9050e+00,\n",
      "         -1.2044e+00, -2.9215e+00, -2.6189e+00, -7.1054e+00, -4.7693e+00,\n",
      "          2.2944e+00,  5.6091e+00, -2.3445e-01,  8.3665e-01,  2.2825e+00,\n",
      "         -9.3704e-01],\n",
      "        [-2.0465e-02, -1.6898e-01,  4.6884e-03, -2.1835e-01, -1.4134e-02,\n",
      "          2.3937e-01, -1.9268e-01,  1.6381e-02, -8.7349e-02, -1.9443e-02,\n",
      "          1.8382e-01,  1.7453e-02,  9.7454e-02, -1.4658e-01, -1.2921e-01,\n",
      "         -9.5216e-02],\n",
      "        [ 8.7373e+00, -3.5057e+00,  1.8473e+00, -6.7639e-01, -6.7541e+00,\n",
      "          5.3190e+00, -1.1404e+01,  2.7284e+00,  1.7338e+00, -3.6339e+00,\n",
      "          5.8818e-01, -4.0760e+00,  1.4487e+00,  3.6801e+00,  5.9073e+00,\n",
      "         -2.8687e+00],\n",
      "        [-1.4128e-06,  1.8756e-06,  7.8359e-06,  7.1943e-06,  4.7343e-06,\n",
      "          1.0487e-05,  1.1927e-05,  3.0498e-06,  8.2721e-06,  5.1546e-06,\n",
      "         -2.4324e-06,  7.3483e-06, -4.5070e-07,  3.8348e-06, -1.7414e-06,\n",
      "          1.0731e-05],\n",
      "        [-6.0197e-04, -1.9696e-03,  1.9888e-03,  1.0853e-03,  1.5873e-03,\n",
      "         -1.6432e-04, -1.7716e-03, -1.2581e-04, -2.6398e-03, -1.2052e-03,\n",
      "          6.6964e-04, -2.1616e-04, -6.8257e-04,  1.6227e-03, -1.5671e-03,\n",
      "          1.6121e-03],\n",
      "        [-8.5078e+00,  5.4552e+00, -1.1497e+01, -1.8277e+01,  3.3508e+01,\n",
      "         -2.0858e+01,  2.3576e+01, -4.0414e+00, -3.6027e+00, -1.3596e+00,\n",
      "         -6.7715e+00,  1.3244e+01, -4.8807e+00, -1.0462e+01,  4.1599e+00,\n",
      "          1.0952e+01],\n",
      "        [-2.0919e+00,  1.1579e+00, -2.6749e+00, -4.0198e+00,  9.1023e+00,\n",
      "         -5.8605e+00,  6.1434e+00, -1.0478e+00, -8.6244e-01, -3.2879e-01,\n",
      "         -1.3266e+00,  3.5132e+00, -5.8555e-01, -2.1606e+00,  7.0748e-01,\n",
      "          2.4020e+00],\n",
      "        [-4.8903e+00, -4.2716e+00,  6.7877e+00, -8.5739e+00, -7.2420e-01,\n",
      "          6.1117e-01, -3.0097e+00, -3.3118e+00, -9.3574e+00, -5.9375e+00,\n",
      "          1.6654e+00,  1.2633e+01,  3.2495e+00,  3.7211e+00,  1.1547e+01,\n",
      "         -2.3574e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-3.7149e-01,  3.8145e+00, -4.5554e+00,  2.0103e+00,  2.2539e+00,\n",
      "         -3.1317e+00, -1.3212e+00, -5.3889e-02,  4.5411e-01, -2.7530e-01,\n",
      "          1.9784e-01, -3.3469e+00,  3.8667e-01,  2.1715e+00,  5.5798e-01,\n",
      "         -2.1681e+00],\n",
      "        [-5.1901e+00, -3.2642e+00,  3.9874e+00, -9.8310e+00, -6.1010e+00,\n",
      "          5.0305e+00, -6.8321e+00, -8.1527e+00,  2.2404e+00, -1.1142e+01,\n",
      "          3.4321e+00,  1.6277e+01,  2.3690e+00,  5.2484e+00,  1.6516e+01,\n",
      "         -4.7877e+00],\n",
      "        [-7.7769e+00,  4.9787e+00, -1.0505e+01, -1.6682e+01,  3.0585e+01,\n",
      "         -1.9053e+01,  2.1516e+01, -3.6893e+00, -3.2714e+00, -1.2480e+00,\n",
      "         -6.1806e+00,  1.2090e+01, -4.4558e+00, -9.5546e+00,  3.8138e+00,\n",
      "          9.9889e+00],\n",
      "        [-6.1168e+00, -2.0224e+00, -2.2762e+00, -1.1439e+01, -5.7772e+00,\n",
      "         -1.0048e+01, -6.3277e+00, -9.4257e+00, -2.0254e+01, -6.7102e+00,\n",
      "         -3.0686e-01,  1.4779e+01,  4.0600e+00,  4.1585e+00,  1.2137e+01,\n",
      "         -8.4160e+00],\n",
      "        [ 1.7924e-01, -2.8996e-01,  1.1403e+00,  5.3557e-01,  4.3234e-01,\n",
      "          3.4872e-01, -3.3030e-02,  2.0113e+00, -2.5325e-01,  3.7023e-01,\n",
      "         -5.5212e-01, -1.0196e+00, -6.7438e-01,  7.0172e-01, -5.7767e-01,\n",
      "          6.7384e-01],\n",
      "        [ 5.3086e-02, -1.5843e-01,  1.7128e-01,  3.9703e-01,  3.8279e-01,\n",
      "         -3.5292e-01,  6.6105e-02,  2.6226e-02,  2.5486e-02,  1.4364e-02,\n",
      "          2.5140e-01,  6.8849e-02,  4.2911e-01,  3.3461e-01, -2.4049e-01,\n",
      "         -2.5922e-01],\n",
      "        [-3.6027e-02, -5.9238e-02, -6.3951e-02,  4.8253e-02,  1.7774e-03,\n",
      "          3.2161e-02, -2.6379e-02,  7.5314e-04,  5.6029e-02,  7.4675e-03,\n",
      "          1.6233e-02,  1.2431e-02, -2.4530e-02, -2.0781e-02, -5.2676e-02,\n",
      "          4.9321e-02],\n",
      "        [ 4.1856e-03, -1.3106e-03,  1.8145e-05, -9.0035e-04,  3.4429e-03,\n",
      "         -9.7972e-04, -4.9039e-04, -2.9595e-03,  4.8163e-03, -2.3199e-05,\n",
      "         -1.6952e-03, -3.0266e-03, -7.5004e-05, -4.4649e-04,  5.8974e-04,\n",
      "          2.3201e-03],\n",
      "        [ 1.2207e-03, -1.0025e-03,  3.0193e-03,  4.0007e-03,  3.5361e-04,\n",
      "          6.0235e-03,  4.0041e-03, -5.6712e-04, -1.4994e-03, -7.2170e-04,\n",
      "         -9.7149e-04,  3.1060e-04,  1.2880e-03,  7.5767e-04, -4.1032e-03,\n",
      "          3.1251e-03],\n",
      "        [-2.7462e-04, -1.4495e-04, -1.4463e-03,  5.3015e-04,  2.5976e-04,\n",
      "          5.5315e-04, -3.9067e-04,  6.4030e-04,  2.9440e-04,  5.3827e-04,\n",
      "          5.2611e-04,  3.2547e-04, -1.8769e-04, -9.3924e-05, -4.7922e-04,\n",
      "         -2.2793e-04],\n",
      "        [-1.9795e-01,  1.8487e-01,  1.5754e-01,  1.2390e-01, -5.4008e-02,\n",
      "          8.2411e-02,  3.2482e-02,  2.4429e-01, -7.5520e-02,  7.6364e-02,\n",
      "          1.7155e-02, -1.2023e-01, -1.2567e-01,  9.4668e-02,  1.9225e-01,\n",
      "          1.5553e-01],\n",
      "        [-2.0982e+00,  9.4388e-01, -5.8126e+00, -5.3640e+00, -1.3760e+01,\n",
      "          1.2873e+01, -1.0458e+01, -1.2915e+01,  2.8629e+01, -1.4411e+01,\n",
      "          5.1369e+00,  1.2553e+01, -1.6789e+00,  3.7904e+00,  1.4870e+01,\n",
      "         -6.6947e+00],\n",
      "        [-6.8413e+00, -5.9746e+00,  9.5029e+00, -1.2000e+01, -1.0168e+00,\n",
      "          8.5342e-01, -4.2095e+00, -4.6359e+00, -1.3096e+01, -8.3098e+00,\n",
      "          2.3271e+00,  1.7680e+01,  4.5478e+00,  5.2074e+00,  1.6162e+01,\n",
      "         -3.2976e+00],\n",
      "        [ 1.0845e-04,  1.5102e-04,  5.8369e-05, -1.7703e-04, -1.3449e-04,\n",
      "          1.6398e-04, -1.9607e-04,  1.2859e-04,  2.5968e-05, -1.3922e-06,\n",
      "          7.7746e-05, -4.3615e-05, -1.4248e-05,  1.7763e-04,  1.7234e-04,\n",
      "         -1.4778e-05],\n",
      "        [ 4.9617e-02, -1.0900e-01, -1.9687e-02,  5.9173e-02, -8.4248e-03,\n",
      "          2.8707e-02, -9.2281e-02,  3.8032e-03,  2.3851e-02, -3.4378e-02,\n",
      "          8.5586e-03, -4.4050e-02,  1.7032e-02, -1.1159e-02, -5.4994e-04,\n",
      "         -3.9649e-02],\n",
      "        [ 5.5880e-01, -4.5003e+00, -4.7156e+00,  3.9856e-01,  2.2002e+00,\n",
      "         -1.7383e+00,  1.1300e+00, -1.0989e+00, -1.4959e+00, -2.6560e+00,\n",
      "         -4.0428e+00, -2.9184e+00,  2.4089e+00,  3.2987e+00,  2.1147e+00,\n",
      "          9.9454e-01]])\n"
     ]
    }
   ],
   "source": [
    "# Random node features with a fixed dimensionality, e.g., 16\n",
    "num_nodes = len(teams)\n",
    "node_feature_dim = 64\n",
    "node_features = torch.randn((num_nodes, node_feature_dim), dtype=torch.float,)\n",
    "\n",
    "# class GATModel(torch.nn.Module):\n",
    "#     def __init__(self, in_channels, hidden_channels, out_channels, num_layers=2, heads=8, dropout=0.6):\n",
    "#         super(GATModel, self).__init__()\n",
    "#         self.dropout = dropout\n",
    "\n",
    "#         # First GAT layer\n",
    "#         self.convs = torch.nn.ModuleList()\n",
    "#         self.convs.append(GATConv(in_channels, hidden_channels, heads=heads, concat=True, dropout=dropout))\n",
    "\n",
    "#         # Intermediate GAT layers\n",
    "#         for _ in range(num_layers - 2):\n",
    "#             self.convs.append(GATConv(hidden_channels * heads, hidden_channels, heads=heads, concat=True, dropout=dropout))\n",
    "\n",
    "#         # Final GAT layer\n",
    "#         self.convs.append(GATConv(hidden_channels * heads, out_channels, heads=1, concat=False, dropout=dropout))\n",
    "\n",
    "#         # Optional batch normalization\n",
    "#         self.bns = torch.nn.ModuleList([torch.nn.BatchNorm1d(hidden_channels * heads) for _ in range(num_layers - 1)])\n",
    "\n",
    "#     def forward(self, x, edge_index):\n",
    "#         for i, conv in enumerate(self.convs[:-1]):\n",
    "#             x = conv(x, edge_index)\n",
    "#             x = self.bns[i](x)  # Apply batch normalization\n",
    "#             x = F.elu(x)        # Apply activation\n",
    "#             x = F.dropout(x, p=self.dropout, training=self.training)  # Apply dropout\n",
    "#         x = self.convs[-1](x, edge_index)  # Last layer (no activation)\n",
    "#         return x\n",
    "\n",
    "#     def encode(self, x, edge_index):\n",
    "#         return self.forward(x, edge_index)\n",
    "\n",
    "#     def decode(self, z, edge_index):\n",
    "#         # Use dot product to predict if an edge exists\n",
    "#         src, dst = edge_index\n",
    "#         return (z[src] * z[dst]).sum(dim=1)\n",
    "\n",
    "#     def decode_all(self, z):\n",
    "#         # Decode all possible edges in a fully connected graph\n",
    "#         prob_adj = torch.matmul(z, z.t())\n",
    "#         return prob_adj\n",
    "\n",
    "# # RESIDUALS!\n",
    "class GATModel(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=3, heads=8, dropout=0.6):\n",
    "        super(GATModel, self).__init__()\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Define GAT layers\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(GATConv(in_channels, hidden_channels, heads=heads, concat=True, dropout=dropout))\n",
    "\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(GATConv(hidden_channels * heads, hidden_channels, heads=heads, concat=True, dropout=dropout))\n",
    "\n",
    "        self.convs.append(GATConv(hidden_channels * heads, out_channels, heads=1, concat=False, dropout=dropout))\n",
    "\n",
    "        # Optional batch normalization\n",
    "        self.bns = torch.nn.ModuleList([\n",
    "            torch.nn.BatchNorm1d(hidden_channels * heads) for _ in range(num_layers - 1)\n",
    "        ])\n",
    "\n",
    "        # Linear projection for residual connections (if dimensions don't match)\n",
    "        self.residual_proj = torch.nn.ModuleList([\n",
    "            torch.nn.Linear(in_channels if i == 0 else hidden_channels * heads, hidden_channels * heads) \n",
    "            for i in range(num_layers - 1)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for i, conv in enumerate(self.convs[:-1]):\n",
    "            residual = x\n",
    "            x = conv(x, edge_index)\n",
    "            x = self.bns[i](x)  # Batch normalization\n",
    "            x = F.elu(x)        # Activation\n",
    "\n",
    "            # Project residual if dimensions don't match\n",
    "            if residual.size(-1) != x.size(-1):\n",
    "                residual = self.residual_proj[i](residual)\n",
    "\n",
    "            x = x + residual    # Add residual connection\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)  # Dropout\n",
    "\n",
    "        # Final GAT layer (no residual connection, as it's the output layer)\n",
    "        x = self.convs[-1](x, edge_index)\n",
    "        return x\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        return self.forward(x, edge_index)\n",
    "\n",
    "    def decode(self, z, edge_index):\n",
    "        # Use dot product to predict if an edge exists\n",
    "        src, dst = edge_index\n",
    "        return (z[src] * z[dst]).sum(dim=1)\n",
    "\n",
    "# Initialize the model\n",
    "model = GATModel(in_channels=node_feature_dim, hidden_channels=32, out_channels=16, num_layers=4)\n",
    "\n",
    "# Move model and data to the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "data = data.to(device)\n",
    "node_features = node_features.to(device)\n",
    "\n",
    "# Get GAT embeddings in a single forward pass\n",
    "with torch.no_grad():\n",
    "    embeddings = model(node_features, data.edge_index)\n",
    "\n",
    "print(\"GAT Embeddings:\")\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Assuming `embeddings` is a tensor of shape [num_nodes, embedding_dim]\n",
    "embeddings_np = embeddings.cpu().numpy()  # Convert to NumPy for compatibility with sklearn\n",
    "\n",
    "# Choose either PCA or t-SNE for dimensionality reduction\n",
    "# Option 1: PCA\n",
    "# pca = PCA(n_components=2)\n",
    "# reduced_embeddings = pca.fit_transform(embeddings_np)\n",
    "\n",
    "# Option 2: t-SNE (for potentially better separation but slower)\n",
    "tsne = TSNE(n_components=3, random_state=0)\n",
    "reduced_embeddings = tsne.fit_transform(embeddings_np)\n",
    "\n",
    "# Plot the reduced embeddings\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], s=100, alpha=0.7)\n",
    "plt.title(\"GAT Embeddings Visualization\")\n",
    "plt.xlabel(\"Component 1\")\n",
    "plt.ylabel(\"Component 2\")\n",
    "\n",
    "# Optional: Annotate points with team names or IDs\n",
    "for i, team in enumerate(teams):\n",
    "    plt.annotate(team, (reduced_embeddings[i, 0], reduced_embeddings[i, 1]))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import negative_sampling, train_test_split_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "in_channels = node_features.size(1)  # Assuming you use random features as before\n",
    "hidden_channels = 32\n",
    "out_channels = 16\n",
    "model = GATModel(in_channels=in_channels, hidden_channels=hidden_channels, out_channels=out_channels, num_layers=4).to(device)\n",
    "\n",
    "# Temporarily remove edge attributes to allow edge splitting\n",
    "data.edge_attr = None  # Remove edge_attr to use train_test_split_edges\n",
    "\n",
    "# Perform train/test split on edges\n",
    "# data = train_test_split_edges(data)\n",
    "\n",
    "# You can still access `edge_attr` separately if needed\n",
    "edge_attr = data.edge_attr  # Store edge_attr separately if you need it later\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000, Training Loss: 91.70856475830078, Validation Loss: 58.52754592895508\n",
      "Epoch 2/10000, Training Loss: 83.57965850830078, Validation Loss: 109.6576156616211\n",
      "Epoch 3/10000, Training Loss: 63.05228042602539, Validation Loss: 7.598507404327393\n",
      "Epoch 4/10000, Training Loss: 24.561994552612305, Validation Loss: 5.03757381439209\n",
      "Epoch 5/10000, Training Loss: 132.26913452148438, Validation Loss: 702.1585083007812\n",
      "Epoch 6/10000, Training Loss: 53.87751770019531, Validation Loss: 532.1793823242188\n",
      "Epoch 7/10000, Training Loss: 88.12779998779297, Validation Loss: 40.276859283447266\n",
      "Epoch 8/10000, Training Loss: 96.78541564941406, Validation Loss: 71.79641723632812\n",
      "Epoch 9/10000, Training Loss: 179.56214904785156, Validation Loss: 34.05586242675781\n",
      "Epoch 10/10000, Training Loss: 94.84387969970703, Validation Loss: 4.027176380157471\n",
      "Epoch 11/10000, Training Loss: 51.3951416015625, Validation Loss: 10.535810470581055\n",
      "Epoch 12/10000, Training Loss: 21.955108642578125, Validation Loss: 33.579071044921875\n",
      "Epoch 13/10000, Training Loss: 76.09001159667969, Validation Loss: 228.96372985839844\n",
      "Epoch 14/10000, Training Loss: 124.72885131835938, Validation Loss: 292.57574462890625\n",
      "Epoch 15/10000, Training Loss: 96.38347625732422, Validation Loss: 102.53887176513672\n",
      "Epoch 16/10000, Training Loss: 79.07296752929688, Validation Loss: 35.52547836303711\n",
      "Epoch 17/10000, Training Loss: 67.33039855957031, Validation Loss: 289.2603454589844\n",
      "Epoch 18/10000, Training Loss: 92.72358703613281, Validation Loss: 25.737741470336914\n",
      "Epoch 19/10000, Training Loss: 71.91043853759766, Validation Loss: 25.371536254882812\n",
      "Epoch 20/10000, Training Loss: 26.363990783691406, Validation Loss: 19.606962203979492\n",
      "Epoch 21/10000, Training Loss: 138.2655792236328, Validation Loss: 143.8294219970703\n",
      "Epoch 22/10000, Training Loss: 143.58340454101562, Validation Loss: 709.040771484375\n",
      "Epoch 23/10000, Training Loss: 129.19537353515625, Validation Loss: 331.5435485839844\n",
      "Epoch 24/10000, Training Loss: 56.762447357177734, Validation Loss: 174.715576171875\n",
      "Epoch 25/10000, Training Loss: 107.16507720947266, Validation Loss: 91.84151458740234\n",
      "Epoch 26/10000, Training Loss: 54.587989807128906, Validation Loss: 12.604592323303223\n",
      "Epoch 27/10000, Training Loss: 52.65720748901367, Validation Loss: 0.49729910492897034\n",
      "Epoch 28/10000, Training Loss: 17.740825653076172, Validation Loss: 0.8974959254264832\n",
      "Epoch 29/10000, Training Loss: 116.44387817382812, Validation Loss: 2.3130290508270264\n",
      "Epoch 30/10000, Training Loss: 176.79412841796875, Validation Loss: 76.93440246582031\n",
      "Epoch 31/10000, Training Loss: 66.00886535644531, Validation Loss: 16.16702651977539\n",
      "Epoch 32/10000, Training Loss: 130.30526733398438, Validation Loss: 48.09804153442383\n",
      "Epoch 33/10000, Training Loss: 121.04252624511719, Validation Loss: 48.45451354980469\n",
      "Epoch 34/10000, Training Loss: 61.76332473754883, Validation Loss: 60.14735412597656\n",
      "Epoch 35/10000, Training Loss: 7.976128578186035, Validation Loss: 6.158636093139648\n",
      "Epoch 36/10000, Training Loss: 70.92424011230469, Validation Loss: 231.397705078125\n",
      "Epoch 37/10000, Training Loss: 58.64279556274414, Validation Loss: 82.83916473388672\n",
      "Epoch 38/10000, Training Loss: 74.68275451660156, Validation Loss: 29.851478576660156\n",
      "Epoch 39/10000, Training Loss: 72.42671203613281, Validation Loss: 2.2468583583831787\n",
      "Epoch 40/10000, Training Loss: 44.92982482910156, Validation Loss: 99.68451690673828\n",
      "Epoch 41/10000, Training Loss: 14.089466094970703, Validation Loss: 41.10175704956055\n",
      "Epoch 42/10000, Training Loss: 90.05606842041016, Validation Loss: 10.46505069732666\n",
      "Epoch 43/10000, Training Loss: 37.357688903808594, Validation Loss: 80.26445770263672\n",
      "Epoch 44/10000, Training Loss: 51.274051666259766, Validation Loss: 16.11002540588379\n",
      "Epoch 45/10000, Training Loss: 111.98865509033203, Validation Loss: 0.7669802308082581\n",
      "Epoch 46/10000, Training Loss: 56.74721145629883, Validation Loss: 148.31320190429688\n",
      "Epoch 47/10000, Training Loss: 66.19206237792969, Validation Loss: 103.72409057617188\n",
      "Epoch 48/10000, Training Loss: 138.21542358398438, Validation Loss: 13.640289306640625\n",
      "Epoch 49/10000, Training Loss: 43.78303527832031, Validation Loss: 454.9759826660156\n",
      "Epoch 50/10000, Training Loss: 71.52425384521484, Validation Loss: 57.09217834472656\n",
      "Epoch 51/10000, Training Loss: 40.817073822021484, Validation Loss: 8.045380592346191\n",
      "Epoch 52/10000, Training Loss: 43.79400634765625, Validation Loss: 318.6535949707031\n",
      "Epoch 53/10000, Training Loss: 68.0199966430664, Validation Loss: 39.669471740722656\n",
      "Epoch 54/10000, Training Loss: 62.53904724121094, Validation Loss: 41.14019775390625\n",
      "Epoch 55/10000, Training Loss: 55.04881286621094, Validation Loss: 4.214695930480957\n",
      "Epoch 56/10000, Training Loss: 35.45768737792969, Validation Loss: 20.72481918334961\n",
      "Epoch 57/10000, Training Loss: 33.135353088378906, Validation Loss: 57.7799186706543\n",
      "Epoch 58/10000, Training Loss: 129.47633361816406, Validation Loss: 0.7508647441864014\n",
      "Epoch 59/10000, Training Loss: 121.91580963134766, Validation Loss: 0.7598044872283936\n",
      "Epoch 60/10000, Training Loss: 25.033653259277344, Validation Loss: 86.7171630859375\n",
      "Epoch 61/10000, Training Loss: 31.169771194458008, Validation Loss: 31.35485076904297\n",
      "Epoch 62/10000, Training Loss: 25.707029342651367, Validation Loss: 21.375547409057617\n",
      "Epoch 63/10000, Training Loss: 81.44426727294922, Validation Loss: 335.51849365234375\n",
      "Epoch 64/10000, Training Loss: 167.95050048828125, Validation Loss: 52.41434860229492\n",
      "Epoch 65/10000, Training Loss: 14.847025871276855, Validation Loss: 23.565261840820312\n",
      "Epoch 66/10000, Training Loss: 77.91560363769531, Validation Loss: 31.548925399780273\n",
      "Epoch 67/10000, Training Loss: 39.162818908691406, Validation Loss: 40.72941589355469\n",
      "Epoch 68/10000, Training Loss: 151.15069580078125, Validation Loss: 77.43631744384766\n",
      "Epoch 69/10000, Training Loss: 29.793729782104492, Validation Loss: 9.659506797790527\n",
      "Epoch 70/10000, Training Loss: 35.28388977050781, Validation Loss: 51.299381256103516\n",
      "Epoch 71/10000, Training Loss: 28.561710357666016, Validation Loss: 9.877483367919922\n",
      "Epoch 72/10000, Training Loss: 44.963565826416016, Validation Loss: 52.55708694458008\n",
      "Epoch 73/10000, Training Loss: 106.25704193115234, Validation Loss: 41.750221252441406\n",
      "Epoch 74/10000, Training Loss: 22.497623443603516, Validation Loss: 28.63576316833496\n",
      "Epoch 75/10000, Training Loss: 58.513980865478516, Validation Loss: 130.3603057861328\n",
      "Epoch 76/10000, Training Loss: 84.92964935302734, Validation Loss: 1.6039594411849976\n",
      "Epoch 77/10000, Training Loss: 70.23146057128906, Validation Loss: 2.7148666381835938\n",
      "Epoch 78/10000, Training Loss: 32.18499755859375, Validation Loss: 10.948433876037598\n",
      "Epoch 79/10000, Training Loss: 13.889816284179688, Validation Loss: 21.695669174194336\n",
      "Epoch 80/10000, Training Loss: 77.92144775390625, Validation Loss: 650.836669921875\n",
      "Epoch 81/10000, Training Loss: 35.17799377441406, Validation Loss: 27.163124084472656\n",
      "Epoch 82/10000, Training Loss: 144.76382446289062, Validation Loss: 324.6108093261719\n",
      "Epoch 83/10000, Training Loss: 154.68878173828125, Validation Loss: 306.5700988769531\n",
      "Epoch 84/10000, Training Loss: 33.14506912231445, Validation Loss: 3.9459431171417236\n",
      "Epoch 85/10000, Training Loss: 71.82077026367188, Validation Loss: 95.49353790283203\n",
      "Epoch 86/10000, Training Loss: 49.70077896118164, Validation Loss: 3.492677688598633\n",
      "Epoch 87/10000, Training Loss: 34.56326675415039, Validation Loss: 11.506355285644531\n",
      "Epoch 88/10000, Training Loss: 52.80375671386719, Validation Loss: 133.4611053466797\n",
      "Epoch 89/10000, Training Loss: 41.16050338745117, Validation Loss: 11.587790489196777\n",
      "Epoch 90/10000, Training Loss: 103.26748657226562, Validation Loss: 702.24072265625\n",
      "Epoch 91/10000, Training Loss: 112.64783477783203, Validation Loss: 58.390869140625\n",
      "Epoch 92/10000, Training Loss: 85.59315490722656, Validation Loss: 185.2269287109375\n",
      "Epoch 93/10000, Training Loss: 25.990543365478516, Validation Loss: 87.89990997314453\n",
      "Epoch 94/10000, Training Loss: 41.613624572753906, Validation Loss: 184.5541534423828\n",
      "Epoch 95/10000, Training Loss: 114.11448669433594, Validation Loss: 2.9105730056762695\n",
      "Epoch 96/10000, Training Loss: 32.80025100708008, Validation Loss: 73.92988586425781\n",
      "Epoch 97/10000, Training Loss: 41.039024353027344, Validation Loss: 22.564924240112305\n",
      "Epoch 98/10000, Training Loss: 44.729007720947266, Validation Loss: 11.889945983886719\n",
      "Epoch 99/10000, Training Loss: 106.65694427490234, Validation Loss: 14.693794250488281\n",
      "Epoch 100/10000, Training Loss: 32.09933090209961, Validation Loss: 50.56904602050781\n",
      "Epoch 101/10000, Training Loss: 43.40657043457031, Validation Loss: 37.411170959472656\n",
      "Epoch 102/10000, Training Loss: 21.170597076416016, Validation Loss: 0.9960992336273193\n",
      "Epoch 103/10000, Training Loss: 51.800010681152344, Validation Loss: 76.84366607666016\n",
      "Epoch 104/10000, Training Loss: 78.94611358642578, Validation Loss: 71.42369079589844\n",
      "Epoch 105/10000, Training Loss: 88.0166015625, Validation Loss: 72.13212585449219\n",
      "Epoch 106/10000, Training Loss: 118.81946563720703, Validation Loss: 37.433773040771484\n",
      "Epoch 107/10000, Training Loss: 25.668418884277344, Validation Loss: 0.5498673319816589\n",
      "Epoch 108/10000, Training Loss: 69.86758422851562, Validation Loss: 412.216552734375\n",
      "Epoch 109/10000, Training Loss: 31.46317481994629, Validation Loss: 5.065426826477051\n",
      "Epoch 110/10000, Training Loss: 75.21895599365234, Validation Loss: 379.6705627441406\n",
      "Epoch 111/10000, Training Loss: 73.51555633544922, Validation Loss: 86.6487045288086\n",
      "Epoch 112/10000, Training Loss: 43.91728973388672, Validation Loss: 48.3413200378418\n",
      "Epoch 113/10000, Training Loss: 66.7945556640625, Validation Loss: 395.9518127441406\n",
      "Epoch 114/10000, Training Loss: 38.17453384399414, Validation Loss: 8.254646301269531\n",
      "Epoch 115/10000, Training Loss: 67.09871673583984, Validation Loss: 0.9680312275886536\n",
      "Epoch 116/10000, Training Loss: 64.75885772705078, Validation Loss: 221.365966796875\n",
      "Epoch 117/10000, Training Loss: 136.17601013183594, Validation Loss: 70.13772583007812\n",
      "Epoch 118/10000, Training Loss: 67.079833984375, Validation Loss: 72.8812026977539\n",
      "Epoch 119/10000, Training Loss: 99.394775390625, Validation Loss: 62.66014099121094\n",
      "Epoch 120/10000, Training Loss: 19.091777801513672, Validation Loss: 15.609313011169434\n",
      "Epoch 121/10000, Training Loss: 16.518428802490234, Validation Loss: 28.101882934570312\n",
      "Epoch 122/10000, Training Loss: 53.86751174926758, Validation Loss: 51.985897064208984\n",
      "Epoch 123/10000, Training Loss: 38.283660888671875, Validation Loss: 11.400535583496094\n",
      "Epoch 124/10000, Training Loss: 27.51692008972168, Validation Loss: 56.938262939453125\n",
      "Epoch 125/10000, Training Loss: 53.47283172607422, Validation Loss: 183.77088928222656\n",
      "Epoch 126/10000, Training Loss: 46.94700622558594, Validation Loss: 15.735011100769043\n",
      "Epoch 127/10000, Training Loss: 126.33274841308594, Validation Loss: 2.3144991397857666\n",
      "Epoch 128/10000, Training Loss: 44.70014190673828, Validation Loss: 142.1717071533203\n",
      "Epoch 129/10000, Training Loss: 34.91193389892578, Validation Loss: 5.375518798828125\n",
      "Epoch 130/10000, Training Loss: 53.84870910644531, Validation Loss: 239.49473571777344\n",
      "Epoch 131/10000, Training Loss: 37.475738525390625, Validation Loss: 14.769574165344238\n",
      "Epoch 132/10000, Training Loss: 34.204219818115234, Validation Loss: 52.94749450683594\n",
      "Epoch 133/10000, Training Loss: 58.840572357177734, Validation Loss: 38.45066452026367\n",
      "Epoch 134/10000, Training Loss: 17.51760482788086, Validation Loss: 4.35470724105835\n",
      "Epoch 135/10000, Training Loss: 40.883544921875, Validation Loss: 6.788318157196045\n",
      "Epoch 136/10000, Training Loss: 168.76568603515625, Validation Loss: 162.5381317138672\n",
      "Epoch 137/10000, Training Loss: 40.20663833618164, Validation Loss: 8.950379371643066\n",
      "Epoch 138/10000, Training Loss: 37.29142761230469, Validation Loss: 59.677249908447266\n",
      "Epoch 139/10000, Training Loss: 46.788700103759766, Validation Loss: 27.80864143371582\n",
      "Epoch 140/10000, Training Loss: 61.051204681396484, Validation Loss: 203.99195861816406\n",
      "Epoch 141/10000, Training Loss: 58.17716598510742, Validation Loss: 536.482421875\n",
      "Epoch 142/10000, Training Loss: 12.2896728515625, Validation Loss: 0.6724948287010193\n",
      "Epoch 143/10000, Training Loss: 53.682621002197266, Validation Loss: 45.03028869628906\n",
      "Epoch 144/10000, Training Loss: 52.78999710083008, Validation Loss: 58.9125862121582\n",
      "Epoch 145/10000, Training Loss: 47.32383346557617, Validation Loss: 7.745448589324951\n",
      "Epoch 146/10000, Training Loss: 63.67168426513672, Validation Loss: 62.58938980102539\n",
      "Epoch 147/10000, Training Loss: 69.4892578125, Validation Loss: 4.517099857330322\n",
      "Epoch 148/10000, Training Loss: 26.866191864013672, Validation Loss: 0.9797196984291077\n",
      "Epoch 149/10000, Training Loss: 50.567848205566406, Validation Loss: 4.6909942626953125\n",
      "Epoch 150/10000, Training Loss: 86.41139221191406, Validation Loss: 70.534912109375\n",
      "Epoch 151/10000, Training Loss: 77.23397064208984, Validation Loss: 2.4426286220550537\n",
      "Epoch 152/10000, Training Loss: 31.457162857055664, Validation Loss: 251.03855895996094\n",
      "Epoch 153/10000, Training Loss: 35.16992950439453, Validation Loss: 43.19179916381836\n",
      "Epoch 154/10000, Training Loss: 58.37115478515625, Validation Loss: 18.09234619140625\n",
      "Epoch 155/10000, Training Loss: 54.42005157470703, Validation Loss: 29.69559669494629\n",
      "Epoch 156/10000, Training Loss: 14.686807632446289, Validation Loss: 17.362154006958008\n",
      "Epoch 157/10000, Training Loss: 22.307464599609375, Validation Loss: 5.807746887207031\n",
      "Epoch 158/10000, Training Loss: 106.62423706054688, Validation Loss: 4.294142723083496\n",
      "Epoch 159/10000, Training Loss: 20.070363998413086, Validation Loss: 6.770850658416748\n",
      "Epoch 160/10000, Training Loss: 19.075040817260742, Validation Loss: 42.37503433227539\n",
      "Epoch 161/10000, Training Loss: 45.39499282836914, Validation Loss: 238.82330322265625\n",
      "Epoch 162/10000, Training Loss: 59.21369171142578, Validation Loss: 23.282211303710938\n",
      "Epoch 163/10000, Training Loss: 93.56139373779297, Validation Loss: 25.03453826904297\n",
      "Epoch 164/10000, Training Loss: 24.16831398010254, Validation Loss: 59.155120849609375\n",
      "Epoch 165/10000, Training Loss: 46.156925201416016, Validation Loss: 24.731168746948242\n",
      "Epoch 166/10000, Training Loss: 58.41287612915039, Validation Loss: 95.90243530273438\n",
      "Epoch 167/10000, Training Loss: 68.69609832763672, Validation Loss: 76.32103729248047\n",
      "Epoch 168/10000, Training Loss: 26.107351303100586, Validation Loss: 26.80162239074707\n",
      "Epoch 169/10000, Training Loss: 56.277645111083984, Validation Loss: 4.7345194816589355\n",
      "Epoch 170/10000, Training Loss: 30.345874786376953, Validation Loss: 74.13987731933594\n",
      "Epoch 171/10000, Training Loss: 29.743112564086914, Validation Loss: 9.248183250427246\n",
      "Epoch 172/10000, Training Loss: 26.556089401245117, Validation Loss: 0.5005701780319214\n",
      "Epoch 173/10000, Training Loss: 10.436995506286621, Validation Loss: 10.34510612487793\n",
      "Epoch 174/10000, Training Loss: 68.53083038330078, Validation Loss: 236.91241455078125\n",
      "Epoch 175/10000, Training Loss: 38.117408752441406, Validation Loss: 12.077278137207031\n",
      "Epoch 176/10000, Training Loss: 47.6212158203125, Validation Loss: 2.512894868850708\n",
      "Epoch 177/10000, Training Loss: 29.35879135131836, Validation Loss: 12.385566711425781\n",
      "Epoch 178/10000, Training Loss: 65.16138458251953, Validation Loss: 44.70394515991211\n",
      "Epoch 179/10000, Training Loss: 29.309120178222656, Validation Loss: 18.85881805419922\n",
      "Epoch 180/10000, Training Loss: 55.370853424072266, Validation Loss: 99.6402587890625\n",
      "Epoch 181/10000, Training Loss: 93.92159271240234, Validation Loss: 106.49930572509766\n",
      "Epoch 182/10000, Training Loss: 21.20156478881836, Validation Loss: 38.799983978271484\n",
      "Epoch 183/10000, Training Loss: 29.199689865112305, Validation Loss: 82.66366577148438\n",
      "Epoch 184/10000, Training Loss: 49.91189193725586, Validation Loss: 28.84071922302246\n",
      "Epoch 185/10000, Training Loss: 32.11565399169922, Validation Loss: 5.546464443206787\n",
      "Epoch 186/10000, Training Loss: 41.277931213378906, Validation Loss: 69.98351287841797\n",
      "Epoch 187/10000, Training Loss: 22.593278884887695, Validation Loss: 15.060523986816406\n",
      "Epoch 188/10000, Training Loss: 100.98648071289062, Validation Loss: 50.20793151855469\n",
      "Epoch 189/10000, Training Loss: 38.46349334716797, Validation Loss: 54.63557434082031\n",
      "Epoch 190/10000, Training Loss: 27.370262145996094, Validation Loss: 11.934802055358887\n",
      "Epoch 191/10000, Training Loss: 90.65092468261719, Validation Loss: 54.124446868896484\n",
      "Epoch 192/10000, Training Loss: 37.53580856323242, Validation Loss: 60.528751373291016\n",
      "Epoch 193/10000, Training Loss: 21.812803268432617, Validation Loss: 9.153356552124023\n",
      "Epoch 194/10000, Training Loss: 66.02731323242188, Validation Loss: 81.65886688232422\n",
      "Epoch 195/10000, Training Loss: 32.67801284790039, Validation Loss: 14.660380363464355\n",
      "Epoch 196/10000, Training Loss: 17.686809539794922, Validation Loss: 42.45173645019531\n",
      "Epoch 197/10000, Training Loss: 35.399559020996094, Validation Loss: 1.4920868873596191\n",
      "Epoch 198/10000, Training Loss: 54.80543899536133, Validation Loss: 36.96488571166992\n",
      "Epoch 199/10000, Training Loss: 64.23977661132812, Validation Loss: 57.64106369018555\n",
      "Epoch 200/10000, Training Loss: 45.45000457763672, Validation Loss: 212.3274383544922\n",
      "Epoch 201/10000, Training Loss: 24.747873306274414, Validation Loss: 98.25048065185547\n",
      "Epoch 202/10000, Training Loss: 16.99049186706543, Validation Loss: 150.2737274169922\n",
      "Epoch 203/10000, Training Loss: 29.75471305847168, Validation Loss: 30.55906105041504\n",
      "Epoch 204/10000, Training Loss: 31.31144142150879, Validation Loss: 68.190185546875\n",
      "Epoch 205/10000, Training Loss: 43.46591567993164, Validation Loss: 86.69882202148438\n",
      "Epoch 206/10000, Training Loss: 30.63736343383789, Validation Loss: 32.939109802246094\n",
      "Epoch 207/10000, Training Loss: 24.516054153442383, Validation Loss: 54.3975944519043\n",
      "Epoch 208/10000, Training Loss: 54.96982955932617, Validation Loss: 102.820068359375\n",
      "Epoch 209/10000, Training Loss: 83.11394500732422, Validation Loss: 429.8619689941406\n",
      "Epoch 210/10000, Training Loss: 63.18035125732422, Validation Loss: 28.745092391967773\n",
      "Epoch 211/10000, Training Loss: 124.1358642578125, Validation Loss: 114.87894439697266\n",
      "Epoch 212/10000, Training Loss: 31.031064987182617, Validation Loss: 2.5177066326141357\n",
      "Epoch 213/10000, Training Loss: 46.815975189208984, Validation Loss: 120.96118927001953\n",
      "Epoch 214/10000, Training Loss: 41.78239059448242, Validation Loss: 19.47648811340332\n",
      "Epoch 215/10000, Training Loss: 13.95709228515625, Validation Loss: 23.879186630249023\n",
      "Epoch 216/10000, Training Loss: 47.08440399169922, Validation Loss: 11.658955574035645\n",
      "Epoch 217/10000, Training Loss: 42.389671325683594, Validation Loss: 19.9869384765625\n",
      "Epoch 218/10000, Training Loss: 34.7159538269043, Validation Loss: 44.58098220825195\n",
      "Epoch 219/10000, Training Loss: 17.80162811279297, Validation Loss: 43.88115310668945\n",
      "Epoch 220/10000, Training Loss: 90.01704406738281, Validation Loss: 45.52995681762695\n",
      "Epoch 221/10000, Training Loss: 36.471439361572266, Validation Loss: 8.105473518371582\n",
      "Epoch 222/10000, Training Loss: 44.32976531982422, Validation Loss: 0.23583586513996124\n",
      "Epoch 223/10000, Training Loss: 13.698439598083496, Validation Loss: 14.691566467285156\n",
      "Epoch 224/10000, Training Loss: 31.542278289794922, Validation Loss: 16.56693458557129\n",
      "Epoch 225/10000, Training Loss: 25.292043685913086, Validation Loss: 19.308300018310547\n",
      "Epoch 226/10000, Training Loss: 15.881135940551758, Validation Loss: 40.59813690185547\n",
      "Epoch 227/10000, Training Loss: 24.56186866760254, Validation Loss: 0.5555750131607056\n",
      "Epoch 228/10000, Training Loss: 63.486995697021484, Validation Loss: 16.449430465698242\n",
      "Epoch 229/10000, Training Loss: 53.0399169921875, Validation Loss: 10.03726577758789\n",
      "Epoch 230/10000, Training Loss: 26.946977615356445, Validation Loss: 38.05437088012695\n",
      "Epoch 231/10000, Training Loss: 99.01013946533203, Validation Loss: 86.17305755615234\n",
      "Epoch 232/10000, Training Loss: 43.9942626953125, Validation Loss: 15.489750862121582\n",
      "Epoch 233/10000, Training Loss: 27.13027000427246, Validation Loss: 49.0426025390625\n",
      "Epoch 234/10000, Training Loss: 50.359920501708984, Validation Loss: 11.00607681274414\n",
      "Epoch 235/10000, Training Loss: 19.92486572265625, Validation Loss: 6.556655406951904\n",
      "Epoch 236/10000, Training Loss: 38.163665771484375, Validation Loss: 12.177695274353027\n",
      "Epoch 237/10000, Training Loss: 33.51702117919922, Validation Loss: 209.7422637939453\n",
      "Epoch 238/10000, Training Loss: 33.039894104003906, Validation Loss: 16.782672882080078\n",
      "Epoch 239/10000, Training Loss: 77.50547790527344, Validation Loss: 51.20236587524414\n",
      "Epoch 240/10000, Training Loss: 53.98244857788086, Validation Loss: 166.1745147705078\n",
      "Epoch 241/10000, Training Loss: 67.48859405517578, Validation Loss: 104.8774642944336\n",
      "Epoch 242/10000, Training Loss: 33.56295394897461, Validation Loss: 7.17165994644165\n",
      "Epoch 243/10000, Training Loss: 68.0439682006836, Validation Loss: 73.25634002685547\n",
      "Epoch 244/10000, Training Loss: 49.625633239746094, Validation Loss: 6.083072662353516\n",
      "Epoch 245/10000, Training Loss: 46.70665740966797, Validation Loss: 80.25981140136719\n",
      "Epoch 246/10000, Training Loss: 28.529850006103516, Validation Loss: 3.4963748455047607\n",
      "Epoch 247/10000, Training Loss: 19.54640007019043, Validation Loss: 27.19588279724121\n",
      "Epoch 248/10000, Training Loss: 45.70801544189453, Validation Loss: 228.14735412597656\n",
      "Epoch 249/10000, Training Loss: 22.811481475830078, Validation Loss: 22.8162841796875\n",
      "Epoch 250/10000, Training Loss: 31.114521026611328, Validation Loss: 9.065447807312012\n",
      "Epoch 251/10000, Training Loss: 13.852632522583008, Validation Loss: 0.5814453959465027\n",
      "Epoch 252/10000, Training Loss: 26.58731460571289, Validation Loss: 77.46099853515625\n",
      "Epoch 253/10000, Training Loss: 46.431095123291016, Validation Loss: 21.43379783630371\n",
      "Epoch 254/10000, Training Loss: 73.92599487304688, Validation Loss: 38.99111557006836\n",
      "Epoch 255/10000, Training Loss: 20.60322380065918, Validation Loss: 47.11468505859375\n",
      "Epoch 256/10000, Training Loss: 18.307218551635742, Validation Loss: 2.5618369579315186\n",
      "Epoch 257/10000, Training Loss: 37.27901840209961, Validation Loss: 143.7007598876953\n",
      "Epoch 258/10000, Training Loss: 51.21437072753906, Validation Loss: 12.829830169677734\n",
      "Epoch 259/10000, Training Loss: 69.59663391113281, Validation Loss: 35.249019622802734\n",
      "Epoch 260/10000, Training Loss: 82.48843383789062, Validation Loss: 11.030634880065918\n",
      "Epoch 261/10000, Training Loss: 20.97788429260254, Validation Loss: 110.77405548095703\n",
      "Epoch 262/10000, Training Loss: 26.755990982055664, Validation Loss: 39.213890075683594\n",
      "Epoch 263/10000, Training Loss: 23.275236129760742, Validation Loss: 25.863943099975586\n",
      "Epoch 264/10000, Training Loss: 22.477731704711914, Validation Loss: 13.758456230163574\n",
      "Epoch 265/10000, Training Loss: 17.021617889404297, Validation Loss: 1.3894139528274536\n",
      "Epoch 266/10000, Training Loss: 42.98666763305664, Validation Loss: 116.87479400634766\n",
      "Epoch 267/10000, Training Loss: 41.08811569213867, Validation Loss: 67.2370834350586\n",
      "Epoch 268/10000, Training Loss: 33.68855285644531, Validation Loss: 6.384239196777344\n",
      "Epoch 269/10000, Training Loss: 27.992183685302734, Validation Loss: 1.1227202415466309\n",
      "Epoch 270/10000, Training Loss: 147.96473693847656, Validation Loss: 154.3199005126953\n",
      "Epoch 271/10000, Training Loss: 35.661865234375, Validation Loss: 7.722157001495361\n",
      "Epoch 272/10000, Training Loss: 13.539052963256836, Validation Loss: 6.48109769821167\n",
      "Epoch 273/10000, Training Loss: 24.879497528076172, Validation Loss: 21.13478660583496\n",
      "Epoch 274/10000, Training Loss: 43.581539154052734, Validation Loss: 3.2253599166870117\n",
      "Epoch 275/10000, Training Loss: 32.29857635498047, Validation Loss: 57.0337028503418\n",
      "Epoch 276/10000, Training Loss: 23.19359588623047, Validation Loss: 43.20634841918945\n",
      "Epoch 277/10000, Training Loss: 40.333919525146484, Validation Loss: 45.669551849365234\n",
      "Epoch 278/10000, Training Loss: 37.83163070678711, Validation Loss: 1.920326590538025\n",
      "Epoch 279/10000, Training Loss: 62.598995208740234, Validation Loss: 2.021885395050049\n",
      "Epoch 280/10000, Training Loss: 50.50802230834961, Validation Loss: 54.28117752075195\n",
      "Epoch 281/10000, Training Loss: 23.59352684020996, Validation Loss: 25.209630966186523\n",
      "Epoch 282/10000, Training Loss: 43.28434753417969, Validation Loss: 26.335342407226562\n",
      "Epoch 283/10000, Training Loss: 45.94477081298828, Validation Loss: 132.8767547607422\n",
      "Epoch 284/10000, Training Loss: 37.11375427246094, Validation Loss: 12.904379844665527\n",
      "Epoch 285/10000, Training Loss: 33.58582305908203, Validation Loss: 9.384453773498535\n",
      "Epoch 286/10000, Training Loss: 97.50550842285156, Validation Loss: 205.07945251464844\n",
      "Epoch 287/10000, Training Loss: 31.458885192871094, Validation Loss: 38.888736724853516\n",
      "Epoch 288/10000, Training Loss: 14.608076095581055, Validation Loss: 3.1878082752227783\n",
      "Epoch 289/10000, Training Loss: 35.22927474975586, Validation Loss: 30.621335983276367\n",
      "Epoch 290/10000, Training Loss: 31.508174896240234, Validation Loss: 79.61556243896484\n",
      "Epoch 291/10000, Training Loss: 27.960886001586914, Validation Loss: 45.58183288574219\n",
      "Epoch 292/10000, Training Loss: 34.02037811279297, Validation Loss: 17.392080307006836\n",
      "Epoch 293/10000, Training Loss: 26.15500831604004, Validation Loss: 2.521090030670166\n",
      "Epoch 294/10000, Training Loss: 16.618183135986328, Validation Loss: 7.402617931365967\n",
      "Epoch 295/10000, Training Loss: 33.79428482055664, Validation Loss: 120.1905517578125\n",
      "Epoch 296/10000, Training Loss: 14.839845657348633, Validation Loss: 21.453805923461914\n",
      "Epoch 297/10000, Training Loss: 8.338149070739746, Validation Loss: 19.0899715423584\n",
      "Epoch 298/10000, Training Loss: 21.40208625793457, Validation Loss: 3.202744722366333\n",
      "Epoch 299/10000, Training Loss: 24.69793701171875, Validation Loss: 20.003686904907227\n",
      "Epoch 300/10000, Training Loss: 37.83950424194336, Validation Loss: 10.336309432983398\n",
      "Epoch 301/10000, Training Loss: 21.925357818603516, Validation Loss: 37.35720443725586\n",
      "Epoch 302/10000, Training Loss: 24.73682403564453, Validation Loss: 20.635536193847656\n",
      "Epoch 303/10000, Training Loss: 20.453859329223633, Validation Loss: 6.845035076141357\n",
      "Epoch 304/10000, Training Loss: 28.878482818603516, Validation Loss: 3.4725112915039062\n",
      "Epoch 305/10000, Training Loss: 55.62635040283203, Validation Loss: 58.7346076965332\n",
      "Epoch 306/10000, Training Loss: 15.381084442138672, Validation Loss: 22.408103942871094\n",
      "Epoch 307/10000, Training Loss: 32.00617599487305, Validation Loss: 27.6939697265625\n",
      "Epoch 308/10000, Training Loss: 23.907869338989258, Validation Loss: 40.57664108276367\n",
      "Epoch 309/10000, Training Loss: 35.80238723754883, Validation Loss: 63.166751861572266\n",
      "Epoch 310/10000, Training Loss: 27.623371124267578, Validation Loss: 100.1229476928711\n",
      "Epoch 311/10000, Training Loss: 21.596363067626953, Validation Loss: 9.893318176269531\n",
      "Epoch 312/10000, Training Loss: 39.589866638183594, Validation Loss: 58.963130950927734\n",
      "Epoch 313/10000, Training Loss: 77.47111511230469, Validation Loss: 110.70721435546875\n",
      "Epoch 314/10000, Training Loss: 19.922679901123047, Validation Loss: 20.674108505249023\n",
      "Epoch 315/10000, Training Loss: 11.246006965637207, Validation Loss: 0.9124576449394226\n",
      "Epoch 316/10000, Training Loss: 19.010835647583008, Validation Loss: 46.68623733520508\n",
      "Epoch 317/10000, Training Loss: 58.70229721069336, Validation Loss: 38.2572021484375\n",
      "Epoch 318/10000, Training Loss: 65.50802612304688, Validation Loss: 110.76812744140625\n",
      "Epoch 319/10000, Training Loss: 46.775672912597656, Validation Loss: 15.907336235046387\n",
      "Epoch 320/10000, Training Loss: 24.685535430908203, Validation Loss: 22.37346076965332\n",
      "Epoch 321/10000, Training Loss: 10.466456413269043, Validation Loss: 1.9889988899230957\n",
      "Epoch 322/10000, Training Loss: 9.366427421569824, Validation Loss: 8.88547134399414\n",
      "Epoch 323/10000, Training Loss: 30.504838943481445, Validation Loss: 93.8936767578125\n",
      "Epoch 324/10000, Training Loss: 46.41530227661133, Validation Loss: 8.02859115600586\n",
      "Epoch 325/10000, Training Loss: 22.608428955078125, Validation Loss: 61.9444580078125\n",
      "Epoch 326/10000, Training Loss: 33.98107147216797, Validation Loss: 2.2676613330841064\n",
      "Epoch 327/10000, Training Loss: 30.949790954589844, Validation Loss: 36.518287658691406\n",
      "Epoch 328/10000, Training Loss: 30.82835578918457, Validation Loss: 20.3774356842041\n",
      "Epoch 329/10000, Training Loss: 61.975669860839844, Validation Loss: 19.614198684692383\n",
      "Epoch 330/10000, Training Loss: 31.750892639160156, Validation Loss: 22.190603256225586\n",
      "Epoch 331/10000, Training Loss: 21.47946548461914, Validation Loss: 3.705374002456665\n",
      "Epoch 332/10000, Training Loss: 16.43060874938965, Validation Loss: 18.891443252563477\n",
      "Epoch 333/10000, Training Loss: 12.704309463500977, Validation Loss: 18.53940200805664\n",
      "Epoch 334/10000, Training Loss: 21.692811965942383, Validation Loss: 8.752057075500488\n",
      "Epoch 335/10000, Training Loss: 20.48270034790039, Validation Loss: 36.49617004394531\n",
      "Epoch 336/10000, Training Loss: 44.47473907470703, Validation Loss: 5.836690902709961\n",
      "Epoch 337/10000, Training Loss: 51.309993743896484, Validation Loss: 68.59479522705078\n",
      "Epoch 338/10000, Training Loss: 42.24839782714844, Validation Loss: 75.40760803222656\n",
      "Epoch 339/10000, Training Loss: 74.19231414794922, Validation Loss: 16.516305923461914\n",
      "Epoch 340/10000, Training Loss: 19.538440704345703, Validation Loss: 35.43085479736328\n",
      "Epoch 341/10000, Training Loss: 32.188316345214844, Validation Loss: 67.80786895751953\n",
      "Epoch 342/10000, Training Loss: 110.527099609375, Validation Loss: 66.12782287597656\n",
      "Epoch 343/10000, Training Loss: 27.698102951049805, Validation Loss: 55.50032043457031\n",
      "Epoch 344/10000, Training Loss: 27.36984634399414, Validation Loss: 20.58452033996582\n",
      "Epoch 345/10000, Training Loss: 55.70779800415039, Validation Loss: 11.563172340393066\n",
      "Epoch 346/10000, Training Loss: 25.446672439575195, Validation Loss: 29.803945541381836\n",
      "Epoch 347/10000, Training Loss: 28.06709098815918, Validation Loss: 165.51480102539062\n",
      "Epoch 348/10000, Training Loss: 31.050445556640625, Validation Loss: 115.0464096069336\n",
      "Epoch 349/10000, Training Loss: 49.78593444824219, Validation Loss: 56.78371047973633\n",
      "Epoch 350/10000, Training Loss: 35.94931411743164, Validation Loss: 8.633232116699219\n",
      "Epoch 351/10000, Training Loss: 74.91820526123047, Validation Loss: 43.29140853881836\n",
      "Epoch 352/10000, Training Loss: 47.76875305175781, Validation Loss: 78.33478546142578\n",
      "Epoch 353/10000, Training Loss: 19.36007308959961, Validation Loss: 0.408487468957901\n",
      "Epoch 354/10000, Training Loss: 25.911094665527344, Validation Loss: 27.08997917175293\n",
      "Epoch 355/10000, Training Loss: 73.42660522460938, Validation Loss: 101.62869262695312\n",
      "Epoch 356/10000, Training Loss: 19.590564727783203, Validation Loss: 17.85749626159668\n",
      "Epoch 357/10000, Training Loss: 47.95197296142578, Validation Loss: 26.234079360961914\n",
      "Epoch 358/10000, Training Loss: 34.219234466552734, Validation Loss: 2.6174986362457275\n",
      "Epoch 359/10000, Training Loss: 19.804229736328125, Validation Loss: 13.95687484741211\n",
      "Epoch 360/10000, Training Loss: 10.721735954284668, Validation Loss: 20.269773483276367\n",
      "Epoch 361/10000, Training Loss: 12.775100708007812, Validation Loss: 20.874635696411133\n",
      "Epoch 362/10000, Training Loss: 17.021028518676758, Validation Loss: 10.038947105407715\n",
      "Epoch 363/10000, Training Loss: 50.637149810791016, Validation Loss: 26.717748641967773\n",
      "Epoch 364/10000, Training Loss: 38.88981628417969, Validation Loss: 140.43711853027344\n",
      "Epoch 365/10000, Training Loss: 29.734392166137695, Validation Loss: 30.923606872558594\n",
      "Epoch 366/10000, Training Loss: 13.886221885681152, Validation Loss: 24.040983200073242\n",
      "Epoch 367/10000, Training Loss: 35.87601852416992, Validation Loss: 27.89252281188965\n",
      "Epoch 368/10000, Training Loss: 29.343080520629883, Validation Loss: 77.38549041748047\n",
      "Epoch 369/10000, Training Loss: 38.404212951660156, Validation Loss: 14.598030090332031\n",
      "Epoch 370/10000, Training Loss: 10.840234756469727, Validation Loss: 11.186577796936035\n",
      "Epoch 371/10000, Training Loss: 21.602760314941406, Validation Loss: 34.54375076293945\n",
      "Epoch 372/10000, Training Loss: 19.205299377441406, Validation Loss: 30.83231544494629\n",
      "Epoch 373/10000, Training Loss: 38.41971206665039, Validation Loss: 26.10194206237793\n",
      "Epoch 374/10000, Training Loss: 15.747313499450684, Validation Loss: 24.007347106933594\n",
      "Epoch 375/10000, Training Loss: 39.92582321166992, Validation Loss: 90.6477279663086\n",
      "Epoch 376/10000, Training Loss: 9.080832481384277, Validation Loss: 29.89155387878418\n",
      "Epoch 377/10000, Training Loss: 37.19704818725586, Validation Loss: 3.8262455463409424\n",
      "Epoch 378/10000, Training Loss: 18.071552276611328, Validation Loss: 12.653393745422363\n",
      "Epoch 379/10000, Training Loss: 23.51103401184082, Validation Loss: 3.717876434326172\n",
      "Epoch 380/10000, Training Loss: 21.003313064575195, Validation Loss: 41.24700164794922\n",
      "Epoch 381/10000, Training Loss: 24.969022750854492, Validation Loss: 22.656320571899414\n",
      "Epoch 382/10000, Training Loss: 30.413480758666992, Validation Loss: 141.5030975341797\n",
      "Epoch 383/10000, Training Loss: 36.91836166381836, Validation Loss: 15.832653999328613\n",
      "Epoch 384/10000, Training Loss: 15.27033805847168, Validation Loss: 4.852736949920654\n",
      "Epoch 385/10000, Training Loss: 30.98618507385254, Validation Loss: 109.30683135986328\n",
      "Epoch 386/10000, Training Loss: 55.87055206298828, Validation Loss: 35.61445617675781\n",
      "Epoch 387/10000, Training Loss: 29.969627380371094, Validation Loss: 13.302647590637207\n",
      "Epoch 388/10000, Training Loss: 25.038734436035156, Validation Loss: 5.027266979217529\n",
      "Epoch 389/10000, Training Loss: 28.408958435058594, Validation Loss: 13.838732719421387\n",
      "Epoch 390/10000, Training Loss: 61.681331634521484, Validation Loss: 244.95750427246094\n",
      "Epoch 391/10000, Training Loss: 46.146183013916016, Validation Loss: 23.857025146484375\n",
      "Epoch 392/10000, Training Loss: 22.629695892333984, Validation Loss: 2.4281909465789795\n",
      "Epoch 393/10000, Training Loss: 23.599414825439453, Validation Loss: 80.06063079833984\n",
      "Epoch 394/10000, Training Loss: 57.36635208129883, Validation Loss: 33.92744445800781\n",
      "Epoch 395/10000, Training Loss: 29.36521339416504, Validation Loss: 8.671608924865723\n",
      "Epoch 396/10000, Training Loss: 40.121803283691406, Validation Loss: 22.678117752075195\n",
      "Epoch 397/10000, Training Loss: 53.92726516723633, Validation Loss: 109.95053100585938\n",
      "Epoch 398/10000, Training Loss: 26.66327667236328, Validation Loss: 38.550235748291016\n",
      "Epoch 399/10000, Training Loss: 22.630428314208984, Validation Loss: 6.7366042137146\n",
      "Epoch 400/10000, Training Loss: 40.66108703613281, Validation Loss: 0.5167271494865417\n",
      "Epoch 401/10000, Training Loss: 17.932661056518555, Validation Loss: 9.411420822143555\n",
      "Epoch 402/10000, Training Loss: 9.60128116607666, Validation Loss: 9.862170219421387\n",
      "Epoch 403/10000, Training Loss: 33.84888458251953, Validation Loss: 32.7691764831543\n",
      "Epoch 404/10000, Training Loss: 21.067440032958984, Validation Loss: 216.78419494628906\n",
      "Epoch 405/10000, Training Loss: 27.94184684753418, Validation Loss: 114.31934356689453\n",
      "Epoch 406/10000, Training Loss: 25.41910743713379, Validation Loss: 7.6508402824401855\n",
      "Epoch 407/10000, Training Loss: 16.844322204589844, Validation Loss: 16.51348304748535\n",
      "Epoch 408/10000, Training Loss: 81.843505859375, Validation Loss: 76.65795135498047\n",
      "Epoch 409/10000, Training Loss: 41.42870330810547, Validation Loss: 286.5993957519531\n",
      "Epoch 410/10000, Training Loss: 30.954303741455078, Validation Loss: 68.68950653076172\n",
      "Epoch 411/10000, Training Loss: 55.899169921875, Validation Loss: 7.3727264404296875\n",
      "Epoch 412/10000, Training Loss: 22.048282623291016, Validation Loss: 8.360682487487793\n",
      "Epoch 413/10000, Training Loss: 9.931316375732422, Validation Loss: 1.410544991493225\n",
      "Epoch 414/10000, Training Loss: 36.47581100463867, Validation Loss: 49.02473068237305\n",
      "Epoch 415/10000, Training Loss: 11.820985794067383, Validation Loss: 26.231149673461914\n",
      "Epoch 416/10000, Training Loss: 14.223992347717285, Validation Loss: 6.756505489349365\n",
      "Epoch 417/10000, Training Loss: 16.146820068359375, Validation Loss: 12.251887321472168\n",
      "Epoch 418/10000, Training Loss: 26.20629119873047, Validation Loss: 45.51797866821289\n",
      "Epoch 419/10000, Training Loss: 33.918434143066406, Validation Loss: 13.538447380065918\n",
      "Epoch 420/10000, Training Loss: 72.29541015625, Validation Loss: 2.295520782470703\n",
      "Epoch 421/10000, Training Loss: 23.396055221557617, Validation Loss: 26.38092613220215\n",
      "Epoch 422/10000, Training Loss: 21.475053787231445, Validation Loss: 38.76844024658203\n",
      "Epoch 423/10000, Training Loss: 15.93442153930664, Validation Loss: 13.823607444763184\n",
      "Epoch 424/10000, Training Loss: 33.38798904418945, Validation Loss: 53.86684036254883\n",
      "Epoch 425/10000, Training Loss: 21.95610809326172, Validation Loss: 11.282866477966309\n",
      "Epoch 426/10000, Training Loss: 18.31122589111328, Validation Loss: 15.809929847717285\n",
      "Epoch 427/10000, Training Loss: 20.577016830444336, Validation Loss: 6.033339977264404\n",
      "Epoch 428/10000, Training Loss: 47.148597717285156, Validation Loss: 31.360570907592773\n",
      "Epoch 429/10000, Training Loss: 18.16867446899414, Validation Loss: 6.016220569610596\n",
      "Epoch 430/10000, Training Loss: 19.69350814819336, Validation Loss: 36.236087799072266\n",
      "Epoch 431/10000, Training Loss: 46.60237503051758, Validation Loss: 28.070711135864258\n",
      "Epoch 432/10000, Training Loss: 35.892539978027344, Validation Loss: 3.6585476398468018\n",
      "Epoch 433/10000, Training Loss: 20.802919387817383, Validation Loss: 11.892718315124512\n",
      "Epoch 434/10000, Training Loss: 19.887887954711914, Validation Loss: 8.214426040649414\n",
      "Epoch 435/10000, Training Loss: 24.98459815979004, Validation Loss: 21.526620864868164\n",
      "Epoch 436/10000, Training Loss: 21.30349349975586, Validation Loss: 7.849117755889893\n",
      "Epoch 437/10000, Training Loss: 43.55605697631836, Validation Loss: 10.70716381072998\n",
      "Epoch 438/10000, Training Loss: 17.47974395751953, Validation Loss: 7.15117073059082\n",
      "Epoch 439/10000, Training Loss: 51.53765869140625, Validation Loss: 24.309097290039062\n",
      "Epoch 440/10000, Training Loss: 23.973583221435547, Validation Loss: 26.743032455444336\n",
      "Epoch 441/10000, Training Loss: 48.08150100708008, Validation Loss: 20.092321395874023\n",
      "Epoch 442/10000, Training Loss: 30.67860221862793, Validation Loss: 38.20119094848633\n",
      "Epoch 443/10000, Training Loss: 24.598346710205078, Validation Loss: 16.366689682006836\n",
      "Epoch 444/10000, Training Loss: 28.23613739013672, Validation Loss: 65.41071319580078\n",
      "Epoch 445/10000, Training Loss: 32.48809051513672, Validation Loss: 22.9491024017334\n",
      "Epoch 446/10000, Training Loss: 27.759334564208984, Validation Loss: 15.391167640686035\n",
      "Epoch 447/10000, Training Loss: 27.42993927001953, Validation Loss: 53.90195083618164\n",
      "Epoch 448/10000, Training Loss: 30.515073776245117, Validation Loss: 47.2501220703125\n",
      "Epoch 449/10000, Training Loss: 17.723957061767578, Validation Loss: 27.64019775390625\n",
      "Epoch 450/10000, Training Loss: 33.99327087402344, Validation Loss: 48.89644241333008\n",
      "Epoch 451/10000, Training Loss: 22.990642547607422, Validation Loss: 27.48070526123047\n",
      "Epoch 452/10000, Training Loss: 40.15569305419922, Validation Loss: 2.8285675048828125\n",
      "Epoch 453/10000, Training Loss: 14.861151695251465, Validation Loss: 97.50977325439453\n",
      "Epoch 454/10000, Training Loss: 37.15024185180664, Validation Loss: 89.14352416992188\n",
      "Epoch 455/10000, Training Loss: 17.477142333984375, Validation Loss: 48.06368637084961\n",
      "Epoch 456/10000, Training Loss: 14.723673820495605, Validation Loss: 23.376554489135742\n",
      "Epoch 457/10000, Training Loss: 41.157772064208984, Validation Loss: 115.36712646484375\n",
      "Epoch 458/10000, Training Loss: 29.389413833618164, Validation Loss: 19.46174430847168\n",
      "Epoch 459/10000, Training Loss: 16.80264663696289, Validation Loss: 41.68974685668945\n",
      "Epoch 460/10000, Training Loss: 17.691938400268555, Validation Loss: 30.519460678100586\n",
      "Epoch 461/10000, Training Loss: 12.68120288848877, Validation Loss: 10.180399894714355\n",
      "Epoch 462/10000, Training Loss: 61.88146209716797, Validation Loss: 24.377275466918945\n",
      "Epoch 463/10000, Training Loss: 19.101797103881836, Validation Loss: 2.6855571269989014\n",
      "Epoch 464/10000, Training Loss: 23.481346130371094, Validation Loss: 37.28535842895508\n",
      "Epoch 465/10000, Training Loss: 42.48234558105469, Validation Loss: 15.476248741149902\n",
      "Epoch 466/10000, Training Loss: 59.72201919555664, Validation Loss: 44.361820220947266\n",
      "Epoch 467/10000, Training Loss: 16.822120666503906, Validation Loss: 15.403206825256348\n",
      "Epoch 468/10000, Training Loss: 101.10464477539062, Validation Loss: 21.05431365966797\n",
      "Epoch 469/10000, Training Loss: 20.3936767578125, Validation Loss: 27.568466186523438\n",
      "Epoch 470/10000, Training Loss: 21.888334274291992, Validation Loss: 41.21877670288086\n",
      "Epoch 471/10000, Training Loss: 40.6661376953125, Validation Loss: 74.63854217529297\n",
      "Epoch 472/10000, Training Loss: 38.82086944580078, Validation Loss: 85.23326873779297\n",
      "Epoch 473/10000, Training Loss: 25.88087272644043, Validation Loss: 24.79839515686035\n",
      "Epoch 474/10000, Training Loss: 47.108280181884766, Validation Loss: 18.509313583374023\n",
      "Epoch 475/10000, Training Loss: 30.487638473510742, Validation Loss: 64.98780059814453\n",
      "Epoch 476/10000, Training Loss: 29.363059997558594, Validation Loss: 9.5442533493042\n",
      "Epoch 477/10000, Training Loss: 20.850292205810547, Validation Loss: 58.97908020019531\n",
      "Epoch 478/10000, Training Loss: 27.815763473510742, Validation Loss: 78.82318115234375\n",
      "Epoch 479/10000, Training Loss: 11.896552085876465, Validation Loss: 4.2630391120910645\n",
      "Epoch 480/10000, Training Loss: 24.78251838684082, Validation Loss: 41.20673751831055\n",
      "Epoch 481/10000, Training Loss: 23.797809600830078, Validation Loss: 5.6766510009765625\n",
      "Epoch 482/10000, Training Loss: 38.16549301147461, Validation Loss: 21.542831420898438\n",
      "Epoch 483/10000, Training Loss: 32.554710388183594, Validation Loss: 58.8036994934082\n",
      "Epoch 484/10000, Training Loss: 19.89728546142578, Validation Loss: 22.328262329101562\n",
      "Epoch 485/10000, Training Loss: 14.705948829650879, Validation Loss: 22.76603126525879\n",
      "Epoch 486/10000, Training Loss: 17.6520938873291, Validation Loss: 45.20278549194336\n",
      "Epoch 487/10000, Training Loss: 47.47727584838867, Validation Loss: 19.589780807495117\n",
      "Epoch 488/10000, Training Loss: 33.51662826538086, Validation Loss: 19.082754135131836\n",
      "Epoch 489/10000, Training Loss: 12.331267356872559, Validation Loss: 6.792166233062744\n",
      "Epoch 490/10000, Training Loss: 68.81410217285156, Validation Loss: 83.93802642822266\n",
      "Epoch 491/10000, Training Loss: 44.819252014160156, Validation Loss: 117.78030395507812\n",
      "Epoch 492/10000, Training Loss: 23.387977600097656, Validation Loss: 19.94193458557129\n",
      "Epoch 493/10000, Training Loss: 12.403098106384277, Validation Loss: 4.36503791809082\n",
      "Epoch 494/10000, Training Loss: 66.33770751953125, Validation Loss: 13.594525337219238\n",
      "Epoch 495/10000, Training Loss: 25.077594757080078, Validation Loss: 15.11823558807373\n",
      "Epoch 496/10000, Training Loss: 60.01186752319336, Validation Loss: 13.374076843261719\n",
      "Epoch 497/10000, Training Loss: 34.69670104980469, Validation Loss: 19.607511520385742\n",
      "Epoch 498/10000, Training Loss: 31.95555877685547, Validation Loss: 24.174856185913086\n",
      "Epoch 499/10000, Training Loss: 16.14850616455078, Validation Loss: 4.714294910430908\n",
      "Epoch 500/10000, Training Loss: 21.382631301879883, Validation Loss: 6.169286727905273\n",
      "Epoch 501/10000, Training Loss: 18.29966926574707, Validation Loss: 15.232295989990234\n",
      "Epoch 502/10000, Training Loss: 14.968897819519043, Validation Loss: 4.046985149383545\n",
      "Epoch 503/10000, Training Loss: 26.78392791748047, Validation Loss: 103.17322540283203\n",
      "Epoch 504/10000, Training Loss: 58.90453338623047, Validation Loss: 26.21222496032715\n",
      "Epoch 505/10000, Training Loss: 23.89303970336914, Validation Loss: 31.102996826171875\n",
      "Epoch 506/10000, Training Loss: 21.058650970458984, Validation Loss: 36.296226501464844\n",
      "Epoch 507/10000, Training Loss: 13.810115814208984, Validation Loss: 24.551584243774414\n",
      "Epoch 508/10000, Training Loss: 16.847087860107422, Validation Loss: 35.255699157714844\n",
      "Epoch 509/10000, Training Loss: 27.103946685791016, Validation Loss: 31.6610107421875\n",
      "Epoch 510/10000, Training Loss: 11.490754127502441, Validation Loss: 11.570564270019531\n",
      "Epoch 511/10000, Training Loss: 25.234405517578125, Validation Loss: 18.946287155151367\n",
      "Epoch 512/10000, Training Loss: 19.802534103393555, Validation Loss: 27.797574996948242\n",
      "Epoch 513/10000, Training Loss: 14.224613189697266, Validation Loss: 15.252506256103516\n",
      "Epoch 514/10000, Training Loss: 25.979190826416016, Validation Loss: 92.55374145507812\n",
      "Epoch 515/10000, Training Loss: 33.204410552978516, Validation Loss: 92.61406707763672\n",
      "Epoch 516/10000, Training Loss: 24.958208084106445, Validation Loss: 47.495361328125\n",
      "Epoch 517/10000, Training Loss: 37.032798767089844, Validation Loss: 28.537519454956055\n",
      "Epoch 518/10000, Training Loss: 27.738924026489258, Validation Loss: 17.260854721069336\n",
      "Epoch 519/10000, Training Loss: 32.84242248535156, Validation Loss: 12.626311302185059\n",
      "Epoch 520/10000, Training Loss: 18.061525344848633, Validation Loss: 129.9745635986328\n",
      "Epoch 521/10000, Training Loss: 37.830074310302734, Validation Loss: 14.357807159423828\n",
      "Epoch 522/10000, Training Loss: 20.42471694946289, Validation Loss: 26.653432846069336\n",
      "Epoch 523/10000, Training Loss: 23.679975509643555, Validation Loss: 31.019777297973633\n",
      "Epoch 524/10000, Training Loss: 27.383148193359375, Validation Loss: 20.334718704223633\n",
      "Epoch 525/10000, Training Loss: 20.305988311767578, Validation Loss: 23.501060485839844\n",
      "Epoch 526/10000, Training Loss: 20.323152542114258, Validation Loss: 47.34816360473633\n",
      "Epoch 527/10000, Training Loss: 10.794373512268066, Validation Loss: 3.924243211746216\n",
      "Epoch 528/10000, Training Loss: 16.192228317260742, Validation Loss: 29.779401779174805\n",
      "Epoch 529/10000, Training Loss: 17.545257568359375, Validation Loss: 9.991641998291016\n",
      "Epoch 530/10000, Training Loss: 45.35702896118164, Validation Loss: 8.99756908416748\n",
      "Epoch 531/10000, Training Loss: 124.72412109375, Validation Loss: 9.864800453186035\n",
      "Epoch 532/10000, Training Loss: 25.06354522705078, Validation Loss: 2.987301826477051\n",
      "Epoch 533/10000, Training Loss: 15.377126693725586, Validation Loss: 26.002487182617188\n",
      "Epoch 534/10000, Training Loss: 47.36370086669922, Validation Loss: 14.65808391571045\n",
      "Epoch 535/10000, Training Loss: 20.602296829223633, Validation Loss: 15.433940887451172\n",
      "Epoch 536/10000, Training Loss: 28.402835845947266, Validation Loss: 32.646263122558594\n",
      "Epoch 537/10000, Training Loss: 39.8238525390625, Validation Loss: 5.984209060668945\n",
      "Epoch 538/10000, Training Loss: 23.82304573059082, Validation Loss: 60.412017822265625\n",
      "Epoch 539/10000, Training Loss: 29.65033721923828, Validation Loss: 15.622845649719238\n",
      "Epoch 540/10000, Training Loss: 18.208620071411133, Validation Loss: 7.099390506744385\n",
      "Epoch 541/10000, Training Loss: 11.529555320739746, Validation Loss: 8.181310653686523\n",
      "Epoch 542/10000, Training Loss: 22.760498046875, Validation Loss: 21.79022789001465\n",
      "Epoch 543/10000, Training Loss: 14.645977020263672, Validation Loss: 18.689340591430664\n",
      "Epoch 544/10000, Training Loss: 27.80522346496582, Validation Loss: 6.368622303009033\n",
      "Epoch 545/10000, Training Loss: 14.575338363647461, Validation Loss: 12.69428539276123\n",
      "Epoch 546/10000, Training Loss: 19.592655181884766, Validation Loss: 9.228419303894043\n",
      "Epoch 547/10000, Training Loss: 14.858491897583008, Validation Loss: 19.032041549682617\n",
      "Epoch 548/10000, Training Loss: 10.444372177124023, Validation Loss: 9.113919258117676\n",
      "Epoch 549/10000, Training Loss: 29.46800994873047, Validation Loss: 26.038558959960938\n",
      "Epoch 550/10000, Training Loss: 27.153636932373047, Validation Loss: 14.25302791595459\n",
      "Epoch 551/10000, Training Loss: 20.03837013244629, Validation Loss: 37.76165008544922\n",
      "Epoch 552/10000, Training Loss: 14.112325668334961, Validation Loss: 8.111997604370117\n",
      "Epoch 553/10000, Training Loss: 27.42551040649414, Validation Loss: 32.54853820800781\n",
      "Epoch 554/10000, Training Loss: 30.254108428955078, Validation Loss: 54.40854263305664\n",
      "Epoch 555/10000, Training Loss: 23.59775733947754, Validation Loss: 5.841263294219971\n",
      "Epoch 556/10000, Training Loss: 23.206594467163086, Validation Loss: 63.25447463989258\n",
      "Epoch 557/10000, Training Loss: 12.715402603149414, Validation Loss: 10.230603218078613\n",
      "Epoch 558/10000, Training Loss: 16.446260452270508, Validation Loss: 7.208951473236084\n",
      "Epoch 559/10000, Training Loss: 29.060474395751953, Validation Loss: 12.707242012023926\n",
      "Epoch 560/10000, Training Loss: 31.59337043762207, Validation Loss: 16.286054611206055\n",
      "Epoch 561/10000, Training Loss: 18.840532302856445, Validation Loss: 20.373071670532227\n",
      "Epoch 562/10000, Training Loss: 33.43228530883789, Validation Loss: 4.667042255401611\n",
      "Epoch 563/10000, Training Loss: 28.344318389892578, Validation Loss: 7.907339572906494\n",
      "Epoch 564/10000, Training Loss: 44.669864654541016, Validation Loss: 24.706987380981445\n",
      "Epoch 565/10000, Training Loss: 12.203225135803223, Validation Loss: 11.676430702209473\n",
      "Epoch 566/10000, Training Loss: 20.700496673583984, Validation Loss: 1.3863731622695923\n",
      "Epoch 567/10000, Training Loss: 25.831880569458008, Validation Loss: 43.00912857055664\n",
      "Epoch 568/10000, Training Loss: 16.211286544799805, Validation Loss: 0.9787009358406067\n",
      "Epoch 569/10000, Training Loss: 22.108829498291016, Validation Loss: 64.67920684814453\n",
      "Epoch 570/10000, Training Loss: 12.386628150939941, Validation Loss: 14.733098030090332\n",
      "Epoch 571/10000, Training Loss: 13.827042579650879, Validation Loss: 33.91635513305664\n",
      "Epoch 572/10000, Training Loss: 29.709686279296875, Validation Loss: 16.152612686157227\n",
      "Epoch 573/10000, Training Loss: 24.339134216308594, Validation Loss: 10.932068824768066\n",
      "Epoch 574/10000, Training Loss: 13.773163795471191, Validation Loss: 9.393794059753418\n",
      "Epoch 575/10000, Training Loss: 30.63162612915039, Validation Loss: 7.90786600112915\n",
      "Epoch 576/10000, Training Loss: 26.89281463623047, Validation Loss: 31.326501846313477\n",
      "Epoch 577/10000, Training Loss: 17.585729598999023, Validation Loss: 10.935748100280762\n",
      "Epoch 578/10000, Training Loss: 20.744911193847656, Validation Loss: 2.409250259399414\n",
      "Epoch 579/10000, Training Loss: 20.164987564086914, Validation Loss: 32.33406448364258\n",
      "Epoch 580/10000, Training Loss: 33.19375228881836, Validation Loss: 26.063262939453125\n",
      "Epoch 581/10000, Training Loss: 26.091642379760742, Validation Loss: 19.73312759399414\n",
      "Epoch 582/10000, Training Loss: 20.22732162475586, Validation Loss: 6.255178451538086\n",
      "Epoch 583/10000, Training Loss: 25.172739028930664, Validation Loss: 62.09103775024414\n",
      "Epoch 584/10000, Training Loss: 27.35038948059082, Validation Loss: 30.572723388671875\n",
      "Epoch 585/10000, Training Loss: 25.317737579345703, Validation Loss: 32.09560775756836\n",
      "Epoch 586/10000, Training Loss: 14.940436363220215, Validation Loss: 11.260337829589844\n",
      "Epoch 587/10000, Training Loss: 20.80732536315918, Validation Loss: 27.33869743347168\n",
      "Epoch 588/10000, Training Loss: 21.592205047607422, Validation Loss: 14.578556060791016\n",
      "Epoch 589/10000, Training Loss: 37.80899429321289, Validation Loss: 1.1213030815124512\n",
      "Epoch 590/10000, Training Loss: 23.78185272216797, Validation Loss: 20.339696884155273\n",
      "Epoch 591/10000, Training Loss: 10.899937629699707, Validation Loss: 13.146506309509277\n",
      "Epoch 592/10000, Training Loss: 22.41356658935547, Validation Loss: 44.391265869140625\n",
      "Epoch 593/10000, Training Loss: 10.436004638671875, Validation Loss: 23.607620239257812\n",
      "Epoch 594/10000, Training Loss: 35.740379333496094, Validation Loss: 71.61724090576172\n",
      "Epoch 595/10000, Training Loss: 17.495498657226562, Validation Loss: 16.633420944213867\n",
      "Epoch 596/10000, Training Loss: 14.068523406982422, Validation Loss: 8.035194396972656\n",
      "Epoch 597/10000, Training Loss: 17.080305099487305, Validation Loss: 14.041454315185547\n",
      "Epoch 598/10000, Training Loss: 21.379255294799805, Validation Loss: 3.2309436798095703\n",
      "Epoch 599/10000, Training Loss: 34.30630874633789, Validation Loss: 18.949134826660156\n",
      "Epoch 600/10000, Training Loss: 24.213924407958984, Validation Loss: 26.98978614807129\n",
      "Epoch 601/10000, Training Loss: 20.602874755859375, Validation Loss: 34.56121826171875\n",
      "Epoch 602/10000, Training Loss: 17.206565856933594, Validation Loss: 30.710617065429688\n",
      "Epoch 603/10000, Training Loss: 24.424610137939453, Validation Loss: 26.14872169494629\n",
      "Epoch 604/10000, Training Loss: 37.058998107910156, Validation Loss: 22.469438552856445\n",
      "Epoch 605/10000, Training Loss: 14.68775749206543, Validation Loss: 35.957759857177734\n",
      "Epoch 606/10000, Training Loss: 23.718883514404297, Validation Loss: 2.6746180057525635\n",
      "Epoch 607/10000, Training Loss: 14.156304359436035, Validation Loss: 7.64484977722168\n",
      "Epoch 608/10000, Training Loss: 15.639965057373047, Validation Loss: 48.17558288574219\n",
      "Epoch 609/10000, Training Loss: 25.497072219848633, Validation Loss: 25.2818546295166\n",
      "Epoch 610/10000, Training Loss: 19.35873794555664, Validation Loss: 42.464054107666016\n",
      "Epoch 611/10000, Training Loss: 31.411489486694336, Validation Loss: 21.91420555114746\n",
      "Epoch 612/10000, Training Loss: 22.227174758911133, Validation Loss: 15.556086540222168\n",
      "Epoch 613/10000, Training Loss: 13.629215240478516, Validation Loss: 18.38407325744629\n",
      "Epoch 614/10000, Training Loss: 19.895048141479492, Validation Loss: 48.52012634277344\n",
      "Epoch 615/10000, Training Loss: 38.84053039550781, Validation Loss: 2.829800844192505\n",
      "Epoch 616/10000, Training Loss: 16.473390579223633, Validation Loss: 23.571043014526367\n",
      "Epoch 617/10000, Training Loss: 13.084470748901367, Validation Loss: 5.849719524383545\n",
      "Epoch 618/10000, Training Loss: 18.594501495361328, Validation Loss: 8.427840232849121\n",
      "Epoch 619/10000, Training Loss: 13.813332557678223, Validation Loss: 2.3660082817077637\n",
      "Epoch 620/10000, Training Loss: 17.41448211669922, Validation Loss: 28.857206344604492\n",
      "Epoch 621/10000, Training Loss: 19.06917953491211, Validation Loss: 13.226211547851562\n",
      "Epoch 622/10000, Training Loss: 42.612728118896484, Validation Loss: 22.5715274810791\n",
      "Epoch 623/10000, Training Loss: 33.329288482666016, Validation Loss: 7.893711090087891\n",
      "Epoch 624/10000, Training Loss: 13.388413429260254, Validation Loss: 7.558469772338867\n",
      "Epoch 625/10000, Training Loss: 22.367713928222656, Validation Loss: 23.236398696899414\n",
      "Epoch 626/10000, Training Loss: 32.853271484375, Validation Loss: 31.89063262939453\n",
      "Epoch 627/10000, Training Loss: 26.801536560058594, Validation Loss: 53.7322883605957\n",
      "Epoch 628/10000, Training Loss: 30.24362564086914, Validation Loss: 23.470746994018555\n",
      "Epoch 629/10000, Training Loss: 22.962188720703125, Validation Loss: 20.79462242126465\n",
      "Epoch 630/10000, Training Loss: 19.314071655273438, Validation Loss: 18.680505752563477\n",
      "Epoch 631/10000, Training Loss: 14.86687183380127, Validation Loss: 12.086348533630371\n",
      "Epoch 632/10000, Training Loss: 28.606807708740234, Validation Loss: 52.73091506958008\n",
      "Epoch 633/10000, Training Loss: 27.54676628112793, Validation Loss: 8.374293327331543\n",
      "Epoch 634/10000, Training Loss: 24.266666412353516, Validation Loss: 22.2401180267334\n",
      "Epoch 635/10000, Training Loss: 16.507020950317383, Validation Loss: 23.98368263244629\n",
      "Epoch 636/10000, Training Loss: 21.652118682861328, Validation Loss: 31.47174644470215\n",
      "Epoch 637/10000, Training Loss: 8.163694381713867, Validation Loss: 13.570037841796875\n",
      "Epoch 638/10000, Training Loss: 23.932844161987305, Validation Loss: 5.742525577545166\n",
      "Epoch 639/10000, Training Loss: 10.872404098510742, Validation Loss: 30.415590286254883\n",
      "Epoch 640/10000, Training Loss: 24.288982391357422, Validation Loss: 36.718502044677734\n",
      "Epoch 641/10000, Training Loss: 20.71665382385254, Validation Loss: 1.844901442527771\n",
      "Epoch 642/10000, Training Loss: 27.725553512573242, Validation Loss: 23.68108558654785\n",
      "Epoch 643/10000, Training Loss: 14.504533767700195, Validation Loss: 41.72285079956055\n",
      "Epoch 644/10000, Training Loss: 22.546649932861328, Validation Loss: 36.72015380859375\n",
      "Epoch 645/10000, Training Loss: 21.67629623413086, Validation Loss: 74.24491119384766\n",
      "Epoch 646/10000, Training Loss: 16.077838897705078, Validation Loss: 4.6858086585998535\n",
      "Epoch 647/10000, Training Loss: 25.912105560302734, Validation Loss: 45.2540283203125\n",
      "Epoch 648/10000, Training Loss: 12.662276268005371, Validation Loss: 19.340269088745117\n",
      "Epoch 649/10000, Training Loss: 14.642268180847168, Validation Loss: 20.67021369934082\n",
      "Epoch 650/10000, Training Loss: 20.359760284423828, Validation Loss: 18.86382293701172\n",
      "Epoch 651/10000, Training Loss: 9.33938980102539, Validation Loss: 9.912196159362793\n",
      "Epoch 652/10000, Training Loss: 20.367826461791992, Validation Loss: 45.48733901977539\n",
      "Epoch 653/10000, Training Loss: 17.980363845825195, Validation Loss: 95.66861724853516\n",
      "Epoch 654/10000, Training Loss: 10.254632949829102, Validation Loss: 15.075640678405762\n",
      "Epoch 655/10000, Training Loss: 26.248485565185547, Validation Loss: 11.169356346130371\n",
      "Epoch 656/10000, Training Loss: 20.967212677001953, Validation Loss: 17.672489166259766\n",
      "Epoch 657/10000, Training Loss: 10.310171127319336, Validation Loss: 51.09040832519531\n",
      "Epoch 658/10000, Training Loss: 19.718210220336914, Validation Loss: 13.806021690368652\n",
      "Epoch 659/10000, Training Loss: 15.883686065673828, Validation Loss: 5.367149353027344\n",
      "Epoch 660/10000, Training Loss: 12.496485710144043, Validation Loss: 8.636786460876465\n",
      "Epoch 661/10000, Training Loss: 11.478292465209961, Validation Loss: 1.3071843385696411\n",
      "Epoch 662/10000, Training Loss: 29.529197692871094, Validation Loss: 37.178466796875\n",
      "Epoch 663/10000, Training Loss: 7.421414375305176, Validation Loss: 5.900451183319092\n",
      "Epoch 664/10000, Training Loss: 12.317249298095703, Validation Loss: 36.37631607055664\n",
      "Epoch 665/10000, Training Loss: 11.469720840454102, Validation Loss: 0.26328742504119873\n",
      "Epoch 666/10000, Training Loss: 16.45899200439453, Validation Loss: 10.558379173278809\n",
      "Epoch 667/10000, Training Loss: 28.093257904052734, Validation Loss: 8.897998809814453\n",
      "Epoch 668/10000, Training Loss: 16.533517837524414, Validation Loss: 20.89486312866211\n",
      "Epoch 669/10000, Training Loss: 20.358619689941406, Validation Loss: 11.786750793457031\n",
      "Epoch 670/10000, Training Loss: 26.14011001586914, Validation Loss: 15.991448402404785\n",
      "Epoch 671/10000, Training Loss: 12.887704849243164, Validation Loss: 5.882183074951172\n",
      "Epoch 672/10000, Training Loss: 25.848167419433594, Validation Loss: 34.92204666137695\n",
      "Epoch 673/10000, Training Loss: 13.583982467651367, Validation Loss: 17.19723892211914\n",
      "Epoch 674/10000, Training Loss: 21.75908088684082, Validation Loss: 8.133252143859863\n",
      "Epoch 675/10000, Training Loss: 12.55770206451416, Validation Loss: 2.7137668132781982\n",
      "Epoch 676/10000, Training Loss: 12.659723281860352, Validation Loss: 12.14028549194336\n",
      "Epoch 677/10000, Training Loss: 19.26482391357422, Validation Loss: 69.13262939453125\n",
      "Epoch 678/10000, Training Loss: 19.37674331665039, Validation Loss: 1.9426459074020386\n",
      "Epoch 679/10000, Training Loss: 20.55878448486328, Validation Loss: 44.16660690307617\n",
      "Epoch 680/10000, Training Loss: 15.402334213256836, Validation Loss: 8.887173652648926\n",
      "Epoch 681/10000, Training Loss: 10.95250415802002, Validation Loss: 13.73433780670166\n",
      "Epoch 682/10000, Training Loss: 16.29305648803711, Validation Loss: 16.818923950195312\n",
      "Epoch 683/10000, Training Loss: 19.229557037353516, Validation Loss: 29.072999954223633\n",
      "Epoch 684/10000, Training Loss: 29.708972930908203, Validation Loss: 78.14726257324219\n",
      "Epoch 685/10000, Training Loss: 16.045108795166016, Validation Loss: 15.82153034210205\n",
      "Epoch 686/10000, Training Loss: 21.555604934692383, Validation Loss: 26.162841796875\n",
      "Epoch 687/10000, Training Loss: 38.54850387573242, Validation Loss: 14.327054023742676\n",
      "Epoch 688/10000, Training Loss: 22.268423080444336, Validation Loss: 19.369150161743164\n",
      "Epoch 689/10000, Training Loss: 30.75965118408203, Validation Loss: 149.03492736816406\n",
      "Epoch 690/10000, Training Loss: 28.34139633178711, Validation Loss: 15.482281684875488\n",
      "Epoch 691/10000, Training Loss: 48.47312927246094, Validation Loss: 12.513518333435059\n",
      "Epoch 692/10000, Training Loss: 12.618561744689941, Validation Loss: 5.2478203773498535\n",
      "Epoch 693/10000, Training Loss: 16.30124282836914, Validation Loss: 14.774646759033203\n",
      "Epoch 694/10000, Training Loss: 18.392610549926758, Validation Loss: 15.922477722167969\n",
      "Epoch 695/10000, Training Loss: 12.056565284729004, Validation Loss: 6.371466159820557\n",
      "Epoch 696/10000, Training Loss: 14.644638061523438, Validation Loss: 39.01388931274414\n",
      "Epoch 697/10000, Training Loss: 25.62057876586914, Validation Loss: 11.756556510925293\n",
      "Epoch 698/10000, Training Loss: 39.06948471069336, Validation Loss: 117.8770523071289\n",
      "Epoch 699/10000, Training Loss: 13.352420806884766, Validation Loss: 5.707080841064453\n",
      "Epoch 700/10000, Training Loss: 21.28675651550293, Validation Loss: 26.20406150817871\n",
      "Epoch 701/10000, Training Loss: 35.11943817138672, Validation Loss: 33.93741989135742\n",
      "Epoch 702/10000, Training Loss: 18.450870513916016, Validation Loss: 36.456600189208984\n",
      "Epoch 703/10000, Training Loss: 18.23324203491211, Validation Loss: 16.68541145324707\n",
      "Epoch 704/10000, Training Loss: 17.463417053222656, Validation Loss: 21.461755752563477\n",
      "Epoch 705/10000, Training Loss: 25.19859504699707, Validation Loss: 23.803537368774414\n",
      "Epoch 706/10000, Training Loss: 9.945786476135254, Validation Loss: 5.118772029876709\n",
      "Epoch 707/10000, Training Loss: 13.83308219909668, Validation Loss: 22.9687442779541\n",
      "Epoch 708/10000, Training Loss: 23.087242126464844, Validation Loss: 4.599815845489502\n",
      "Epoch 709/10000, Training Loss: 13.691448211669922, Validation Loss: 6.4870171546936035\n",
      "Epoch 710/10000, Training Loss: 22.96343994140625, Validation Loss: 263.9547424316406\n",
      "Epoch 711/10000, Training Loss: 19.289060592651367, Validation Loss: 35.61627197265625\n",
      "Epoch 712/10000, Training Loss: 13.484987258911133, Validation Loss: 3.051093101501465\n",
      "Epoch 713/10000, Training Loss: 13.275754928588867, Validation Loss: 21.739341735839844\n",
      "Epoch 714/10000, Training Loss: 13.989022254943848, Validation Loss: 26.182607650756836\n",
      "Epoch 715/10000, Training Loss: 15.549091339111328, Validation Loss: 2.3513948917388916\n",
      "Epoch 716/10000, Training Loss: 38.386837005615234, Validation Loss: 75.54798126220703\n",
      "Epoch 717/10000, Training Loss: 15.234003067016602, Validation Loss: 21.434799194335938\n",
      "Epoch 718/10000, Training Loss: 64.78919982910156, Validation Loss: 108.00545501708984\n",
      "Epoch 719/10000, Training Loss: 13.41888427734375, Validation Loss: 2.068943738937378\n",
      "Epoch 720/10000, Training Loss: 20.142601013183594, Validation Loss: 13.630915641784668\n",
      "Epoch 721/10000, Training Loss: 10.259922981262207, Validation Loss: 12.957763671875\n",
      "Epoch 722/10000, Training Loss: 21.891202926635742, Validation Loss: 58.77333450317383\n",
      "Epoch 723/10000, Training Loss: 12.290090560913086, Validation Loss: 5.173436164855957\n",
      "Epoch 724/10000, Training Loss: 17.31397247314453, Validation Loss: 2.989689588546753\n",
      "Epoch 725/10000, Training Loss: 26.306550979614258, Validation Loss: 17.63309097290039\n",
      "Epoch 726/10000, Training Loss: 14.758056640625, Validation Loss: 12.503178596496582\n",
      "Epoch 727/10000, Training Loss: 12.52981948852539, Validation Loss: 5.936318874359131\n",
      "Epoch 728/10000, Training Loss: 20.30866813659668, Validation Loss: 183.7076416015625\n",
      "Epoch 729/10000, Training Loss: 19.480846405029297, Validation Loss: 57.90716552734375\n",
      "Epoch 730/10000, Training Loss: 18.15882110595703, Validation Loss: 17.207136154174805\n",
      "Epoch 731/10000, Training Loss: 17.89531707763672, Validation Loss: 47.28095626831055\n",
      "Epoch 732/10000, Training Loss: 20.880647659301758, Validation Loss: 27.366012573242188\n",
      "Epoch 733/10000, Training Loss: 27.76441192626953, Validation Loss: 25.745405197143555\n",
      "Epoch 734/10000, Training Loss: 20.704286575317383, Validation Loss: 6.930271625518799\n",
      "Epoch 735/10000, Training Loss: 14.792234420776367, Validation Loss: 9.712827682495117\n",
      "Epoch 736/10000, Training Loss: 25.529876708984375, Validation Loss: 11.636043548583984\n",
      "Epoch 737/10000, Training Loss: 10.28647518157959, Validation Loss: 8.598746299743652\n",
      "Epoch 738/10000, Training Loss: 5.769920825958252, Validation Loss: 2.29036021232605\n",
      "Epoch 739/10000, Training Loss: 18.296369552612305, Validation Loss: 41.682838439941406\n",
      "Epoch 740/10000, Training Loss: 16.804574966430664, Validation Loss: 11.445568084716797\n",
      "Epoch 741/10000, Training Loss: 20.830116271972656, Validation Loss: 15.193901062011719\n",
      "Epoch 742/10000, Training Loss: 28.481578826904297, Validation Loss: 10.555214881896973\n",
      "Epoch 743/10000, Training Loss: 14.864659309387207, Validation Loss: 3.380072832107544\n",
      "Epoch 744/10000, Training Loss: 9.647296905517578, Validation Loss: 3.82966685295105\n",
      "Epoch 745/10000, Training Loss: 15.47386646270752, Validation Loss: 7.735328197479248\n",
      "Epoch 746/10000, Training Loss: 20.845306396484375, Validation Loss: 14.852755546569824\n",
      "Epoch 747/10000, Training Loss: 59.92259216308594, Validation Loss: 19.59054183959961\n",
      "Epoch 748/10000, Training Loss: 17.62904167175293, Validation Loss: 2.0818707942962646\n",
      "Epoch 749/10000, Training Loss: 13.092184066772461, Validation Loss: 17.665756225585938\n",
      "Epoch 750/10000, Training Loss: 23.78626251220703, Validation Loss: 39.33197021484375\n",
      "Epoch 751/10000, Training Loss: 18.086688995361328, Validation Loss: 26.941024780273438\n",
      "Epoch 752/10000, Training Loss: 12.574420928955078, Validation Loss: 2.4190495014190674\n",
      "Epoch 753/10000, Training Loss: 8.575142860412598, Validation Loss: 8.960221290588379\n",
      "Epoch 754/10000, Training Loss: 15.783949851989746, Validation Loss: 51.218143463134766\n",
      "Epoch 755/10000, Training Loss: 14.865457534790039, Validation Loss: 17.426895141601562\n",
      "Epoch 756/10000, Training Loss: 15.196105003356934, Validation Loss: 11.731724739074707\n",
      "Epoch 757/10000, Training Loss: 28.085935592651367, Validation Loss: 21.2662296295166\n",
      "Epoch 758/10000, Training Loss: 13.815018653869629, Validation Loss: 26.224029541015625\n",
      "Epoch 759/10000, Training Loss: 25.40689468383789, Validation Loss: 32.4924201965332\n",
      "Epoch 760/10000, Training Loss: 17.041858673095703, Validation Loss: 13.69083309173584\n",
      "Epoch 761/10000, Training Loss: 23.08298110961914, Validation Loss: 122.39712524414062\n",
      "Epoch 762/10000, Training Loss: 18.7417049407959, Validation Loss: 12.84902286529541\n",
      "Epoch 763/10000, Training Loss: 14.657779693603516, Validation Loss: 12.374201774597168\n",
      "Epoch 764/10000, Training Loss: 14.924013137817383, Validation Loss: 7.510018825531006\n",
      "Epoch 765/10000, Training Loss: 10.990422248840332, Validation Loss: 1.8595691919326782\n",
      "Epoch 766/10000, Training Loss: 13.213912963867188, Validation Loss: 10.019072532653809\n",
      "Epoch 767/10000, Training Loss: 21.385784149169922, Validation Loss: 14.029475212097168\n",
      "Epoch 768/10000, Training Loss: 17.04456901550293, Validation Loss: 37.99251937866211\n",
      "Epoch 769/10000, Training Loss: 20.044700622558594, Validation Loss: 27.450973510742188\n",
      "Epoch 770/10000, Training Loss: 30.344207763671875, Validation Loss: 116.8545913696289\n",
      "Epoch 771/10000, Training Loss: 15.829106330871582, Validation Loss: 24.744766235351562\n",
      "Epoch 772/10000, Training Loss: 20.119035720825195, Validation Loss: 19.277603149414062\n",
      "Epoch 773/10000, Training Loss: 22.240516662597656, Validation Loss: 25.274452209472656\n",
      "Epoch 774/10000, Training Loss: 12.570314407348633, Validation Loss: 27.23110008239746\n",
      "Epoch 775/10000, Training Loss: 14.273660659790039, Validation Loss: 91.77880096435547\n",
      "Epoch 776/10000, Training Loss: 22.397493362426758, Validation Loss: 7.72804594039917\n",
      "Epoch 777/10000, Training Loss: 12.639713287353516, Validation Loss: 32.819278717041016\n",
      "Epoch 778/10000, Training Loss: 16.902280807495117, Validation Loss: 6.618921279907227\n",
      "Epoch 779/10000, Training Loss: 17.236164093017578, Validation Loss: 13.51335620880127\n",
      "Epoch 780/10000, Training Loss: 11.056550025939941, Validation Loss: 8.200746536254883\n",
      "Epoch 781/10000, Training Loss: 11.322994232177734, Validation Loss: 5.5645222663879395\n",
      "Epoch 782/10000, Training Loss: 15.74658203125, Validation Loss: 6.495176792144775\n",
      "Epoch 783/10000, Training Loss: 18.231842041015625, Validation Loss: 10.067453384399414\n",
      "Epoch 784/10000, Training Loss: 33.540863037109375, Validation Loss: 23.5588436126709\n",
      "Epoch 785/10000, Training Loss: 15.049367904663086, Validation Loss: 19.794891357421875\n",
      "Epoch 786/10000, Training Loss: 23.75719451904297, Validation Loss: 36.69786071777344\n",
      "Epoch 787/10000, Training Loss: 21.783823013305664, Validation Loss: 68.78050994873047\n",
      "Epoch 788/10000, Training Loss: 21.162078857421875, Validation Loss: 7.816450119018555\n",
      "Epoch 789/10000, Training Loss: 16.562313079833984, Validation Loss: 21.80259895324707\n",
      "Epoch 790/10000, Training Loss: 25.89525032043457, Validation Loss: 4.84151029586792\n",
      "Epoch 791/10000, Training Loss: 13.250585556030273, Validation Loss: 22.08970832824707\n",
      "Epoch 792/10000, Training Loss: 15.089487075805664, Validation Loss: 8.725433349609375\n",
      "Epoch 793/10000, Training Loss: 11.958429336547852, Validation Loss: 4.850296974182129\n",
      "Epoch 794/10000, Training Loss: 17.627098083496094, Validation Loss: 11.803828239440918\n",
      "Epoch 795/10000, Training Loss: 10.493677139282227, Validation Loss: 13.253856658935547\n",
      "Epoch 796/10000, Training Loss: 13.723186492919922, Validation Loss: 11.328473091125488\n",
      "Epoch 797/10000, Training Loss: 11.032611846923828, Validation Loss: 23.227859497070312\n",
      "Epoch 798/10000, Training Loss: 30.711036682128906, Validation Loss: 16.77794647216797\n",
      "Epoch 799/10000, Training Loss: 31.104949951171875, Validation Loss: 13.498509407043457\n",
      "Epoch 800/10000, Training Loss: 8.364788055419922, Validation Loss: 7.784425735473633\n",
      "Epoch 801/10000, Training Loss: 27.631778717041016, Validation Loss: 1.9776495695114136\n",
      "Epoch 802/10000, Training Loss: 30.478519439697266, Validation Loss: 3.1690075397491455\n",
      "Epoch 803/10000, Training Loss: 42.0059814453125, Validation Loss: 27.41523551940918\n",
      "Epoch 804/10000, Training Loss: 11.5574951171875, Validation Loss: 18.327621459960938\n",
      "Epoch 805/10000, Training Loss: 15.894135475158691, Validation Loss: 2.313448429107666\n",
      "Epoch 806/10000, Training Loss: 10.313651084899902, Validation Loss: 23.204416275024414\n",
      "Epoch 807/10000, Training Loss: 9.869063377380371, Validation Loss: 10.134678840637207\n",
      "Epoch 808/10000, Training Loss: 18.033071517944336, Validation Loss: 7.4848952293396\n",
      "Epoch 809/10000, Training Loss: 19.48748016357422, Validation Loss: 4.2667083740234375\n",
      "Epoch 810/10000, Training Loss: 11.967996597290039, Validation Loss: 18.496557235717773\n",
      "Epoch 811/10000, Training Loss: 16.297639846801758, Validation Loss: 5.342073917388916\n",
      "Epoch 812/10000, Training Loss: 14.732542991638184, Validation Loss: 13.508430480957031\n",
      "Epoch 813/10000, Training Loss: 16.623931884765625, Validation Loss: 11.367851257324219\n",
      "Epoch 814/10000, Training Loss: 29.019582748413086, Validation Loss: 8.547953605651855\n",
      "Epoch 815/10000, Training Loss: 25.498472213745117, Validation Loss: 2.9485738277435303\n",
      "Epoch 816/10000, Training Loss: 10.974305152893066, Validation Loss: 21.338815689086914\n",
      "Epoch 817/10000, Training Loss: 21.34803009033203, Validation Loss: 35.41783905029297\n",
      "Epoch 818/10000, Training Loss: 20.243574142456055, Validation Loss: 8.59155559539795\n",
      "Epoch 819/10000, Training Loss: 12.584249496459961, Validation Loss: 24.82268714904785\n",
      "Epoch 820/10000, Training Loss: 10.74564266204834, Validation Loss: 12.950711250305176\n",
      "Epoch 821/10000, Training Loss: 19.342912673950195, Validation Loss: 23.40215492248535\n",
      "Epoch 822/10000, Training Loss: 19.305810928344727, Validation Loss: 28.41324806213379\n",
      "Epoch 823/10000, Training Loss: 19.07413673400879, Validation Loss: 21.09281349182129\n",
      "Epoch 824/10000, Training Loss: 23.846628189086914, Validation Loss: 59.97541427612305\n",
      "Epoch 825/10000, Training Loss: 10.80317211151123, Validation Loss: 9.72549057006836\n",
      "Epoch 826/10000, Training Loss: 12.977089881896973, Validation Loss: 16.795557022094727\n",
      "Epoch 827/10000, Training Loss: 16.738733291625977, Validation Loss: 21.54901885986328\n",
      "Epoch 828/10000, Training Loss: 19.051755905151367, Validation Loss: 9.707208633422852\n",
      "Epoch 829/10000, Training Loss: 14.617087364196777, Validation Loss: 16.64522361755371\n",
      "Epoch 830/10000, Training Loss: 35.56332015991211, Validation Loss: 0.972588837146759\n",
      "Epoch 831/10000, Training Loss: 41.440956115722656, Validation Loss: 43.42256164550781\n",
      "Epoch 832/10000, Training Loss: 17.603200912475586, Validation Loss: 15.285452842712402\n",
      "Epoch 833/10000, Training Loss: 28.316049575805664, Validation Loss: 34.64357376098633\n",
      "Epoch 834/10000, Training Loss: 23.61515998840332, Validation Loss: 6.293519973754883\n",
      "Epoch 835/10000, Training Loss: 15.755491256713867, Validation Loss: 16.539674758911133\n",
      "Epoch 836/10000, Training Loss: 12.503311157226562, Validation Loss: 8.064634323120117\n",
      "Epoch 837/10000, Training Loss: 9.73713207244873, Validation Loss: 11.892345428466797\n",
      "Epoch 838/10000, Training Loss: 11.926804542541504, Validation Loss: 23.625085830688477\n",
      "Epoch 839/10000, Training Loss: 19.00572967529297, Validation Loss: 16.663854598999023\n",
      "Epoch 840/10000, Training Loss: 14.264589309692383, Validation Loss: 13.580577850341797\n",
      "Epoch 841/10000, Training Loss: 11.773225784301758, Validation Loss: 3.404320001602173\n",
      "Epoch 842/10000, Training Loss: 12.554076194763184, Validation Loss: 8.912135124206543\n",
      "Epoch 843/10000, Training Loss: 18.003219604492188, Validation Loss: 22.997528076171875\n",
      "Epoch 844/10000, Training Loss: 31.04572296142578, Validation Loss: 96.61141204833984\n",
      "Epoch 845/10000, Training Loss: 22.96204376220703, Validation Loss: 12.247069358825684\n",
      "Epoch 846/10000, Training Loss: 10.040252685546875, Validation Loss: 8.325879096984863\n",
      "Epoch 847/10000, Training Loss: 17.82685661315918, Validation Loss: 7.192559719085693\n",
      "Epoch 848/10000, Training Loss: 13.64814567565918, Validation Loss: 1.943613052368164\n",
      "Epoch 849/10000, Training Loss: 13.576203346252441, Validation Loss: 15.7489595413208\n",
      "Epoch 850/10000, Training Loss: 23.69568634033203, Validation Loss: 5.675779819488525\n",
      "Epoch 851/10000, Training Loss: 11.965262413024902, Validation Loss: 6.691789627075195\n",
      "Epoch 852/10000, Training Loss: 34.66124725341797, Validation Loss: 20.47743034362793\n",
      "Epoch 853/10000, Training Loss: 19.94373893737793, Validation Loss: 2.6646106243133545\n",
      "Epoch 854/10000, Training Loss: 10.092119216918945, Validation Loss: 21.522218704223633\n",
      "Epoch 855/10000, Training Loss: 20.971370697021484, Validation Loss: 7.242432117462158\n",
      "Epoch 856/10000, Training Loss: 19.783267974853516, Validation Loss: 1.3539577722549438\n",
      "Epoch 857/10000, Training Loss: 11.615241050720215, Validation Loss: 17.477697372436523\n",
      "Epoch 858/10000, Training Loss: 14.995108604431152, Validation Loss: 9.93302059173584\n",
      "Epoch 859/10000, Training Loss: 14.578536033630371, Validation Loss: 3.622706651687622\n",
      "Epoch 860/10000, Training Loss: 11.983285903930664, Validation Loss: 33.91765213012695\n",
      "Epoch 861/10000, Training Loss: 17.578168869018555, Validation Loss: 3.0493743419647217\n",
      "Epoch 862/10000, Training Loss: 15.955411911010742, Validation Loss: 20.47002601623535\n",
      "Epoch 863/10000, Training Loss: 11.522324562072754, Validation Loss: 4.130401134490967\n",
      "Epoch 864/10000, Training Loss: 15.411663055419922, Validation Loss: 5.64387845993042\n",
      "Epoch 865/10000, Training Loss: 10.132877349853516, Validation Loss: 5.971738338470459\n",
      "Epoch 866/10000, Training Loss: 12.297107696533203, Validation Loss: 6.541500568389893\n",
      "Epoch 867/10000, Training Loss: 19.264549255371094, Validation Loss: 9.395380020141602\n",
      "Epoch 868/10000, Training Loss: 17.862354278564453, Validation Loss: 9.076169967651367\n",
      "Epoch 869/10000, Training Loss: 11.267430305480957, Validation Loss: 5.943143367767334\n",
      "Epoch 870/10000, Training Loss: 14.250995635986328, Validation Loss: 10.571768760681152\n",
      "Epoch 871/10000, Training Loss: 14.282285690307617, Validation Loss: 18.740022659301758\n",
      "Epoch 872/10000, Training Loss: 11.295623779296875, Validation Loss: 13.791787147521973\n",
      "Epoch 873/10000, Training Loss: 22.002647399902344, Validation Loss: 25.80853271484375\n",
      "Epoch 874/10000, Training Loss: 19.54500961303711, Validation Loss: 14.817255973815918\n",
      "Epoch 875/10000, Training Loss: 17.862398147583008, Validation Loss: 9.098530769348145\n",
      "Epoch 876/10000, Training Loss: 11.879758834838867, Validation Loss: 24.506933212280273\n",
      "Epoch 877/10000, Training Loss: 9.500914573669434, Validation Loss: 0.9807769656181335\n",
      "Epoch 878/10000, Training Loss: 14.154905319213867, Validation Loss: 17.42731285095215\n",
      "Epoch 879/10000, Training Loss: 14.903261184692383, Validation Loss: 40.173439025878906\n",
      "Epoch 880/10000, Training Loss: 10.81794548034668, Validation Loss: 21.58354377746582\n",
      "Epoch 881/10000, Training Loss: 11.229386329650879, Validation Loss: 6.3593058586120605\n",
      "Epoch 882/10000, Training Loss: 29.193147659301758, Validation Loss: 29.315866470336914\n",
      "Epoch 883/10000, Training Loss: 13.383614540100098, Validation Loss: 7.678794860839844\n",
      "Epoch 884/10000, Training Loss: 32.524051666259766, Validation Loss: 21.179174423217773\n",
      "Epoch 885/10000, Training Loss: 14.035843849182129, Validation Loss: 14.41878890991211\n",
      "Epoch 886/10000, Training Loss: 5.944979190826416, Validation Loss: 5.311464309692383\n",
      "Epoch 887/10000, Training Loss: 12.885125160217285, Validation Loss: 11.921753883361816\n",
      "Epoch 888/10000, Training Loss: 11.691844940185547, Validation Loss: 5.626121520996094\n",
      "Epoch 889/10000, Training Loss: 18.30989646911621, Validation Loss: 9.572759628295898\n",
      "Epoch 890/10000, Training Loss: 11.157614707946777, Validation Loss: 21.317245483398438\n",
      "Epoch 891/10000, Training Loss: 35.354156494140625, Validation Loss: 53.929534912109375\n",
      "Epoch 892/10000, Training Loss: 7.381195068359375, Validation Loss: 32.86650848388672\n",
      "Epoch 893/10000, Training Loss: 21.18631935119629, Validation Loss: 4.811310291290283\n",
      "Epoch 894/10000, Training Loss: 15.018884658813477, Validation Loss: 16.404083251953125\n",
      "Epoch 895/10000, Training Loss: 7.377866268157959, Validation Loss: 12.76581859588623\n",
      "Epoch 896/10000, Training Loss: 27.091060638427734, Validation Loss: 51.933624267578125\n",
      "Epoch 897/10000, Training Loss: 16.82101821899414, Validation Loss: 29.40595245361328\n",
      "Epoch 898/10000, Training Loss: 9.22446060180664, Validation Loss: 0.9737383723258972\n",
      "Epoch 899/10000, Training Loss: 16.56786346435547, Validation Loss: 6.823113918304443\n",
      "Epoch 900/10000, Training Loss: 7.763199806213379, Validation Loss: 22.219213485717773\n",
      "Epoch 901/10000, Training Loss: 14.891157150268555, Validation Loss: 6.872857570648193\n",
      "Epoch 902/10000, Training Loss: 22.722572326660156, Validation Loss: 7.25504732131958\n",
      "Epoch 903/10000, Training Loss: 7.418493270874023, Validation Loss: 6.002273082733154\n",
      "Epoch 904/10000, Training Loss: 8.306462287902832, Validation Loss: 4.788970947265625\n",
      "Epoch 905/10000, Training Loss: 10.627368927001953, Validation Loss: 7.730325222015381\n",
      "Epoch 906/10000, Training Loss: 15.96165943145752, Validation Loss: 11.899677276611328\n",
      "Epoch 907/10000, Training Loss: 17.33864402770996, Validation Loss: 19.00215721130371\n",
      "Epoch 908/10000, Training Loss: 20.542612075805664, Validation Loss: 18.95380973815918\n",
      "Epoch 909/10000, Training Loss: 7.20930814743042, Validation Loss: 4.220147609710693\n",
      "Epoch 910/10000, Training Loss: 11.482914924621582, Validation Loss: 2.28100323677063\n",
      "Epoch 911/10000, Training Loss: 9.487750053405762, Validation Loss: 10.903800010681152\n",
      "Epoch 912/10000, Training Loss: 6.356132984161377, Validation Loss: 9.7083158493042\n",
      "Epoch 913/10000, Training Loss: 11.316132545471191, Validation Loss: 10.0339937210083\n",
      "Epoch 914/10000, Training Loss: 11.853652000427246, Validation Loss: 76.03409576416016\n",
      "Epoch 915/10000, Training Loss: 10.086775779724121, Validation Loss: 29.56818962097168\n",
      "Epoch 916/10000, Training Loss: 14.369467735290527, Validation Loss: 2.798640251159668\n",
      "Epoch 917/10000, Training Loss: 17.020328521728516, Validation Loss: 16.622167587280273\n",
      "Epoch 918/10000, Training Loss: 12.749872207641602, Validation Loss: 26.770492553710938\n",
      "Epoch 919/10000, Training Loss: 17.211977005004883, Validation Loss: 42.67796325683594\n",
      "Epoch 920/10000, Training Loss: 15.267289161682129, Validation Loss: 8.144270896911621\n",
      "Epoch 921/10000, Training Loss: 12.90998649597168, Validation Loss: 8.908469200134277\n",
      "Epoch 922/10000, Training Loss: 11.164582252502441, Validation Loss: 16.79377555847168\n",
      "Epoch 923/10000, Training Loss: 20.90119171142578, Validation Loss: 14.636699676513672\n",
      "Epoch 924/10000, Training Loss: 8.19731616973877, Validation Loss: 16.76273536682129\n",
      "Epoch 925/10000, Training Loss: 21.95616912841797, Validation Loss: 24.01055908203125\n",
      "Epoch 926/10000, Training Loss: 11.480137825012207, Validation Loss: 38.955955505371094\n",
      "Epoch 927/10000, Training Loss: 30.255250930786133, Validation Loss: 35.46854019165039\n",
      "Epoch 928/10000, Training Loss: 18.087446212768555, Validation Loss: 19.00420570373535\n",
      "Epoch 929/10000, Training Loss: 14.028858184814453, Validation Loss: 11.550782203674316\n",
      "Epoch 930/10000, Training Loss: 12.345208168029785, Validation Loss: 15.26435375213623\n",
      "Epoch 931/10000, Training Loss: 11.60204029083252, Validation Loss: 9.550427436828613\n",
      "Epoch 932/10000, Training Loss: 11.294001579284668, Validation Loss: 9.401885032653809\n",
      "Epoch 933/10000, Training Loss: 15.156804084777832, Validation Loss: 12.142375946044922\n",
      "Epoch 934/10000, Training Loss: 14.42714786529541, Validation Loss: 17.367971420288086\n",
      "Epoch 935/10000, Training Loss: 16.87160873413086, Validation Loss: 4.104565143585205\n",
      "Epoch 936/10000, Training Loss: 18.85108184814453, Validation Loss: 28.308347702026367\n",
      "Epoch 937/10000, Training Loss: 13.941133499145508, Validation Loss: 11.548298835754395\n",
      "Epoch 938/10000, Training Loss: 15.862483024597168, Validation Loss: 23.951995849609375\n",
      "Epoch 939/10000, Training Loss: 13.762893676757812, Validation Loss: 0.9250606894493103\n",
      "Epoch 940/10000, Training Loss: 17.235347747802734, Validation Loss: 6.901962757110596\n",
      "Epoch 941/10000, Training Loss: 16.858386993408203, Validation Loss: 13.862763404846191\n",
      "Epoch 942/10000, Training Loss: 8.825948715209961, Validation Loss: 8.018230438232422\n",
      "Epoch 943/10000, Training Loss: 12.58879280090332, Validation Loss: 28.10419464111328\n",
      "Epoch 944/10000, Training Loss: 10.524603843688965, Validation Loss: 11.69806957244873\n",
      "Epoch 945/10000, Training Loss: 9.08340835571289, Validation Loss: 19.2546329498291\n",
      "Epoch 946/10000, Training Loss: 24.044151306152344, Validation Loss: 9.948226928710938\n",
      "Epoch 947/10000, Training Loss: 12.16781997680664, Validation Loss: 4.029819488525391\n",
      "Epoch 948/10000, Training Loss: 20.793806076049805, Validation Loss: 36.30526351928711\n",
      "Epoch 949/10000, Training Loss: 11.7718505859375, Validation Loss: 65.85985565185547\n",
      "Epoch 950/10000, Training Loss: 13.082422256469727, Validation Loss: 14.650215148925781\n",
      "Epoch 951/10000, Training Loss: 15.308063507080078, Validation Loss: 29.914480209350586\n",
      "Epoch 952/10000, Training Loss: 16.556018829345703, Validation Loss: 38.03965377807617\n",
      "Epoch 953/10000, Training Loss: 9.257820129394531, Validation Loss: 12.27226734161377\n",
      "Epoch 954/10000, Training Loss: 13.837114334106445, Validation Loss: 8.347006797790527\n",
      "Epoch 955/10000, Training Loss: 7.978901386260986, Validation Loss: 21.23986053466797\n",
      "Epoch 956/10000, Training Loss: 12.112372398376465, Validation Loss: 16.352848052978516\n",
      "Epoch 957/10000, Training Loss: 13.395929336547852, Validation Loss: 9.841328620910645\n",
      "Epoch 958/10000, Training Loss: 8.192602157592773, Validation Loss: 10.443089485168457\n",
      "Epoch 959/10000, Training Loss: 7.608362674713135, Validation Loss: 23.759702682495117\n",
      "Epoch 960/10000, Training Loss: 15.888056755065918, Validation Loss: 34.26758575439453\n",
      "Epoch 961/10000, Training Loss: 11.280893325805664, Validation Loss: 11.994685173034668\n",
      "Epoch 962/10000, Training Loss: 11.558537483215332, Validation Loss: 20.643295288085938\n",
      "Epoch 963/10000, Training Loss: 11.17560863494873, Validation Loss: 12.212374687194824\n",
      "Epoch 964/10000, Training Loss: 13.59023380279541, Validation Loss: 9.593208312988281\n",
      "Epoch 965/10000, Training Loss: 19.184450149536133, Validation Loss: 24.5159912109375\n",
      "Epoch 966/10000, Training Loss: 11.993534088134766, Validation Loss: 22.36591339111328\n",
      "Epoch 967/10000, Training Loss: 21.754384994506836, Validation Loss: 12.593669891357422\n",
      "Epoch 968/10000, Training Loss: 10.23541259765625, Validation Loss: 11.981186866760254\n",
      "Epoch 969/10000, Training Loss: 12.591379165649414, Validation Loss: 14.906731605529785\n",
      "Epoch 970/10000, Training Loss: 18.859432220458984, Validation Loss: 69.9510726928711\n",
      "Epoch 971/10000, Training Loss: 9.656933784484863, Validation Loss: 11.683624267578125\n",
      "Epoch 972/10000, Training Loss: 9.885372161865234, Validation Loss: 5.362100601196289\n",
      "Epoch 973/10000, Training Loss: 12.762843132019043, Validation Loss: 10.459485054016113\n",
      "Epoch 974/10000, Training Loss: 12.768726348876953, Validation Loss: 14.07169246673584\n",
      "Epoch 975/10000, Training Loss: 16.32261848449707, Validation Loss: 37.79684829711914\n",
      "Epoch 976/10000, Training Loss: 12.67568302154541, Validation Loss: 10.956757545471191\n",
      "Epoch 977/10000, Training Loss: 6.833290100097656, Validation Loss: 10.345775604248047\n",
      "Epoch 978/10000, Training Loss: 16.64641571044922, Validation Loss: 19.861385345458984\n",
      "Epoch 979/10000, Training Loss: 10.748801231384277, Validation Loss: 11.026344299316406\n",
      "Epoch 980/10000, Training Loss: 19.891490936279297, Validation Loss: 12.099700927734375\n",
      "Epoch 981/10000, Training Loss: 13.550153732299805, Validation Loss: 8.329437255859375\n",
      "Epoch 982/10000, Training Loss: 14.171116828918457, Validation Loss: 5.084127902984619\n",
      "Epoch 983/10000, Training Loss: 21.73271942138672, Validation Loss: 44.32809066772461\n",
      "Epoch 984/10000, Training Loss: 11.602191925048828, Validation Loss: 25.02235984802246\n",
      "Epoch 985/10000, Training Loss: 12.81744384765625, Validation Loss: 8.501948356628418\n",
      "Epoch 986/10000, Training Loss: 7.938440322875977, Validation Loss: 13.654303550720215\n",
      "Epoch 987/10000, Training Loss: 16.410472869873047, Validation Loss: 3.2313578128814697\n",
      "Epoch 988/10000, Training Loss: 12.788813591003418, Validation Loss: 10.782185554504395\n",
      "Epoch 989/10000, Training Loss: 8.706818580627441, Validation Loss: 9.848884582519531\n",
      "Epoch 990/10000, Training Loss: 10.527471542358398, Validation Loss: 29.262601852416992\n",
      "Epoch 991/10000, Training Loss: 32.79878616333008, Validation Loss: 9.147550582885742\n",
      "Epoch 992/10000, Training Loss: 11.663175582885742, Validation Loss: 18.824617385864258\n",
      "Epoch 993/10000, Training Loss: 18.703739166259766, Validation Loss: 28.280357360839844\n",
      "Epoch 994/10000, Training Loss: 12.788771629333496, Validation Loss: 9.813977241516113\n",
      "Epoch 995/10000, Training Loss: 12.299418449401855, Validation Loss: 29.014989852905273\n",
      "Epoch 996/10000, Training Loss: 19.007465362548828, Validation Loss: 8.180238723754883\n",
      "Epoch 997/10000, Training Loss: 27.699077606201172, Validation Loss: 13.489741325378418\n",
      "Epoch 998/10000, Training Loss: 4.417999744415283, Validation Loss: 16.84940528869629\n",
      "Epoch 999/10000, Training Loss: 10.160663604736328, Validation Loss: 14.918667793273926\n",
      "Epoch 1000/10000, Training Loss: 30.40365982055664, Validation Loss: 54.17584228515625\n",
      "Epoch 1001/10000, Training Loss: 14.603323936462402, Validation Loss: 4.213189125061035\n",
      "Epoch 1002/10000, Training Loss: 7.370786190032959, Validation Loss: 2.409435749053955\n",
      "Epoch 1003/10000, Training Loss: 8.296875953674316, Validation Loss: 16.6689453125\n",
      "Epoch 1004/10000, Training Loss: 5.317235469818115, Validation Loss: 3.747116804122925\n",
      "Epoch 1005/10000, Training Loss: 23.69023323059082, Validation Loss: 2.8655622005462646\n",
      "Epoch 1006/10000, Training Loss: 19.941925048828125, Validation Loss: 16.309154510498047\n",
      "Epoch 1007/10000, Training Loss: 9.32956600189209, Validation Loss: 5.1003618240356445\n",
      "Epoch 1008/10000, Training Loss: 12.273367881774902, Validation Loss: 22.167503356933594\n",
      "Epoch 1009/10000, Training Loss: 18.902843475341797, Validation Loss: 46.62601089477539\n",
      "Epoch 1010/10000, Training Loss: 6.919219017028809, Validation Loss: 6.233739376068115\n",
      "Epoch 1011/10000, Training Loss: 12.246578216552734, Validation Loss: 35.630043029785156\n",
      "Epoch 1012/10000, Training Loss: 10.406538963317871, Validation Loss: 5.859168529510498\n",
      "Epoch 1013/10000, Training Loss: 10.082636833190918, Validation Loss: 13.137864112854004\n",
      "Epoch 1014/10000, Training Loss: 26.735599517822266, Validation Loss: 10.771363258361816\n",
      "Epoch 1015/10000, Training Loss: 19.306133270263672, Validation Loss: 12.755492210388184\n",
      "Epoch 1016/10000, Training Loss: 11.316631317138672, Validation Loss: 9.377755165100098\n",
      "Epoch 1017/10000, Training Loss: 16.430438995361328, Validation Loss: 25.90911102294922\n",
      "Epoch 1018/10000, Training Loss: 7.252406120300293, Validation Loss: 13.802014350891113\n",
      "Epoch 1019/10000, Training Loss: 23.352333068847656, Validation Loss: 5.4307379722595215\n",
      "Epoch 1020/10000, Training Loss: 8.952387809753418, Validation Loss: 14.085639953613281\n",
      "Epoch 1021/10000, Training Loss: 10.349404335021973, Validation Loss: 10.301884651184082\n",
      "Epoch 1022/10000, Training Loss: 13.81505012512207, Validation Loss: 37.41347885131836\n",
      "Epoch 1023/10000, Training Loss: 8.142897605895996, Validation Loss: 7.133671283721924\n",
      "Epoch 1024/10000, Training Loss: 13.739871978759766, Validation Loss: 6.30402946472168\n",
      "Epoch 1025/10000, Training Loss: 11.62356185913086, Validation Loss: 17.19150161743164\n",
      "Epoch 1026/10000, Training Loss: 13.532644271850586, Validation Loss: 17.697925567626953\n",
      "Epoch 1027/10000, Training Loss: 21.686891555786133, Validation Loss: 11.469819068908691\n",
      "Epoch 1028/10000, Training Loss: 11.9527587890625, Validation Loss: 6.931972980499268\n",
      "Epoch 1029/10000, Training Loss: 9.026869773864746, Validation Loss: 19.321929931640625\n",
      "Epoch 1030/10000, Training Loss: 14.123717308044434, Validation Loss: 6.067521572113037\n",
      "Epoch 1031/10000, Training Loss: 9.661308288574219, Validation Loss: 11.20607852935791\n",
      "Epoch 1032/10000, Training Loss: 28.097949981689453, Validation Loss: 12.585618019104004\n",
      "Epoch 1033/10000, Training Loss: 14.921018600463867, Validation Loss: 15.215487480163574\n",
      "Epoch 1034/10000, Training Loss: 13.193803787231445, Validation Loss: 7.741191864013672\n",
      "Epoch 1035/10000, Training Loss: 20.61252784729004, Validation Loss: 22.0638484954834\n",
      "Epoch 1036/10000, Training Loss: 8.019234657287598, Validation Loss: 10.5189790725708\n",
      "Epoch 1037/10000, Training Loss: 8.415919303894043, Validation Loss: 6.60175085067749\n",
      "Epoch 1038/10000, Training Loss: 9.755904197692871, Validation Loss: 29.247116088867188\n",
      "Epoch 1039/10000, Training Loss: 15.611593246459961, Validation Loss: 5.541327953338623\n",
      "Epoch 1040/10000, Training Loss: 11.460546493530273, Validation Loss: 19.724971771240234\n",
      "Epoch 1041/10000, Training Loss: 13.396169662475586, Validation Loss: 13.717514991760254\n",
      "Epoch 1042/10000, Training Loss: 10.387782096862793, Validation Loss: 8.019673347473145\n",
      "Epoch 1043/10000, Training Loss: 11.952595710754395, Validation Loss: 0.9627415537834167\n",
      "Epoch 1044/10000, Training Loss: 12.668574333190918, Validation Loss: 17.097280502319336\n",
      "Epoch 1045/10000, Training Loss: 6.8983073234558105, Validation Loss: 5.175137042999268\n",
      "Epoch 1046/10000, Training Loss: 12.642186164855957, Validation Loss: 6.907541751861572\n",
      "Epoch 1047/10000, Training Loss: 23.683061599731445, Validation Loss: 17.50688934326172\n",
      "Epoch 1048/10000, Training Loss: 8.625382423400879, Validation Loss: 4.50623893737793\n",
      "Epoch 1049/10000, Training Loss: 24.172109603881836, Validation Loss: 25.08216094970703\n",
      "Epoch 1050/10000, Training Loss: 10.359940528869629, Validation Loss: 13.966678619384766\n",
      "Epoch 1051/10000, Training Loss: 12.335315704345703, Validation Loss: 23.979589462280273\n",
      "Epoch 1052/10000, Training Loss: 12.588069915771484, Validation Loss: 8.220549583435059\n",
      "Epoch 1053/10000, Training Loss: 9.366316795349121, Validation Loss: 6.081274032592773\n",
      "Epoch 1054/10000, Training Loss: 16.652971267700195, Validation Loss: 33.46521759033203\n",
      "Epoch 1055/10000, Training Loss: 18.063098907470703, Validation Loss: 120.88166046142578\n",
      "Epoch 1056/10000, Training Loss: 6.6993584632873535, Validation Loss: 4.3871049880981445\n",
      "Epoch 1057/10000, Training Loss: 13.447282791137695, Validation Loss: 18.74566078186035\n",
      "Epoch 1058/10000, Training Loss: 13.639815330505371, Validation Loss: 20.821395874023438\n",
      "Epoch 1059/10000, Training Loss: 14.974080085754395, Validation Loss: 33.839813232421875\n",
      "Epoch 1060/10000, Training Loss: 12.107257843017578, Validation Loss: 35.20247268676758\n",
      "Epoch 1061/10000, Training Loss: 10.338380813598633, Validation Loss: 1.51771879196167\n",
      "Epoch 1062/10000, Training Loss: 9.387580871582031, Validation Loss: 5.135313510894775\n",
      "Epoch 1063/10000, Training Loss: 16.179645538330078, Validation Loss: 14.674911499023438\n",
      "Epoch 1064/10000, Training Loss: 15.659381866455078, Validation Loss: 19.031530380249023\n",
      "Epoch 1065/10000, Training Loss: 12.971230506896973, Validation Loss: 0.40551674365997314\n",
      "Epoch 1066/10000, Training Loss: 10.814716339111328, Validation Loss: 4.060800075531006\n",
      "Epoch 1067/10000, Training Loss: 13.033989906311035, Validation Loss: 24.77576446533203\n",
      "Epoch 1068/10000, Training Loss: 16.063352584838867, Validation Loss: 16.928447723388672\n",
      "Epoch 1069/10000, Training Loss: 12.712508201599121, Validation Loss: 5.294225215911865\n",
      "Epoch 1070/10000, Training Loss: 13.209131240844727, Validation Loss: 11.200940132141113\n",
      "Epoch 1071/10000, Training Loss: 10.08566665649414, Validation Loss: 3.6856110095977783\n",
      "Epoch 1072/10000, Training Loss: 11.731649398803711, Validation Loss: 19.92962074279785\n",
      "Epoch 1073/10000, Training Loss: 13.790947914123535, Validation Loss: 12.117497444152832\n",
      "Epoch 1074/10000, Training Loss: 12.763154029846191, Validation Loss: 8.667937278747559\n",
      "Epoch 1075/10000, Training Loss: 10.270391464233398, Validation Loss: 4.241541385650635\n",
      "Epoch 1076/10000, Training Loss: 12.88376235961914, Validation Loss: 29.843399047851562\n",
      "Epoch 1077/10000, Training Loss: 19.06557273864746, Validation Loss: 9.378375053405762\n",
      "Epoch 1078/10000, Training Loss: 16.313247680664062, Validation Loss: 34.585628509521484\n",
      "Epoch 1079/10000, Training Loss: 8.591630935668945, Validation Loss: 23.435745239257812\n",
      "Epoch 1080/10000, Training Loss: 14.52723503112793, Validation Loss: 11.81717300415039\n",
      "Epoch 1081/10000, Training Loss: 14.040968894958496, Validation Loss: 32.96440505981445\n",
      "Epoch 1082/10000, Training Loss: 7.519548416137695, Validation Loss: 10.833695411682129\n",
      "Epoch 1083/10000, Training Loss: 10.11695384979248, Validation Loss: 21.36517906188965\n",
      "Epoch 1084/10000, Training Loss: 15.756552696228027, Validation Loss: 2.8508989810943604\n",
      "Epoch 1085/10000, Training Loss: 17.46060562133789, Validation Loss: 16.42537498474121\n",
      "Epoch 1086/10000, Training Loss: 11.357175827026367, Validation Loss: 2.6194727420806885\n",
      "Epoch 1087/10000, Training Loss: 11.872208595275879, Validation Loss: 5.535160064697266\n",
      "Epoch 1088/10000, Training Loss: 16.121694564819336, Validation Loss: 10.454621315002441\n",
      "Epoch 1089/10000, Training Loss: 12.287491798400879, Validation Loss: 10.282910346984863\n",
      "Epoch 1090/10000, Training Loss: 12.638713836669922, Validation Loss: 18.267393112182617\n",
      "Epoch 1091/10000, Training Loss: 10.958332061767578, Validation Loss: 17.164621353149414\n",
      "Epoch 1092/10000, Training Loss: 14.047891616821289, Validation Loss: 24.1351261138916\n",
      "Epoch 1093/10000, Training Loss: 14.989676475524902, Validation Loss: 16.922983169555664\n",
      "Epoch 1094/10000, Training Loss: 7.880706787109375, Validation Loss: 11.223297119140625\n",
      "Epoch 1095/10000, Training Loss: 14.833446502685547, Validation Loss: 16.979677200317383\n",
      "Epoch 1096/10000, Training Loss: 10.000601768493652, Validation Loss: 3.1672260761260986\n",
      "Epoch 1097/10000, Training Loss: 10.428516387939453, Validation Loss: 17.69811248779297\n",
      "Epoch 1098/10000, Training Loss: 8.604493141174316, Validation Loss: 9.003032684326172\n",
      "Epoch 1099/10000, Training Loss: 12.239264488220215, Validation Loss: 3.8508589267730713\n",
      "Epoch 1100/10000, Training Loss: 9.386285781860352, Validation Loss: 12.172860145568848\n",
      "Epoch 1101/10000, Training Loss: 7.2963385581970215, Validation Loss: 16.705162048339844\n",
      "Epoch 1102/10000, Training Loss: 26.684154510498047, Validation Loss: 15.649334907531738\n",
      "Epoch 1103/10000, Training Loss: 12.486682891845703, Validation Loss: 13.254600524902344\n",
      "Epoch 1104/10000, Training Loss: 9.748400688171387, Validation Loss: 21.54404640197754\n",
      "Epoch 1105/10000, Training Loss: 15.431878089904785, Validation Loss: 17.605485916137695\n",
      "Epoch 1106/10000, Training Loss: 10.63306999206543, Validation Loss: 6.6093902587890625\n",
      "Epoch 1107/10000, Training Loss: 8.360880851745605, Validation Loss: 1.2191356420516968\n",
      "Epoch 1108/10000, Training Loss: 10.124734878540039, Validation Loss: 8.172679901123047\n",
      "Epoch 1109/10000, Training Loss: 13.301992416381836, Validation Loss: 21.430227279663086\n",
      "Epoch 1110/10000, Training Loss: 25.307157516479492, Validation Loss: 8.974586486816406\n",
      "Epoch 1111/10000, Training Loss: 5.452138900756836, Validation Loss: 4.125091075897217\n",
      "Epoch 1112/10000, Training Loss: 13.301722526550293, Validation Loss: 15.409404754638672\n",
      "Epoch 1113/10000, Training Loss: 12.208250999450684, Validation Loss: 25.187728881835938\n",
      "Epoch 1114/10000, Training Loss: 10.57161808013916, Validation Loss: 1.7766222953796387\n",
      "Epoch 1115/10000, Training Loss: 24.268064498901367, Validation Loss: 35.42305374145508\n",
      "Epoch 1116/10000, Training Loss: 17.512834548950195, Validation Loss: 14.037236213684082\n",
      "Epoch 1117/10000, Training Loss: 7.105624198913574, Validation Loss: 7.618682384490967\n",
      "Epoch 1118/10000, Training Loss: 11.498477935791016, Validation Loss: 1.121168851852417\n",
      "Epoch 1119/10000, Training Loss: 6.255408763885498, Validation Loss: 5.449625492095947\n",
      "Epoch 1120/10000, Training Loss: 11.060903549194336, Validation Loss: 14.14578914642334\n",
      "Epoch 1121/10000, Training Loss: 17.68043327331543, Validation Loss: 6.804052352905273\n",
      "Epoch 1122/10000, Training Loss: 11.71890926361084, Validation Loss: 20.42431640625\n",
      "Epoch 1123/10000, Training Loss: 11.910150527954102, Validation Loss: 6.404607772827148\n",
      "Epoch 1124/10000, Training Loss: 9.231008529663086, Validation Loss: 16.61459732055664\n",
      "Epoch 1125/10000, Training Loss: 9.566117286682129, Validation Loss: 6.40283203125\n",
      "Epoch 1126/10000, Training Loss: 11.761876106262207, Validation Loss: 31.25705909729004\n",
      "Epoch 1127/10000, Training Loss: 13.095717430114746, Validation Loss: 25.68865203857422\n",
      "Epoch 1128/10000, Training Loss: 14.469491958618164, Validation Loss: 16.747833251953125\n",
      "Epoch 1129/10000, Training Loss: 9.947279930114746, Validation Loss: 3.877673387527466\n",
      "Epoch 1130/10000, Training Loss: 9.054412841796875, Validation Loss: 7.755544662475586\n",
      "Epoch 1131/10000, Training Loss: 25.216096878051758, Validation Loss: 18.278501510620117\n",
      "Epoch 1132/10000, Training Loss: 18.406593322753906, Validation Loss: 67.25306701660156\n",
      "Epoch 1133/10000, Training Loss: 15.488923072814941, Validation Loss: 31.232328414916992\n",
      "Epoch 1134/10000, Training Loss: 11.470561027526855, Validation Loss: 16.222105026245117\n",
      "Epoch 1135/10000, Training Loss: 12.452340126037598, Validation Loss: 11.712374687194824\n",
      "Epoch 1136/10000, Training Loss: 10.631892204284668, Validation Loss: 11.226974487304688\n",
      "Epoch 1137/10000, Training Loss: 11.866217613220215, Validation Loss: 1.902303695678711\n",
      "Epoch 1138/10000, Training Loss: 6.212313175201416, Validation Loss: 6.212809085845947\n",
      "Epoch 1139/10000, Training Loss: 14.472518920898438, Validation Loss: 5.033888339996338\n",
      "Epoch 1140/10000, Training Loss: 8.937226295471191, Validation Loss: 6.557004928588867\n",
      "Epoch 1141/10000, Training Loss: 13.645508766174316, Validation Loss: 13.103275299072266\n",
      "Epoch 1142/10000, Training Loss: 9.833416938781738, Validation Loss: 10.65623950958252\n",
      "Epoch 1143/10000, Training Loss: 13.779549598693848, Validation Loss: 10.204607009887695\n",
      "Epoch 1144/10000, Training Loss: 10.586156845092773, Validation Loss: 7.590307712554932\n",
      "Epoch 1145/10000, Training Loss: 11.543354034423828, Validation Loss: 35.6644287109375\n",
      "Epoch 1146/10000, Training Loss: 10.482335090637207, Validation Loss: 25.951181411743164\n",
      "Epoch 1147/10000, Training Loss: 6.7489166259765625, Validation Loss: 7.0477519035339355\n",
      "Epoch 1148/10000, Training Loss: 8.947478294372559, Validation Loss: 11.323962211608887\n",
      "Epoch 1149/10000, Training Loss: 14.268096923828125, Validation Loss: 11.353131294250488\n",
      "Epoch 1150/10000, Training Loss: 12.673023223876953, Validation Loss: 4.907963275909424\n",
      "Epoch 1151/10000, Training Loss: 10.878755569458008, Validation Loss: 6.8627028465271\n",
      "Epoch 1152/10000, Training Loss: 7.4176716804504395, Validation Loss: 14.864070892333984\n",
      "Epoch 1153/10000, Training Loss: 12.38127613067627, Validation Loss: 3.045779228210449\n",
      "Epoch 1154/10000, Training Loss: 11.709970474243164, Validation Loss: 5.874748706817627\n",
      "Epoch 1155/10000, Training Loss: 8.087306022644043, Validation Loss: 19.41474723815918\n",
      "Epoch 1156/10000, Training Loss: 7.692159652709961, Validation Loss: 3.729646682739258\n",
      "Epoch 1157/10000, Training Loss: 7.640152931213379, Validation Loss: 11.090899467468262\n",
      "Epoch 1158/10000, Training Loss: 12.705343246459961, Validation Loss: 40.19437026977539\n",
      "Epoch 1159/10000, Training Loss: 17.593544006347656, Validation Loss: 17.833593368530273\n",
      "Epoch 1160/10000, Training Loss: 9.23376178741455, Validation Loss: 8.133875846862793\n",
      "Epoch 1161/10000, Training Loss: 8.025781631469727, Validation Loss: 6.460638046264648\n",
      "Epoch 1162/10000, Training Loss: 6.001640796661377, Validation Loss: 7.920286178588867\n",
      "Epoch 1163/10000, Training Loss: 12.96681022644043, Validation Loss: 2.4027864933013916\n",
      "Epoch 1164/10000, Training Loss: 9.64547061920166, Validation Loss: 16.351150512695312\n",
      "Epoch 1165/10000, Training Loss: 10.85704231262207, Validation Loss: 31.536394119262695\n",
      "Epoch 1166/10000, Training Loss: 11.518125534057617, Validation Loss: 11.84728717803955\n",
      "Epoch 1167/10000, Training Loss: 10.721436500549316, Validation Loss: 6.823887348175049\n",
      "Epoch 1168/10000, Training Loss: 10.582228660583496, Validation Loss: 14.62572193145752\n",
      "Epoch 1169/10000, Training Loss: 13.82657241821289, Validation Loss: 9.876228332519531\n",
      "Epoch 1170/10000, Training Loss: 11.541055679321289, Validation Loss: 11.512652397155762\n",
      "Epoch 1171/10000, Training Loss: 14.676359176635742, Validation Loss: 7.0011067390441895\n",
      "Epoch 1172/10000, Training Loss: 13.587042808532715, Validation Loss: 9.122136116027832\n",
      "Epoch 1173/10000, Training Loss: 12.790715217590332, Validation Loss: 19.800458908081055\n",
      "Epoch 1174/10000, Training Loss: 6.618849754333496, Validation Loss: 9.55977725982666\n",
      "Epoch 1175/10000, Training Loss: 8.812020301818848, Validation Loss: 10.066465377807617\n",
      "Epoch 1176/10000, Training Loss: 8.56782054901123, Validation Loss: 5.205269813537598\n",
      "Epoch 1177/10000, Training Loss: 23.07695770263672, Validation Loss: 10.413702011108398\n",
      "Epoch 1178/10000, Training Loss: 11.18836784362793, Validation Loss: 30.229509353637695\n",
      "Epoch 1179/10000, Training Loss: 15.604376792907715, Validation Loss: 8.093718528747559\n",
      "Epoch 1180/10000, Training Loss: 9.35791301727295, Validation Loss: 14.492903709411621\n",
      "Epoch 1181/10000, Training Loss: 10.139219284057617, Validation Loss: 13.602954864501953\n",
      "Epoch 1182/10000, Training Loss: 8.661157608032227, Validation Loss: 5.738877773284912\n",
      "Epoch 1183/10000, Training Loss: 9.294999122619629, Validation Loss: 12.503645896911621\n",
      "Epoch 1184/10000, Training Loss: 17.201454162597656, Validation Loss: 4.828296661376953\n",
      "Epoch 1185/10000, Training Loss: 8.952215194702148, Validation Loss: 1.8926973342895508\n",
      "Epoch 1186/10000, Training Loss: 8.03363037109375, Validation Loss: 8.684833526611328\n",
      "Epoch 1187/10000, Training Loss: 6.656895160675049, Validation Loss: 15.153035163879395\n",
      "Epoch 1188/10000, Training Loss: 10.815049171447754, Validation Loss: 22.94049835205078\n",
      "Epoch 1189/10000, Training Loss: 8.763352394104004, Validation Loss: 6.063217639923096\n",
      "Epoch 1190/10000, Training Loss: 38.515377044677734, Validation Loss: 14.724774360656738\n",
      "Epoch 1191/10000, Training Loss: 9.390337944030762, Validation Loss: 50.65091323852539\n",
      "Epoch 1192/10000, Training Loss: 9.219463348388672, Validation Loss: 26.281526565551758\n",
      "Epoch 1193/10000, Training Loss: 10.214835166931152, Validation Loss: 4.986886501312256\n",
      "Epoch 1194/10000, Training Loss: 10.050459861755371, Validation Loss: 15.940773010253906\n",
      "Epoch 1195/10000, Training Loss: 10.537125587463379, Validation Loss: 7.3804168701171875\n",
      "Epoch 1196/10000, Training Loss: 14.758228302001953, Validation Loss: 22.596940994262695\n",
      "Epoch 1197/10000, Training Loss: 10.297112464904785, Validation Loss: 10.551196098327637\n",
      "Epoch 1198/10000, Training Loss: 10.769420623779297, Validation Loss: 2.952554941177368\n",
      "Epoch 1199/10000, Training Loss: 10.351226806640625, Validation Loss: 6.101810455322266\n",
      "Epoch 1200/10000, Training Loss: 16.054746627807617, Validation Loss: 7.917504787445068\n",
      "Epoch 1201/10000, Training Loss: 18.489070892333984, Validation Loss: 10.92846965789795\n",
      "Epoch 1202/10000, Training Loss: 7.849552631378174, Validation Loss: 5.410279750823975\n",
      "Epoch 1203/10000, Training Loss: 8.961146354675293, Validation Loss: 1.6282304525375366\n",
      "Epoch 1204/10000, Training Loss: 16.742427825927734, Validation Loss: 86.71385955810547\n",
      "Epoch 1205/10000, Training Loss: 9.359236717224121, Validation Loss: 6.235158443450928\n",
      "Epoch 1206/10000, Training Loss: 10.541192054748535, Validation Loss: 6.534046173095703\n",
      "Epoch 1207/10000, Training Loss: 6.495275020599365, Validation Loss: 14.2600736618042\n",
      "Epoch 1208/10000, Training Loss: 8.646674156188965, Validation Loss: 3.9067413806915283\n",
      "Epoch 1209/10000, Training Loss: 15.592317581176758, Validation Loss: 17.4638671875\n",
      "Epoch 1210/10000, Training Loss: 10.356329917907715, Validation Loss: 15.635746955871582\n",
      "Epoch 1211/10000, Training Loss: 9.743773460388184, Validation Loss: 14.058475494384766\n",
      "Epoch 1212/10000, Training Loss: 12.29766845703125, Validation Loss: 8.930285453796387\n",
      "Epoch 1213/10000, Training Loss: 13.60451602935791, Validation Loss: 17.371891021728516\n",
      "Epoch 1214/10000, Training Loss: 10.12574291229248, Validation Loss: 9.653637886047363\n",
      "Epoch 1215/10000, Training Loss: 12.172060012817383, Validation Loss: 6.813043594360352\n",
      "Epoch 1216/10000, Training Loss: 7.10615348815918, Validation Loss: 8.156785011291504\n",
      "Epoch 1217/10000, Training Loss: 14.95130443572998, Validation Loss: 21.074129104614258\n",
      "Epoch 1218/10000, Training Loss: 6.679868698120117, Validation Loss: 17.61100196838379\n",
      "Epoch 1219/10000, Training Loss: 12.268219947814941, Validation Loss: 9.838717460632324\n",
      "Epoch 1220/10000, Training Loss: 12.347427368164062, Validation Loss: 8.096111297607422\n",
      "Epoch 1221/10000, Training Loss: 10.874281883239746, Validation Loss: 28.78514289855957\n",
      "Epoch 1222/10000, Training Loss: 16.140674591064453, Validation Loss: 16.959184646606445\n",
      "Epoch 1223/10000, Training Loss: 13.663359642028809, Validation Loss: 12.239338874816895\n",
      "Epoch 1224/10000, Training Loss: 12.537216186523438, Validation Loss: 15.993228912353516\n",
      "Epoch 1225/10000, Training Loss: 15.765869140625, Validation Loss: 4.669591903686523\n",
      "Epoch 1226/10000, Training Loss: 6.9733357429504395, Validation Loss: 9.461594581604004\n",
      "Epoch 1227/10000, Training Loss: 10.847574234008789, Validation Loss: 17.48680877685547\n",
      "Epoch 1228/10000, Training Loss: 15.744478225708008, Validation Loss: 4.216001987457275\n",
      "Epoch 1229/10000, Training Loss: 10.51010799407959, Validation Loss: 13.068534851074219\n",
      "Epoch 1230/10000, Training Loss: 12.45890998840332, Validation Loss: 18.528331756591797\n",
      "Epoch 1231/10000, Training Loss: 9.133454322814941, Validation Loss: 4.230357646942139\n",
      "Epoch 1232/10000, Training Loss: 6.841705799102783, Validation Loss: 6.31063985824585\n",
      "Epoch 1233/10000, Training Loss: 8.462380409240723, Validation Loss: 8.694920539855957\n",
      "Epoch 1234/10000, Training Loss: 11.873177528381348, Validation Loss: 12.450726509094238\n",
      "Epoch 1235/10000, Training Loss: 12.664844512939453, Validation Loss: 32.438045501708984\n",
      "Epoch 1236/10000, Training Loss: 6.699265480041504, Validation Loss: 15.123058319091797\n",
      "Epoch 1237/10000, Training Loss: 9.059718132019043, Validation Loss: 7.874671936035156\n",
      "Epoch 1238/10000, Training Loss: 21.46330451965332, Validation Loss: 18.546472549438477\n",
      "Epoch 1239/10000, Training Loss: 7.991547584533691, Validation Loss: 21.14899253845215\n",
      "Epoch 1240/10000, Training Loss: 16.351213455200195, Validation Loss: 18.9188175201416\n",
      "Epoch 1241/10000, Training Loss: 7.283810615539551, Validation Loss: 25.51732635498047\n",
      "Epoch 1242/10000, Training Loss: 9.081389427185059, Validation Loss: 12.7467679977417\n",
      "Epoch 1243/10000, Training Loss: 11.27580738067627, Validation Loss: 19.263139724731445\n",
      "Epoch 1244/10000, Training Loss: 13.423200607299805, Validation Loss: 2.026919364929199\n",
      "Epoch 1245/10000, Training Loss: 7.6811418533325195, Validation Loss: 16.040485382080078\n",
      "Epoch 1246/10000, Training Loss: 9.783263206481934, Validation Loss: 10.353199005126953\n",
      "Epoch 1247/10000, Training Loss: 7.713961124420166, Validation Loss: 3.467348337173462\n",
      "Epoch 1248/10000, Training Loss: 18.420808792114258, Validation Loss: 20.46889877319336\n",
      "Epoch 1249/10000, Training Loss: 17.263185501098633, Validation Loss: 25.60724639892578\n",
      "Epoch 1250/10000, Training Loss: 15.340474128723145, Validation Loss: 7.597599506378174\n",
      "Epoch 1251/10000, Training Loss: 10.438894271850586, Validation Loss: 13.45223617553711\n",
      "Epoch 1252/10000, Training Loss: 11.137264251708984, Validation Loss: 37.5990104675293\n",
      "Epoch 1253/10000, Training Loss: 12.305963516235352, Validation Loss: 14.161690711975098\n",
      "Epoch 1254/10000, Training Loss: 6.6220479011535645, Validation Loss: 15.199828147888184\n",
      "Epoch 1255/10000, Training Loss: 7.793545722961426, Validation Loss: 9.167657852172852\n",
      "Epoch 1256/10000, Training Loss: 11.252487182617188, Validation Loss: 2.477368116378784\n",
      "Epoch 1257/10000, Training Loss: 7.8090739250183105, Validation Loss: 9.59169864654541\n",
      "Epoch 1258/10000, Training Loss: 12.016684532165527, Validation Loss: 9.133606910705566\n",
      "Epoch 1259/10000, Training Loss: 6.619723320007324, Validation Loss: 1.5125848054885864\n",
      "Epoch 1260/10000, Training Loss: 10.434712409973145, Validation Loss: 22.802602767944336\n",
      "Epoch 1261/10000, Training Loss: 12.797995567321777, Validation Loss: 16.342044830322266\n",
      "Epoch 1262/10000, Training Loss: 9.458517074584961, Validation Loss: 6.0388007164001465\n",
      "Epoch 1263/10000, Training Loss: 8.471113204956055, Validation Loss: 12.270806312561035\n",
      "Epoch 1264/10000, Training Loss: 17.669431686401367, Validation Loss: 10.927083015441895\n",
      "Epoch 1265/10000, Training Loss: 9.516776084899902, Validation Loss: 11.991307258605957\n",
      "Epoch 1266/10000, Training Loss: 5.239758014678955, Validation Loss: 7.77182149887085\n",
      "Epoch 1267/10000, Training Loss: 9.154582977294922, Validation Loss: 10.1131010055542\n",
      "Epoch 1268/10000, Training Loss: 8.648004531860352, Validation Loss: 4.5321550369262695\n",
      "Epoch 1269/10000, Training Loss: 8.998076438903809, Validation Loss: 23.452951431274414\n",
      "Epoch 1270/10000, Training Loss: 9.2651948928833, Validation Loss: 8.966711044311523\n",
      "Epoch 1271/10000, Training Loss: 6.2069196701049805, Validation Loss: 8.952011108398438\n",
      "Epoch 1272/10000, Training Loss: 10.088857650756836, Validation Loss: 19.35721778869629\n",
      "Epoch 1273/10000, Training Loss: 11.156352996826172, Validation Loss: 4.388246059417725\n",
      "Epoch 1274/10000, Training Loss: 13.572089195251465, Validation Loss: 14.141255378723145\n",
      "Epoch 1275/10000, Training Loss: 10.098496437072754, Validation Loss: 13.014910697937012\n",
      "Epoch 1276/10000, Training Loss: 16.055099487304688, Validation Loss: 44.458133697509766\n",
      "Epoch 1277/10000, Training Loss: 6.4871649742126465, Validation Loss: 2.352421522140503\n",
      "Epoch 1278/10000, Training Loss: 11.683979988098145, Validation Loss: 6.603076457977295\n",
      "Epoch 1279/10000, Training Loss: 8.707452774047852, Validation Loss: 6.858585357666016\n",
      "Epoch 1280/10000, Training Loss: 8.243842124938965, Validation Loss: 9.76305103302002\n",
      "Epoch 1281/10000, Training Loss: 8.646788597106934, Validation Loss: 8.138355255126953\n",
      "Epoch 1282/10000, Training Loss: 8.001453399658203, Validation Loss: 6.498850345611572\n",
      "Epoch 1283/10000, Training Loss: 9.268131256103516, Validation Loss: 12.70494556427002\n",
      "Epoch 1284/10000, Training Loss: 12.10350513458252, Validation Loss: 9.364738464355469\n",
      "Epoch 1285/10000, Training Loss: 15.566829681396484, Validation Loss: 23.318084716796875\n",
      "Epoch 1286/10000, Training Loss: 8.239994049072266, Validation Loss: 26.148950576782227\n",
      "Epoch 1287/10000, Training Loss: 6.449679851531982, Validation Loss: 6.914334774017334\n",
      "Epoch 1288/10000, Training Loss: 8.516054153442383, Validation Loss: 7.917016983032227\n",
      "Epoch 1289/10000, Training Loss: 6.7287492752075195, Validation Loss: 14.77978801727295\n",
      "Epoch 1290/10000, Training Loss: 19.235795974731445, Validation Loss: 25.90669822692871\n",
      "Epoch 1291/10000, Training Loss: 6.538250923156738, Validation Loss: 6.823409557342529\n",
      "Epoch 1292/10000, Training Loss: 11.633373260498047, Validation Loss: 6.38701057434082\n",
      "Epoch 1293/10000, Training Loss: 6.937469959259033, Validation Loss: 23.389902114868164\n",
      "Epoch 1294/10000, Training Loss: 6.8443450927734375, Validation Loss: 23.3524227142334\n",
      "Epoch 1295/10000, Training Loss: 4.626384258270264, Validation Loss: 27.39353370666504\n",
      "Epoch 1296/10000, Training Loss: 7.178969383239746, Validation Loss: 17.35710334777832\n",
      "Epoch 1297/10000, Training Loss: 10.757566452026367, Validation Loss: 7.35112190246582\n",
      "Epoch 1298/10000, Training Loss: 6.8101983070373535, Validation Loss: 31.42815589904785\n",
      "Epoch 1299/10000, Training Loss: 7.98162317276001, Validation Loss: 5.251394271850586\n",
      "Epoch 1300/10000, Training Loss: 19.982112884521484, Validation Loss: 19.50011444091797\n",
      "Epoch 1301/10000, Training Loss: 8.968329429626465, Validation Loss: 11.677689552307129\n",
      "Epoch 1302/10000, Training Loss: 14.510242462158203, Validation Loss: 11.36114501953125\n",
      "Epoch 1303/10000, Training Loss: 4.88734245300293, Validation Loss: 6.159893035888672\n",
      "Epoch 1304/10000, Training Loss: 6.849630355834961, Validation Loss: 13.300209045410156\n",
      "Epoch 1305/10000, Training Loss: 9.164060592651367, Validation Loss: 5.530766010284424\n",
      "Epoch 1306/10000, Training Loss: 12.110085487365723, Validation Loss: 8.574662208557129\n",
      "Epoch 1307/10000, Training Loss: 11.456131935119629, Validation Loss: 15.352801322937012\n",
      "Epoch 1308/10000, Training Loss: 9.474455833435059, Validation Loss: 8.197249412536621\n",
      "Epoch 1309/10000, Training Loss: 7.732587814331055, Validation Loss: 5.977163314819336\n",
      "Epoch 1310/10000, Training Loss: 7.464213848114014, Validation Loss: 9.52412223815918\n",
      "Epoch 1311/10000, Training Loss: 13.845982551574707, Validation Loss: 3.690056562423706\n",
      "Epoch 1312/10000, Training Loss: 7.5384626388549805, Validation Loss: 15.847846984863281\n",
      "Epoch 1313/10000, Training Loss: 13.213062286376953, Validation Loss: 10.239890098571777\n",
      "Epoch 1314/10000, Training Loss: 6.163257122039795, Validation Loss: 8.339241981506348\n",
      "Epoch 1315/10000, Training Loss: 9.06701946258545, Validation Loss: 18.35725212097168\n",
      "Epoch 1316/10000, Training Loss: 12.513533592224121, Validation Loss: 7.9927144050598145\n",
      "Epoch 1317/10000, Training Loss: 16.039836883544922, Validation Loss: 9.261185646057129\n",
      "Epoch 1318/10000, Training Loss: 10.432891845703125, Validation Loss: 17.201416015625\n",
      "Epoch 1319/10000, Training Loss: 6.069767951965332, Validation Loss: 8.31801700592041\n",
      "Epoch 1320/10000, Training Loss: 9.344457626342773, Validation Loss: 58.5255241394043\n",
      "Epoch 1321/10000, Training Loss: 9.748335838317871, Validation Loss: 6.852375030517578\n",
      "Epoch 1322/10000, Training Loss: 7.3270792961120605, Validation Loss: 17.530834197998047\n",
      "Epoch 1323/10000, Training Loss: 11.807458877563477, Validation Loss: 12.772248268127441\n",
      "Epoch 1324/10000, Training Loss: 6.758849620819092, Validation Loss: 9.019204139709473\n",
      "Epoch 1325/10000, Training Loss: 9.332843780517578, Validation Loss: 3.7377498149871826\n",
      "Epoch 1326/10000, Training Loss: 6.574870586395264, Validation Loss: 5.903029918670654\n",
      "Epoch 1327/10000, Training Loss: 19.592445373535156, Validation Loss: 51.99237060546875\n",
      "Epoch 1328/10000, Training Loss: 7.347757339477539, Validation Loss: 6.0597100257873535\n",
      "Epoch 1329/10000, Training Loss: 6.102157115936279, Validation Loss: 7.006126403808594\n",
      "Epoch 1330/10000, Training Loss: 10.086736679077148, Validation Loss: 9.846607208251953\n",
      "Epoch 1331/10000, Training Loss: 5.375473976135254, Validation Loss: 11.955662727355957\n",
      "Epoch 1332/10000, Training Loss: 6.200851917266846, Validation Loss: 13.079025268554688\n",
      "Epoch 1333/10000, Training Loss: 6.073820114135742, Validation Loss: 5.457535266876221\n",
      "Epoch 1334/10000, Training Loss: 14.337188720703125, Validation Loss: 10.984386444091797\n",
      "Epoch 1335/10000, Training Loss: 6.828084468841553, Validation Loss: 2.644164800643921\n",
      "Epoch 1336/10000, Training Loss: 13.596076011657715, Validation Loss: 21.008399963378906\n",
      "Epoch 1337/10000, Training Loss: 6.863361835479736, Validation Loss: 19.941370010375977\n",
      "Epoch 1338/10000, Training Loss: 11.605640411376953, Validation Loss: 16.96372413635254\n",
      "Epoch 1339/10000, Training Loss: 8.756446838378906, Validation Loss: 7.462015151977539\n",
      "Epoch 1340/10000, Training Loss: 9.478227615356445, Validation Loss: 2.647871732711792\n",
      "Epoch 1341/10000, Training Loss: 7.593516826629639, Validation Loss: 1.2995991706848145\n",
      "Epoch 1342/10000, Training Loss: 13.248014450073242, Validation Loss: 7.633474826812744\n",
      "Epoch 1343/10000, Training Loss: 9.048898696899414, Validation Loss: 5.94264554977417\n",
      "Epoch 1344/10000, Training Loss: 12.447951316833496, Validation Loss: 19.70805549621582\n",
      "Epoch 1345/10000, Training Loss: 9.12915325164795, Validation Loss: 19.979555130004883\n",
      "Epoch 1346/10000, Training Loss: 8.189312934875488, Validation Loss: 3.6369152069091797\n",
      "Epoch 1347/10000, Training Loss: 8.249589920043945, Validation Loss: 9.360823631286621\n",
      "Epoch 1348/10000, Training Loss: 9.49787712097168, Validation Loss: 8.299139976501465\n",
      "Epoch 1349/10000, Training Loss: 10.202779769897461, Validation Loss: 19.257003784179688\n",
      "Epoch 1350/10000, Training Loss: 8.424921035766602, Validation Loss: 14.248226165771484\n",
      "Epoch 1351/10000, Training Loss: 13.73814582824707, Validation Loss: 13.589478492736816\n",
      "Epoch 1352/10000, Training Loss: 6.489596843719482, Validation Loss: 2.7904281616210938\n",
      "Epoch 1353/10000, Training Loss: 7.847334861755371, Validation Loss: 4.128174304962158\n",
      "Epoch 1354/10000, Training Loss: 7.561970233917236, Validation Loss: 16.344697952270508\n",
      "Epoch 1355/10000, Training Loss: 5.8339104652404785, Validation Loss: 13.621844291687012\n",
      "Epoch 1356/10000, Training Loss: 10.653514862060547, Validation Loss: 1.9948493242263794\n",
      "Epoch 1357/10000, Training Loss: 11.845023155212402, Validation Loss: 5.878088474273682\n",
      "Epoch 1358/10000, Training Loss: 7.470371723175049, Validation Loss: 17.092573165893555\n",
      "Epoch 1359/10000, Training Loss: 5.916166305541992, Validation Loss: 15.930642127990723\n",
      "Epoch 1360/10000, Training Loss: 8.81374740600586, Validation Loss: 9.11945629119873\n",
      "Epoch 1361/10000, Training Loss: 7.145321369171143, Validation Loss: 6.209670543670654\n",
      "Epoch 1362/10000, Training Loss: 8.55150318145752, Validation Loss: 9.306036949157715\n",
      "Epoch 1363/10000, Training Loss: 16.56171417236328, Validation Loss: 22.271860122680664\n",
      "Epoch 1364/10000, Training Loss: 10.57561206817627, Validation Loss: 8.527876853942871\n",
      "Epoch 1365/10000, Training Loss: 9.475218772888184, Validation Loss: 3.1225812435150146\n",
      "Epoch 1366/10000, Training Loss: 20.355531692504883, Validation Loss: 15.877850532531738\n",
      "Epoch 1367/10000, Training Loss: 6.7923126220703125, Validation Loss: 3.2554962635040283\n",
      "Epoch 1368/10000, Training Loss: 5.929995536804199, Validation Loss: 9.315686225891113\n",
      "Epoch 1369/10000, Training Loss: 7.297658443450928, Validation Loss: 13.20336627960205\n",
      "Epoch 1370/10000, Training Loss: 6.055140018463135, Validation Loss: 2.5690805912017822\n",
      "Epoch 1371/10000, Training Loss: 6.361358165740967, Validation Loss: 17.096923828125\n",
      "Epoch 1372/10000, Training Loss: 7.136861324310303, Validation Loss: 7.894733428955078\n",
      "Epoch 1373/10000, Training Loss: 12.97536849975586, Validation Loss: 6.226650714874268\n",
      "Epoch 1374/10000, Training Loss: 8.823904037475586, Validation Loss: 4.365878105163574\n",
      "Epoch 1375/10000, Training Loss: 9.07004165649414, Validation Loss: 0.4729750454425812\n",
      "Epoch 1376/10000, Training Loss: 13.781848907470703, Validation Loss: 15.303313255310059\n",
      "Epoch 1377/10000, Training Loss: 8.4074125289917, Validation Loss: 3.764674425125122\n",
      "Epoch 1378/10000, Training Loss: 6.204625129699707, Validation Loss: 11.183220863342285\n",
      "Epoch 1379/10000, Training Loss: 5.216635704040527, Validation Loss: 7.924900054931641\n",
      "Epoch 1380/10000, Training Loss: 7.30521297454834, Validation Loss: 11.760680198669434\n",
      "Epoch 1381/10000, Training Loss: 8.395925521850586, Validation Loss: 7.955364227294922\n",
      "Epoch 1382/10000, Training Loss: 25.370861053466797, Validation Loss: 6.226243495941162\n",
      "Epoch 1383/10000, Training Loss: 7.404929161071777, Validation Loss: 3.096548318862915\n",
      "Epoch 1384/10000, Training Loss: 7.08123779296875, Validation Loss: 13.113758087158203\n",
      "Epoch 1385/10000, Training Loss: 5.900585651397705, Validation Loss: 2.5764997005462646\n",
      "Epoch 1386/10000, Training Loss: 9.198184967041016, Validation Loss: 31.70196533203125\n",
      "Epoch 1387/10000, Training Loss: 8.239388465881348, Validation Loss: 10.790902137756348\n",
      "Epoch 1388/10000, Training Loss: 11.03819465637207, Validation Loss: 53.8638916015625\n",
      "Epoch 1389/10000, Training Loss: 6.674708843231201, Validation Loss: 5.119487285614014\n",
      "Epoch 1390/10000, Training Loss: 9.863908767700195, Validation Loss: 30.837656021118164\n",
      "Epoch 1391/10000, Training Loss: 4.114555358886719, Validation Loss: 8.883233070373535\n",
      "Epoch 1392/10000, Training Loss: 5.746233940124512, Validation Loss: 9.456582069396973\n",
      "Epoch 1393/10000, Training Loss: 11.779318809509277, Validation Loss: 12.058731079101562\n",
      "Epoch 1394/10000, Training Loss: 12.106586456298828, Validation Loss: 12.765686988830566\n",
      "Epoch 1395/10000, Training Loss: 7.2564005851745605, Validation Loss: 2.4388833045959473\n",
      "Epoch 1396/10000, Training Loss: 12.135257720947266, Validation Loss: 31.686906814575195\n",
      "Epoch 1397/10000, Training Loss: 9.666797637939453, Validation Loss: 7.777280807495117\n",
      "Epoch 1398/10000, Training Loss: 9.75670051574707, Validation Loss: 6.926784515380859\n",
      "Epoch 1399/10000, Training Loss: 6.674180030822754, Validation Loss: 8.823430061340332\n",
      "Epoch 1400/10000, Training Loss: 6.814033508300781, Validation Loss: 3.38936448097229\n",
      "Epoch 1401/10000, Training Loss: 9.249574661254883, Validation Loss: 3.518157958984375\n",
      "Epoch 1402/10000, Training Loss: 4.906831741333008, Validation Loss: 11.678009033203125\n",
      "Epoch 1403/10000, Training Loss: 9.625981330871582, Validation Loss: 5.77850341796875\n",
      "Epoch 1404/10000, Training Loss: 10.6600341796875, Validation Loss: 22.393484115600586\n",
      "Epoch 1405/10000, Training Loss: 8.413960456848145, Validation Loss: 7.885538578033447\n",
      "Epoch 1406/10000, Training Loss: 7.0374555587768555, Validation Loss: 4.263373851776123\n",
      "Epoch 1407/10000, Training Loss: 8.763450622558594, Validation Loss: 12.04655933380127\n",
      "Epoch 1408/10000, Training Loss: 11.228032112121582, Validation Loss: 7.263019561767578\n",
      "Epoch 1409/10000, Training Loss: 6.0477776527404785, Validation Loss: 9.623926162719727\n",
      "Epoch 1410/10000, Training Loss: 7.020052909851074, Validation Loss: 16.8433895111084\n",
      "Epoch 1411/10000, Training Loss: 8.678878784179688, Validation Loss: 6.918668746948242\n",
      "Epoch 1412/10000, Training Loss: 8.479862213134766, Validation Loss: 4.661104679107666\n",
      "Epoch 1413/10000, Training Loss: 6.477874279022217, Validation Loss: 3.9134209156036377\n",
      "Epoch 1414/10000, Training Loss: 7.974027156829834, Validation Loss: 14.567587852478027\n",
      "Epoch 1415/10000, Training Loss: 12.994945526123047, Validation Loss: 7.555917739868164\n",
      "Epoch 1416/10000, Training Loss: 6.643626689910889, Validation Loss: 9.006592750549316\n",
      "Epoch 1417/10000, Training Loss: 7.004090309143066, Validation Loss: 12.489466667175293\n",
      "Epoch 1418/10000, Training Loss: 6.521590232849121, Validation Loss: 5.483680248260498\n",
      "Epoch 1419/10000, Training Loss: 9.846803665161133, Validation Loss: 13.424266815185547\n",
      "Epoch 1420/10000, Training Loss: 10.959562301635742, Validation Loss: 8.200037002563477\n",
      "Epoch 1421/10000, Training Loss: 6.02589750289917, Validation Loss: 5.756652355194092\n",
      "Epoch 1422/10000, Training Loss: 5.776854991912842, Validation Loss: 9.00762939453125\n",
      "Epoch 1423/10000, Training Loss: 5.954833507537842, Validation Loss: 13.787914276123047\n",
      "Epoch 1424/10000, Training Loss: 7.478004455566406, Validation Loss: 2.2133610248565674\n",
      "Epoch 1425/10000, Training Loss: 5.947247505187988, Validation Loss: 5.259284973144531\n",
      "Epoch 1426/10000, Training Loss: 7.562504291534424, Validation Loss: 45.85174560546875\n",
      "Epoch 1427/10000, Training Loss: 5.188578128814697, Validation Loss: 5.300410270690918\n",
      "Epoch 1428/10000, Training Loss: 6.247828483581543, Validation Loss: 1.8683527708053589\n",
      "Epoch 1429/10000, Training Loss: 6.0301666259765625, Validation Loss: 2.9306640625\n",
      "Epoch 1430/10000, Training Loss: 7.26876163482666, Validation Loss: 25.958322525024414\n",
      "Epoch 1431/10000, Training Loss: 7.871882438659668, Validation Loss: 5.853723526000977\n",
      "Epoch 1432/10000, Training Loss: 5.153296947479248, Validation Loss: 0.9124775528907776\n",
      "Epoch 1433/10000, Training Loss: 6.565007209777832, Validation Loss: 1.5999091863632202\n",
      "Epoch 1434/10000, Training Loss: 6.93284273147583, Validation Loss: 2.759444236755371\n",
      "Epoch 1435/10000, Training Loss: 10.325276374816895, Validation Loss: 21.4412899017334\n",
      "Epoch 1436/10000, Training Loss: 6.593966007232666, Validation Loss: 10.389843940734863\n",
      "Epoch 1437/10000, Training Loss: 7.6381964683532715, Validation Loss: 11.372535705566406\n",
      "Epoch 1438/10000, Training Loss: 7.755254745483398, Validation Loss: 1.152136206626892\n",
      "Epoch 1439/10000, Training Loss: 8.192133903503418, Validation Loss: 6.82227087020874\n",
      "Epoch 1440/10000, Training Loss: 6.568047523498535, Validation Loss: 3.6453940868377686\n",
      "Epoch 1441/10000, Training Loss: 9.872991561889648, Validation Loss: 9.07093334197998\n",
      "Epoch 1442/10000, Training Loss: 10.48829460144043, Validation Loss: 26.241012573242188\n",
      "Epoch 1443/10000, Training Loss: 10.990782737731934, Validation Loss: 13.58071517944336\n",
      "Epoch 1444/10000, Training Loss: 7.991567611694336, Validation Loss: 13.142838478088379\n",
      "Epoch 1445/10000, Training Loss: 7.857604503631592, Validation Loss: 15.838371276855469\n",
      "Epoch 1446/10000, Training Loss: 5.467198848724365, Validation Loss: 7.0413384437561035\n",
      "Epoch 1447/10000, Training Loss: 6.872246742248535, Validation Loss: 1.9995945692062378\n",
      "Epoch 1448/10000, Training Loss: 9.95917797088623, Validation Loss: 10.241909980773926\n",
      "Epoch 1449/10000, Training Loss: 6.14202356338501, Validation Loss: 25.477582931518555\n",
      "Epoch 1450/10000, Training Loss: 8.237869262695312, Validation Loss: 5.216864109039307\n",
      "Epoch 1451/10000, Training Loss: 9.551258087158203, Validation Loss: 7.546514511108398\n",
      "Epoch 1452/10000, Training Loss: 7.223492622375488, Validation Loss: 2.8233489990234375\n",
      "Epoch 1453/10000, Training Loss: 6.728261947631836, Validation Loss: 17.862266540527344\n",
      "Epoch 1454/10000, Training Loss: 13.004840850830078, Validation Loss: 9.239823341369629\n",
      "Epoch 1455/10000, Training Loss: 8.761778831481934, Validation Loss: 5.739853382110596\n",
      "Epoch 1456/10000, Training Loss: 5.019726276397705, Validation Loss: 2.377612829208374\n",
      "Epoch 1457/10000, Training Loss: 8.101597785949707, Validation Loss: 5.432003021240234\n",
      "Epoch 1458/10000, Training Loss: 14.005060195922852, Validation Loss: 11.196979522705078\n",
      "Epoch 1459/10000, Training Loss: 7.763016700744629, Validation Loss: 4.307349681854248\n",
      "Epoch 1460/10000, Training Loss: 7.3279008865356445, Validation Loss: 11.425575256347656\n",
      "Epoch 1461/10000, Training Loss: 3.954207420349121, Validation Loss: 1.3642855882644653\n",
      "Epoch 1462/10000, Training Loss: 12.80062198638916, Validation Loss: 6.91241979598999\n",
      "Epoch 1463/10000, Training Loss: 7.836277961730957, Validation Loss: 12.534937858581543\n",
      "Epoch 1464/10000, Training Loss: 5.430330753326416, Validation Loss: 4.0765581130981445\n",
      "Epoch 1465/10000, Training Loss: 6.9522318840026855, Validation Loss: 8.155898094177246\n",
      "Epoch 1466/10000, Training Loss: 11.798564910888672, Validation Loss: 3.0952975749969482\n",
      "Epoch 1467/10000, Training Loss: 14.92319107055664, Validation Loss: 6.567972660064697\n",
      "Epoch 1468/10000, Training Loss: 5.8607001304626465, Validation Loss: 10.575885772705078\n",
      "Epoch 1469/10000, Training Loss: 7.243042945861816, Validation Loss: 8.437642097473145\n",
      "Epoch 1470/10000, Training Loss: 7.204511642456055, Validation Loss: 11.76689624786377\n",
      "Epoch 1471/10000, Training Loss: 5.965792179107666, Validation Loss: 7.310290813446045\n",
      "Epoch 1472/10000, Training Loss: 11.478215217590332, Validation Loss: 3.4217071533203125\n",
      "Epoch 1473/10000, Training Loss: 7.184226036071777, Validation Loss: 8.7688627243042\n",
      "Epoch 1474/10000, Training Loss: 5.230774879455566, Validation Loss: 3.6200950145721436\n",
      "Epoch 1475/10000, Training Loss: 7.527076244354248, Validation Loss: 3.1429011821746826\n",
      "Epoch 1476/10000, Training Loss: 10.234156608581543, Validation Loss: 13.97257137298584\n",
      "Epoch 1477/10000, Training Loss: 11.678842544555664, Validation Loss: 2.7328574657440186\n",
      "Epoch 1478/10000, Training Loss: 9.512943267822266, Validation Loss: 40.781272888183594\n",
      "Epoch 1479/10000, Training Loss: 11.77072811126709, Validation Loss: 9.54146671295166\n",
      "Epoch 1480/10000, Training Loss: 6.497752666473389, Validation Loss: 6.25252103805542\n",
      "Epoch 1481/10000, Training Loss: 7.294683456420898, Validation Loss: 12.028839111328125\n",
      "Epoch 1482/10000, Training Loss: 5.960434913635254, Validation Loss: 5.3673248291015625\n",
      "Epoch 1483/10000, Training Loss: 7.07341194152832, Validation Loss: 6.3873610496521\n",
      "Epoch 1484/10000, Training Loss: 4.983683109283447, Validation Loss: 10.651579856872559\n",
      "Epoch 1485/10000, Training Loss: 6.7817840576171875, Validation Loss: 5.570157527923584\n",
      "Epoch 1486/10000, Training Loss: 6.145990371704102, Validation Loss: 14.315081596374512\n",
      "Epoch 1487/10000, Training Loss: 10.606496810913086, Validation Loss: 10.469685554504395\n",
      "Epoch 1488/10000, Training Loss: 6.376058578491211, Validation Loss: 4.695753574371338\n",
      "Epoch 1489/10000, Training Loss: 6.0409979820251465, Validation Loss: 6.538862228393555\n",
      "Epoch 1490/10000, Training Loss: 7.0729570388793945, Validation Loss: 6.837224960327148\n",
      "Epoch 1491/10000, Training Loss: 14.744572639465332, Validation Loss: 3.1037991046905518\n",
      "Epoch 1492/10000, Training Loss: 5.666165351867676, Validation Loss: 3.1969833374023438\n",
      "Epoch 1493/10000, Training Loss: 4.306215286254883, Validation Loss: 9.555331230163574\n",
      "Epoch 1494/10000, Training Loss: 9.168757438659668, Validation Loss: 9.158551216125488\n",
      "Epoch 1495/10000, Training Loss: 6.694007396697998, Validation Loss: 6.821281909942627\n",
      "Epoch 1496/10000, Training Loss: 6.979748249053955, Validation Loss: 29.967172622680664\n",
      "Epoch 1497/10000, Training Loss: 7.928679466247559, Validation Loss: 3.0975139141082764\n",
      "Epoch 1498/10000, Training Loss: 4.270412921905518, Validation Loss: 5.767290115356445\n",
      "Epoch 1499/10000, Training Loss: 6.388973712921143, Validation Loss: 7.23870325088501\n",
      "Epoch 1500/10000, Training Loss: 9.051430702209473, Validation Loss: 14.930991172790527\n",
      "Epoch 1501/10000, Training Loss: 7.130213737487793, Validation Loss: 10.241303443908691\n",
      "Epoch 1502/10000, Training Loss: 7.518072128295898, Validation Loss: 12.924823760986328\n",
      "Epoch 1503/10000, Training Loss: 6.823660850524902, Validation Loss: 15.22372817993164\n",
      "Epoch 1504/10000, Training Loss: 8.293912887573242, Validation Loss: 6.695302963256836\n",
      "Epoch 1505/10000, Training Loss: 7.336501121520996, Validation Loss: 7.8616509437561035\n",
      "Epoch 1506/10000, Training Loss: 6.552763938903809, Validation Loss: 7.113344669342041\n",
      "Epoch 1507/10000, Training Loss: 4.847725868225098, Validation Loss: 7.964117527008057\n",
      "Epoch 1508/10000, Training Loss: 8.17573070526123, Validation Loss: 10.485623359680176\n",
      "Epoch 1509/10000, Training Loss: 7.74391508102417, Validation Loss: 2.9621031284332275\n",
      "Epoch 1510/10000, Training Loss: 8.483562469482422, Validation Loss: 7.0326151847839355\n",
      "Epoch 1511/10000, Training Loss: 7.809664726257324, Validation Loss: 1.0416862964630127\n",
      "Epoch 1512/10000, Training Loss: 8.775796890258789, Validation Loss: 1.0287729501724243\n",
      "Epoch 1513/10000, Training Loss: 7.5383100509643555, Validation Loss: 9.196749687194824\n",
      "Epoch 1514/10000, Training Loss: 7.029358863830566, Validation Loss: 16.849349975585938\n",
      "Epoch 1515/10000, Training Loss: 6.105260848999023, Validation Loss: 6.378208637237549\n",
      "Epoch 1516/10000, Training Loss: 8.888946533203125, Validation Loss: 7.277132034301758\n",
      "Epoch 1517/10000, Training Loss: 7.338644981384277, Validation Loss: 18.145376205444336\n",
      "Epoch 1518/10000, Training Loss: 11.885979652404785, Validation Loss: 15.344691276550293\n",
      "Epoch 1519/10000, Training Loss: 8.45136833190918, Validation Loss: 5.228878974914551\n",
      "Epoch 1520/10000, Training Loss: 9.66207504272461, Validation Loss: 23.287538528442383\n",
      "Epoch 1521/10000, Training Loss: 6.261650562286377, Validation Loss: 20.66243553161621\n",
      "Epoch 1522/10000, Training Loss: 5.5060906410217285, Validation Loss: 2.1425538063049316\n",
      "Epoch 1523/10000, Training Loss: 3.6958491802215576, Validation Loss: 6.132562637329102\n",
      "Epoch 1524/10000, Training Loss: 8.281546592712402, Validation Loss: 7.735044956207275\n",
      "Epoch 1525/10000, Training Loss: 9.248869895935059, Validation Loss: 5.806972503662109\n",
      "Epoch 1526/10000, Training Loss: 7.622375965118408, Validation Loss: 5.37582540512085\n",
      "Epoch 1527/10000, Training Loss: 9.142560958862305, Validation Loss: 4.7838358879089355\n",
      "Epoch 1528/10000, Training Loss: 7.173827648162842, Validation Loss: 7.969066619873047\n",
      "Epoch 1529/10000, Training Loss: 7.333164691925049, Validation Loss: 22.393775939941406\n",
      "Epoch 1530/10000, Training Loss: 5.669623851776123, Validation Loss: 14.835659980773926\n",
      "Epoch 1531/10000, Training Loss: 8.055471420288086, Validation Loss: 3.955965042114258\n",
      "Epoch 1532/10000, Training Loss: 7.65993595123291, Validation Loss: 7.69913911819458\n",
      "Epoch 1533/10000, Training Loss: 8.102446556091309, Validation Loss: 8.202214241027832\n",
      "Epoch 1534/10000, Training Loss: 8.75599479675293, Validation Loss: 20.814966201782227\n",
      "Epoch 1535/10000, Training Loss: 10.527510643005371, Validation Loss: 10.862833976745605\n",
      "Epoch 1536/10000, Training Loss: 6.343932151794434, Validation Loss: 5.378978729248047\n",
      "Epoch 1537/10000, Training Loss: 7.846142292022705, Validation Loss: 9.662766456604004\n",
      "Epoch 1538/10000, Training Loss: 10.21617317199707, Validation Loss: 9.151741981506348\n",
      "Epoch 1539/10000, Training Loss: 6.88014554977417, Validation Loss: 5.452874660491943\n",
      "Epoch 1540/10000, Training Loss: 10.441877365112305, Validation Loss: 12.754398345947266\n",
      "Epoch 1541/10000, Training Loss: 6.446375846862793, Validation Loss: 3.208921432495117\n",
      "Epoch 1542/10000, Training Loss: 10.539958953857422, Validation Loss: 3.9315922260284424\n",
      "Epoch 1543/10000, Training Loss: 6.347864627838135, Validation Loss: 3.8747928142547607\n",
      "Epoch 1544/10000, Training Loss: 7.6035590171813965, Validation Loss: 7.301290512084961\n",
      "Epoch 1545/10000, Training Loss: 5.926155090332031, Validation Loss: 5.603885650634766\n",
      "Epoch 1546/10000, Training Loss: 11.733499526977539, Validation Loss: 14.052462577819824\n",
      "Epoch 1547/10000, Training Loss: 7.302002906799316, Validation Loss: 7.112678527832031\n",
      "Epoch 1548/10000, Training Loss: 5.177391529083252, Validation Loss: 4.017764091491699\n",
      "Epoch 1549/10000, Training Loss: 4.947525978088379, Validation Loss: 52.048160552978516\n",
      "Epoch 1550/10000, Training Loss: 5.870561599731445, Validation Loss: 13.048806190490723\n",
      "Epoch 1551/10000, Training Loss: 11.25709056854248, Validation Loss: 26.164247512817383\n",
      "Epoch 1552/10000, Training Loss: 4.696423053741455, Validation Loss: 5.748943328857422\n",
      "Epoch 1553/10000, Training Loss: 6.232140064239502, Validation Loss: 2.120602607727051\n",
      "Epoch 1554/10000, Training Loss: 5.63776159286499, Validation Loss: 2.4741294384002686\n",
      "Epoch 1555/10000, Training Loss: 5.074679374694824, Validation Loss: 0.4361182153224945\n",
      "Epoch 1556/10000, Training Loss: 7.819362163543701, Validation Loss: 4.565511226654053\n",
      "Epoch 1557/10000, Training Loss: 3.514021635055542, Validation Loss: 0.5555453896522522\n",
      "Epoch 1558/10000, Training Loss: 6.917500972747803, Validation Loss: 1.8578394651412964\n",
      "Epoch 1559/10000, Training Loss: 6.895409107208252, Validation Loss: 38.83409881591797\n",
      "Epoch 1560/10000, Training Loss: 7.668501853942871, Validation Loss: 10.37027359008789\n",
      "Epoch 1561/10000, Training Loss: 7.085223197937012, Validation Loss: 6.934683322906494\n",
      "Epoch 1562/10000, Training Loss: 5.198137283325195, Validation Loss: 9.04560375213623\n",
      "Epoch 1563/10000, Training Loss: 5.7223896980285645, Validation Loss: 8.060333251953125\n",
      "Epoch 1564/10000, Training Loss: 4.880912780761719, Validation Loss: 4.345864772796631\n",
      "Epoch 1565/10000, Training Loss: 13.929312705993652, Validation Loss: 3.6625988483428955\n",
      "Epoch 1566/10000, Training Loss: 9.316397666931152, Validation Loss: 3.536729574203491\n",
      "Epoch 1567/10000, Training Loss: 9.284403800964355, Validation Loss: 16.751771926879883\n",
      "Epoch 1568/10000, Training Loss: 8.701216697692871, Validation Loss: 6.352240085601807\n",
      "Epoch 1569/10000, Training Loss: 10.258955955505371, Validation Loss: 8.18178939819336\n",
      "Epoch 1570/10000, Training Loss: 9.646788597106934, Validation Loss: 0.7391901016235352\n",
      "Epoch 1571/10000, Training Loss: 7.2906575202941895, Validation Loss: 8.252280235290527\n",
      "Epoch 1572/10000, Training Loss: 12.119024276733398, Validation Loss: 11.244128227233887\n",
      "Epoch 1573/10000, Training Loss: 8.68832015991211, Validation Loss: 5.19831657409668\n",
      "Epoch 1574/10000, Training Loss: 6.721614360809326, Validation Loss: 5.011366367340088\n",
      "Epoch 1575/10000, Training Loss: 7.6590704917907715, Validation Loss: 3.979903221130371\n",
      "Epoch 1576/10000, Training Loss: 7.517029285430908, Validation Loss: 8.59238338470459\n",
      "Epoch 1577/10000, Training Loss: 8.0023775100708, Validation Loss: 12.099164009094238\n",
      "Epoch 1578/10000, Training Loss: 6.0563578605651855, Validation Loss: 9.349181175231934\n",
      "Epoch 1579/10000, Training Loss: 5.415591716766357, Validation Loss: 11.803874015808105\n",
      "Epoch 1580/10000, Training Loss: 4.798407077789307, Validation Loss: 4.611844539642334\n",
      "Epoch 1581/10000, Training Loss: 6.171165466308594, Validation Loss: 15.054938316345215\n",
      "Epoch 1582/10000, Training Loss: 6.3729143142700195, Validation Loss: 56.9498176574707\n",
      "Epoch 1583/10000, Training Loss: 4.583009719848633, Validation Loss: 5.157186031341553\n",
      "Epoch 1584/10000, Training Loss: 6.964345455169678, Validation Loss: 7.416013717651367\n",
      "Epoch 1585/10000, Training Loss: 6.907102584838867, Validation Loss: 8.226832389831543\n",
      "Epoch 1586/10000, Training Loss: 7.293818473815918, Validation Loss: 3.5734643936157227\n",
      "Epoch 1587/10000, Training Loss: 7.93937349319458, Validation Loss: 6.91542387008667\n",
      "Epoch 1588/10000, Training Loss: 8.063450813293457, Validation Loss: 3.734766960144043\n",
      "Epoch 1589/10000, Training Loss: 4.654751777648926, Validation Loss: 5.7056884765625\n",
      "Epoch 1590/10000, Training Loss: 10.451003074645996, Validation Loss: 3.253753900527954\n",
      "Epoch 1591/10000, Training Loss: 8.213857650756836, Validation Loss: 18.207162857055664\n",
      "Epoch 1592/10000, Training Loss: 8.553879737854004, Validation Loss: 8.345959663391113\n",
      "Epoch 1593/10000, Training Loss: 7.878230094909668, Validation Loss: 11.6491117477417\n",
      "Epoch 1594/10000, Training Loss: 9.600573539733887, Validation Loss: 23.57539939880371\n",
      "Epoch 1595/10000, Training Loss: 7.688893795013428, Validation Loss: 9.154962539672852\n",
      "Epoch 1596/10000, Training Loss: 6.6110005378723145, Validation Loss: 8.02536678314209\n",
      "Epoch 1597/10000, Training Loss: 12.47784423828125, Validation Loss: 9.442180633544922\n",
      "Epoch 1598/10000, Training Loss: 4.928945064544678, Validation Loss: 7.465410232543945\n",
      "Epoch 1599/10000, Training Loss: 4.257323265075684, Validation Loss: 1.7268080711364746\n",
      "Epoch 1600/10000, Training Loss: 6.7852091789245605, Validation Loss: 28.77982521057129\n",
      "Epoch 1601/10000, Training Loss: 4.710618019104004, Validation Loss: 8.117475509643555\n",
      "Epoch 1602/10000, Training Loss: 4.721542835235596, Validation Loss: 6.293495178222656\n",
      "Epoch 1603/10000, Training Loss: 9.406475067138672, Validation Loss: 16.788766860961914\n",
      "Epoch 1604/10000, Training Loss: 6.843304634094238, Validation Loss: 4.421675205230713\n",
      "Epoch 1605/10000, Training Loss: 6.390754699707031, Validation Loss: 6.332147121429443\n",
      "Epoch 1606/10000, Training Loss: 8.187602996826172, Validation Loss: 9.751150131225586\n",
      "Epoch 1607/10000, Training Loss: 6.5336103439331055, Validation Loss: 4.1915740966796875\n",
      "Epoch 1608/10000, Training Loss: 7.103582382202148, Validation Loss: 2.9311962127685547\n",
      "Epoch 1609/10000, Training Loss: 3.9462740421295166, Validation Loss: 12.119064331054688\n",
      "Epoch 1610/10000, Training Loss: 6.970728874206543, Validation Loss: 4.927606582641602\n",
      "Epoch 1611/10000, Training Loss: 5.2413458824157715, Validation Loss: 7.2606940269470215\n",
      "Epoch 1612/10000, Training Loss: 7.546695232391357, Validation Loss: 10.227639198303223\n",
      "Epoch 1613/10000, Training Loss: 12.39065933227539, Validation Loss: 14.86325740814209\n",
      "Epoch 1614/10000, Training Loss: 5.249944686889648, Validation Loss: 7.11213493347168\n",
      "Epoch 1615/10000, Training Loss: 5.693528175354004, Validation Loss: 1.5267521142959595\n",
      "Epoch 1616/10000, Training Loss: 7.244510173797607, Validation Loss: 6.811759948730469\n",
      "Epoch 1617/10000, Training Loss: 10.051234245300293, Validation Loss: 19.969993591308594\n",
      "Epoch 1618/10000, Training Loss: 8.372976303100586, Validation Loss: 13.116375923156738\n",
      "Epoch 1619/10000, Training Loss: 5.780789375305176, Validation Loss: 14.597175598144531\n",
      "Epoch 1620/10000, Training Loss: 5.49318265914917, Validation Loss: 1.9519906044006348\n",
      "Epoch 1621/10000, Training Loss: 3.792527437210083, Validation Loss: 1.196365475654602\n",
      "Epoch 1622/10000, Training Loss: 5.087021827697754, Validation Loss: 4.5127434730529785\n",
      "Epoch 1623/10000, Training Loss: 6.447296142578125, Validation Loss: 9.761435508728027\n",
      "Epoch 1624/10000, Training Loss: 5.118342399597168, Validation Loss: 1.777788758277893\n",
      "Epoch 1625/10000, Training Loss: 11.184602737426758, Validation Loss: 13.25442123413086\n",
      "Epoch 1626/10000, Training Loss: 6.694611072540283, Validation Loss: 4.5973219871521\n",
      "Epoch 1627/10000, Training Loss: 8.543069839477539, Validation Loss: 6.964967727661133\n",
      "Epoch 1628/10000, Training Loss: 5.092911720275879, Validation Loss: 5.886997699737549\n",
      "Epoch 1629/10000, Training Loss: 9.086950302124023, Validation Loss: 9.39704418182373\n",
      "Epoch 1630/10000, Training Loss: 4.849939346313477, Validation Loss: 16.605243682861328\n",
      "Epoch 1631/10000, Training Loss: 5.8331475257873535, Validation Loss: 10.624324798583984\n",
      "Epoch 1632/10000, Training Loss: 6.036557674407959, Validation Loss: 3.0000057220458984\n",
      "Epoch 1633/10000, Training Loss: 3.8061165809631348, Validation Loss: 0.2892855107784271\n",
      "Epoch 1634/10000, Training Loss: 9.073080062866211, Validation Loss: 19.041837692260742\n",
      "Epoch 1635/10000, Training Loss: 7.3611040115356445, Validation Loss: 6.536447048187256\n",
      "Epoch 1636/10000, Training Loss: 8.591142654418945, Validation Loss: 10.39373779296875\n",
      "Epoch 1637/10000, Training Loss: 4.971889972686768, Validation Loss: 3.150806188583374\n",
      "Epoch 1638/10000, Training Loss: 7.906564712524414, Validation Loss: 11.704718589782715\n",
      "Epoch 1639/10000, Training Loss: 7.718062400817871, Validation Loss: 17.44386863708496\n",
      "Epoch 1640/10000, Training Loss: 6.858310222625732, Validation Loss: 5.912482738494873\n",
      "Epoch 1641/10000, Training Loss: 7.574472904205322, Validation Loss: 5.359194278717041\n",
      "Epoch 1642/10000, Training Loss: 9.484670639038086, Validation Loss: 12.8728666305542\n",
      "Epoch 1643/10000, Training Loss: 7.967124938964844, Validation Loss: 11.491251945495605\n",
      "Epoch 1644/10000, Training Loss: 9.769510269165039, Validation Loss: 6.433567523956299\n",
      "Epoch 1645/10000, Training Loss: 8.60749340057373, Validation Loss: 9.48156452178955\n",
      "Epoch 1646/10000, Training Loss: 5.521421432495117, Validation Loss: 5.0120038986206055\n",
      "Epoch 1647/10000, Training Loss: 10.912805557250977, Validation Loss: 9.105856895446777\n",
      "Epoch 1648/10000, Training Loss: 7.890978813171387, Validation Loss: 6.872747898101807\n",
      "Epoch 1649/10000, Training Loss: 12.482985496520996, Validation Loss: 16.129119873046875\n",
      "Epoch 1650/10000, Training Loss: 8.30440616607666, Validation Loss: 42.16001510620117\n",
      "Epoch 1651/10000, Training Loss: 4.696297645568848, Validation Loss: 6.6665802001953125\n",
      "Epoch 1652/10000, Training Loss: 6.204105854034424, Validation Loss: 2.791330337524414\n",
      "Epoch 1653/10000, Training Loss: 5.981101036071777, Validation Loss: 8.100716590881348\n",
      "Epoch 1654/10000, Training Loss: 6.717694282531738, Validation Loss: 7.666492938995361\n",
      "Epoch 1655/10000, Training Loss: 12.178141593933105, Validation Loss: 12.445775032043457\n",
      "Epoch 1656/10000, Training Loss: 5.900522708892822, Validation Loss: 7.7131171226501465\n",
      "Epoch 1657/10000, Training Loss: 9.28950309753418, Validation Loss: 17.133638381958008\n",
      "Epoch 1658/10000, Training Loss: 4.53945779800415, Validation Loss: 1.9588016271591187\n",
      "Epoch 1659/10000, Training Loss: 7.46628999710083, Validation Loss: 3.3099148273468018\n",
      "Epoch 1660/10000, Training Loss: 7.3954386711120605, Validation Loss: 3.6598308086395264\n",
      "Epoch 1661/10000, Training Loss: 10.674365997314453, Validation Loss: 4.486581802368164\n",
      "Epoch 1662/10000, Training Loss: 6.880753517150879, Validation Loss: 9.40597152709961\n",
      "Epoch 1663/10000, Training Loss: 12.718017578125, Validation Loss: 12.969544410705566\n",
      "Epoch 1664/10000, Training Loss: 6.905974388122559, Validation Loss: 5.2808451652526855\n",
      "Epoch 1665/10000, Training Loss: 7.194855690002441, Validation Loss: 3.685277223587036\n",
      "Epoch 1666/10000, Training Loss: 4.516686916351318, Validation Loss: 7.537903308868408\n",
      "Epoch 1667/10000, Training Loss: 6.651952743530273, Validation Loss: 1.6642862558364868\n",
      "Epoch 1668/10000, Training Loss: 4.8848466873168945, Validation Loss: 1.214103102684021\n",
      "Epoch 1669/10000, Training Loss: 7.1401472091674805, Validation Loss: 7.680643558502197\n",
      "Epoch 1670/10000, Training Loss: 8.634724617004395, Validation Loss: 16.50489616394043\n",
      "Epoch 1671/10000, Training Loss: 10.148727416992188, Validation Loss: 21.1977596282959\n",
      "Epoch 1672/10000, Training Loss: 11.37409496307373, Validation Loss: 9.344849586486816\n",
      "Epoch 1673/10000, Training Loss: 6.305247783660889, Validation Loss: 10.373637199401855\n",
      "Epoch 1674/10000, Training Loss: 4.2823286056518555, Validation Loss: 6.529077053070068\n",
      "Epoch 1675/10000, Training Loss: 7.312093734741211, Validation Loss: 7.567384719848633\n",
      "Epoch 1676/10000, Training Loss: 4.501494884490967, Validation Loss: 7.082388401031494\n",
      "Epoch 1677/10000, Training Loss: 6.400945663452148, Validation Loss: 8.795363426208496\n",
      "Epoch 1678/10000, Training Loss: 8.464664459228516, Validation Loss: 8.986348152160645\n",
      "Epoch 1679/10000, Training Loss: 4.078640937805176, Validation Loss: 2.116558790206909\n",
      "Epoch 1680/10000, Training Loss: 10.272884368896484, Validation Loss: 4.721766948699951\n",
      "Epoch 1681/10000, Training Loss: 8.457854270935059, Validation Loss: 11.16248607635498\n",
      "Epoch 1682/10000, Training Loss: 6.0760674476623535, Validation Loss: 7.695407390594482\n",
      "Epoch 1683/10000, Training Loss: 6.778519630432129, Validation Loss: 5.884130001068115\n",
      "Epoch 1684/10000, Training Loss: 7.413662433624268, Validation Loss: 7.732198238372803\n",
      "Epoch 1685/10000, Training Loss: 5.442070484161377, Validation Loss: 3.4688634872436523\n",
      "Epoch 1686/10000, Training Loss: 7.613593101501465, Validation Loss: 9.10187816619873\n",
      "Epoch 1687/10000, Training Loss: 9.448662757873535, Validation Loss: 10.651369094848633\n",
      "Epoch 1688/10000, Training Loss: 5.719401836395264, Validation Loss: 15.103947639465332\n",
      "Epoch 1689/10000, Training Loss: 4.987069129943848, Validation Loss: 1.555180549621582\n",
      "Epoch 1690/10000, Training Loss: 8.057967185974121, Validation Loss: 6.802438259124756\n",
      "Epoch 1691/10000, Training Loss: 5.198171615600586, Validation Loss: 3.2514076232910156\n",
      "Epoch 1692/10000, Training Loss: 5.882891654968262, Validation Loss: 4.75497579574585\n",
      "Epoch 1693/10000, Training Loss: 6.650993824005127, Validation Loss: 10.178932189941406\n",
      "Epoch 1694/10000, Training Loss: 6.809376239776611, Validation Loss: 8.430213928222656\n",
      "Epoch 1695/10000, Training Loss: 4.657247066497803, Validation Loss: 7.53831148147583\n",
      "Epoch 1696/10000, Training Loss: 5.853302001953125, Validation Loss: 2.018131971359253\n",
      "Epoch 1697/10000, Training Loss: 4.104250431060791, Validation Loss: 4.469501972198486\n",
      "Epoch 1698/10000, Training Loss: 5.290448188781738, Validation Loss: 4.912506580352783\n",
      "Epoch 1699/10000, Training Loss: 6.066889762878418, Validation Loss: 3.643320083618164\n",
      "Epoch 1700/10000, Training Loss: 7.98461389541626, Validation Loss: 10.493420600891113\n",
      "Epoch 1701/10000, Training Loss: 10.690537452697754, Validation Loss: 4.6025872230529785\n",
      "Epoch 1702/10000, Training Loss: 6.220582008361816, Validation Loss: 2.7416365146636963\n",
      "Epoch 1703/10000, Training Loss: 6.678589344024658, Validation Loss: 9.553008079528809\n",
      "Epoch 1704/10000, Training Loss: 6.3695478439331055, Validation Loss: 12.181700706481934\n",
      "Epoch 1705/10000, Training Loss: 7.243842601776123, Validation Loss: 8.181350708007812\n",
      "Epoch 1706/10000, Training Loss: 6.048096656799316, Validation Loss: 11.294517517089844\n",
      "Epoch 1707/10000, Training Loss: 6.2994184494018555, Validation Loss: 4.700138568878174\n",
      "Epoch 1708/10000, Training Loss: 4.66933536529541, Validation Loss: 6.875816345214844\n",
      "Epoch 1709/10000, Training Loss: 3.7146542072296143, Validation Loss: 2.6215779781341553\n",
      "Epoch 1710/10000, Training Loss: 4.753234386444092, Validation Loss: 11.340553283691406\n",
      "Epoch 1711/10000, Training Loss: 5.204441547393799, Validation Loss: 9.955451011657715\n",
      "Epoch 1712/10000, Training Loss: 6.995336055755615, Validation Loss: 4.480993270874023\n",
      "Epoch 1713/10000, Training Loss: 6.389114856719971, Validation Loss: 2.128235101699829\n",
      "Epoch 1714/10000, Training Loss: 5.160079002380371, Validation Loss: 6.452052593231201\n",
      "Epoch 1715/10000, Training Loss: 6.255660057067871, Validation Loss: 12.70937442779541\n",
      "Epoch 1716/10000, Training Loss: 8.65384578704834, Validation Loss: 6.833017826080322\n",
      "Epoch 1717/10000, Training Loss: 6.81292200088501, Validation Loss: 6.166738986968994\n",
      "Epoch 1718/10000, Training Loss: 5.788529396057129, Validation Loss: 12.984585762023926\n",
      "Epoch 1719/10000, Training Loss: 6.525260925292969, Validation Loss: 4.633838653564453\n",
      "Epoch 1720/10000, Training Loss: 11.80586051940918, Validation Loss: 22.47612953186035\n",
      "Epoch 1721/10000, Training Loss: 5.440147876739502, Validation Loss: 10.468460083007812\n",
      "Epoch 1722/10000, Training Loss: 10.914381980895996, Validation Loss: 4.142559051513672\n",
      "Epoch 1723/10000, Training Loss: 8.325825691223145, Validation Loss: 4.877064228057861\n",
      "Epoch 1724/10000, Training Loss: 5.485001087188721, Validation Loss: 15.789589881896973\n",
      "Epoch 1725/10000, Training Loss: 8.098264694213867, Validation Loss: 24.098604202270508\n",
      "Epoch 1726/10000, Training Loss: 7.2819037437438965, Validation Loss: 3.3717708587646484\n",
      "Epoch 1727/10000, Training Loss: 7.23334264755249, Validation Loss: 16.45485496520996\n",
      "Epoch 1728/10000, Training Loss: 5.942988395690918, Validation Loss: 6.411212921142578\n",
      "Epoch 1729/10000, Training Loss: 6.9444580078125, Validation Loss: 10.259020805358887\n",
      "Epoch 1730/10000, Training Loss: 12.113771438598633, Validation Loss: 10.754866600036621\n",
      "Epoch 1731/10000, Training Loss: 13.144487380981445, Validation Loss: 8.748129844665527\n",
      "Epoch 1732/10000, Training Loss: 7.4142255783081055, Validation Loss: 9.136467933654785\n",
      "Epoch 1733/10000, Training Loss: 4.543954849243164, Validation Loss: 9.231561660766602\n",
      "Epoch 1734/10000, Training Loss: 4.499124526977539, Validation Loss: 2.372159719467163\n",
      "Epoch 1735/10000, Training Loss: 3.815406560897827, Validation Loss: 2.8333511352539062\n",
      "Epoch 1736/10000, Training Loss: 4.393959999084473, Validation Loss: 5.189800262451172\n",
      "Epoch 1737/10000, Training Loss: 8.981026649475098, Validation Loss: 14.725299835205078\n",
      "Epoch 1738/10000, Training Loss: 6.068165302276611, Validation Loss: 4.275611877441406\n",
      "Epoch 1739/10000, Training Loss: 4.7133073806762695, Validation Loss: 3.189885139465332\n",
      "Epoch 1740/10000, Training Loss: 6.338860034942627, Validation Loss: 8.130902290344238\n",
      "Epoch 1741/10000, Training Loss: 5.147002696990967, Validation Loss: 8.565081596374512\n",
      "Epoch 1742/10000, Training Loss: 5.605184555053711, Validation Loss: 2.595834493637085\n",
      "Epoch 1743/10000, Training Loss: 6.071175575256348, Validation Loss: 7.2657470703125\n",
      "Epoch 1744/10000, Training Loss: 11.289220809936523, Validation Loss: 7.503881931304932\n",
      "Epoch 1745/10000, Training Loss: 6.191929817199707, Validation Loss: 8.937432289123535\n",
      "Epoch 1746/10000, Training Loss: 4.0285563468933105, Validation Loss: 2.9348604679107666\n",
      "Epoch 1747/10000, Training Loss: 8.348245620727539, Validation Loss: 13.469158172607422\n",
      "Epoch 1748/10000, Training Loss: 4.157192707061768, Validation Loss: 7.075196743011475\n",
      "Epoch 1749/10000, Training Loss: 6.572018623352051, Validation Loss: 10.916983604431152\n",
      "Epoch 1750/10000, Training Loss: 6.2293620109558105, Validation Loss: 7.715219974517822\n",
      "Epoch 1751/10000, Training Loss: 6.527002334594727, Validation Loss: 5.456364154815674\n",
      "Epoch 1752/10000, Training Loss: 6.8236541748046875, Validation Loss: 4.153262615203857\n",
      "Epoch 1753/10000, Training Loss: 3.7552223205566406, Validation Loss: 0.9747506976127625\n",
      "Epoch 1754/10000, Training Loss: 5.826475143432617, Validation Loss: 1.228045105934143\n",
      "Epoch 1755/10000, Training Loss: 7.205461502075195, Validation Loss: 3.477454423904419\n",
      "Epoch 1756/10000, Training Loss: 7.731503009796143, Validation Loss: 6.913393497467041\n",
      "Epoch 1757/10000, Training Loss: 10.311687469482422, Validation Loss: 24.348045349121094\n",
      "Epoch 1758/10000, Training Loss: 4.806247711181641, Validation Loss: 4.26269006729126\n",
      "Epoch 1759/10000, Training Loss: 6.222614765167236, Validation Loss: 7.251054763793945\n",
      "Epoch 1760/10000, Training Loss: 5.017928600311279, Validation Loss: 12.97626781463623\n",
      "Epoch 1761/10000, Training Loss: 5.413285255432129, Validation Loss: 2.283329725265503\n",
      "Epoch 1762/10000, Training Loss: 7.116389274597168, Validation Loss: 3.628615617752075\n",
      "Epoch 1763/10000, Training Loss: 5.993672847747803, Validation Loss: 10.874161720275879\n",
      "Epoch 1764/10000, Training Loss: 5.315465927124023, Validation Loss: 9.6239013671875\n",
      "Epoch 1765/10000, Training Loss: 7.233340740203857, Validation Loss: 7.308372974395752\n",
      "Epoch 1766/10000, Training Loss: 6.397684097290039, Validation Loss: 15.361477851867676\n",
      "Epoch 1767/10000, Training Loss: 5.567087173461914, Validation Loss: 3.3411943912506104\n",
      "Epoch 1768/10000, Training Loss: 7.393923759460449, Validation Loss: 5.2809739112854\n",
      "Epoch 1769/10000, Training Loss: 4.105785846710205, Validation Loss: 4.79808235168457\n",
      "Epoch 1770/10000, Training Loss: 5.4637603759765625, Validation Loss: 12.76237964630127\n",
      "Epoch 1771/10000, Training Loss: 6.104445934295654, Validation Loss: 17.900197982788086\n",
      "Epoch 1772/10000, Training Loss: 4.67939567565918, Validation Loss: 5.79431676864624\n",
      "Epoch 1773/10000, Training Loss: 6.191201210021973, Validation Loss: 27.209938049316406\n",
      "Epoch 1774/10000, Training Loss: 3.8515727519989014, Validation Loss: 2.7455875873565674\n",
      "Epoch 1775/10000, Training Loss: 3.9966554641723633, Validation Loss: 0.835500955581665\n",
      "Epoch 1776/10000, Training Loss: 6.676987171173096, Validation Loss: 11.863224029541016\n",
      "Epoch 1777/10000, Training Loss: 9.534660339355469, Validation Loss: 12.64175796508789\n",
      "Epoch 1778/10000, Training Loss: 8.753385543823242, Validation Loss: 5.664927005767822\n",
      "Epoch 1779/10000, Training Loss: 6.552626609802246, Validation Loss: 11.312137603759766\n",
      "Epoch 1780/10000, Training Loss: 4.405309200286865, Validation Loss: 4.036988258361816\n",
      "Epoch 1781/10000, Training Loss: 7.788576602935791, Validation Loss: 5.3672356605529785\n",
      "Epoch 1782/10000, Training Loss: 5.839498043060303, Validation Loss: 7.101364612579346\n",
      "Epoch 1783/10000, Training Loss: 6.590697288513184, Validation Loss: 14.345377922058105\n",
      "Epoch 1784/10000, Training Loss: 6.133047103881836, Validation Loss: 5.247798919677734\n",
      "Epoch 1785/10000, Training Loss: 5.7452216148376465, Validation Loss: 5.42857551574707\n",
      "Epoch 1786/10000, Training Loss: 4.527001857757568, Validation Loss: 2.357292890548706\n",
      "Epoch 1787/10000, Training Loss: 6.196375846862793, Validation Loss: 4.262617588043213\n",
      "Epoch 1788/10000, Training Loss: 7.241809368133545, Validation Loss: 12.110169410705566\n",
      "Epoch 1789/10000, Training Loss: 5.013651371002197, Validation Loss: 2.088019609451294\n",
      "Epoch 1790/10000, Training Loss: 5.466699600219727, Validation Loss: 12.201728820800781\n",
      "Epoch 1791/10000, Training Loss: 6.6659135818481445, Validation Loss: 10.902443885803223\n",
      "Epoch 1792/10000, Training Loss: 5.240377426147461, Validation Loss: 8.786529541015625\n",
      "Epoch 1793/10000, Training Loss: 4.193905830383301, Validation Loss: 5.013670444488525\n",
      "Epoch 1794/10000, Training Loss: 5.74725866317749, Validation Loss: 3.009258508682251\n",
      "Epoch 1795/10000, Training Loss: 5.124782085418701, Validation Loss: 3.846097946166992\n",
      "Epoch 1796/10000, Training Loss: 6.01686954498291, Validation Loss: 8.922236442565918\n",
      "Epoch 1797/10000, Training Loss: 9.03013801574707, Validation Loss: 11.139142036437988\n",
      "Epoch 1798/10000, Training Loss: 8.243303298950195, Validation Loss: 0.4969044625759125\n",
      "Epoch 1799/10000, Training Loss: 4.709151268005371, Validation Loss: 7.020346164703369\n",
      "Epoch 1800/10000, Training Loss: 5.507355690002441, Validation Loss: 8.992995262145996\n",
      "Epoch 1801/10000, Training Loss: 6.098899841308594, Validation Loss: 8.75660514831543\n",
      "Epoch 1802/10000, Training Loss: 4.703554153442383, Validation Loss: 15.80706787109375\n",
      "Epoch 1803/10000, Training Loss: 7.305496692657471, Validation Loss: 5.881478786468506\n",
      "Epoch 1804/10000, Training Loss: 5.750314712524414, Validation Loss: 1.4996305704116821\n",
      "Epoch 1805/10000, Training Loss: 6.3800225257873535, Validation Loss: 9.625592231750488\n",
      "Epoch 1806/10000, Training Loss: 6.498570442199707, Validation Loss: 6.0252203941345215\n",
      "Epoch 1807/10000, Training Loss: 4.2447710037231445, Validation Loss: 8.57080078125\n",
      "Epoch 1808/10000, Training Loss: 6.4533891677856445, Validation Loss: 5.994322299957275\n",
      "Epoch 1809/10000, Training Loss: 4.785764217376709, Validation Loss: 9.55305004119873\n",
      "Epoch 1810/10000, Training Loss: 7.025632858276367, Validation Loss: 1.3041795492172241\n",
      "Epoch 1811/10000, Training Loss: 7.970798015594482, Validation Loss: 2.432358980178833\n",
      "Epoch 1812/10000, Training Loss: 4.5256781578063965, Validation Loss: 4.836719989776611\n",
      "Epoch 1813/10000, Training Loss: 4.707306861877441, Validation Loss: 11.827467918395996\n",
      "Epoch 1814/10000, Training Loss: 7.91197395324707, Validation Loss: 7.792139053344727\n",
      "Epoch 1815/10000, Training Loss: 4.245243549346924, Validation Loss: 3.6493704319000244\n",
      "Epoch 1816/10000, Training Loss: 3.1881356239318848, Validation Loss: 5.107347011566162\n",
      "Epoch 1817/10000, Training Loss: 8.034255027770996, Validation Loss: 6.9123215675354\n",
      "Epoch 1818/10000, Training Loss: 5.275527000427246, Validation Loss: 5.580881595611572\n",
      "Epoch 1819/10000, Training Loss: 12.264894485473633, Validation Loss: 5.405803203582764\n",
      "Epoch 1820/10000, Training Loss: 8.808375358581543, Validation Loss: 7.19954252243042\n",
      "Epoch 1821/10000, Training Loss: 3.5156679153442383, Validation Loss: 2.5829367637634277\n",
      "Epoch 1822/10000, Training Loss: 5.436147689819336, Validation Loss: 13.599335670471191\n",
      "Epoch 1823/10000, Training Loss: 4.756158828735352, Validation Loss: 1.9744410514831543\n",
      "Epoch 1824/10000, Training Loss: 4.984127998352051, Validation Loss: 3.313495635986328\n",
      "Epoch 1825/10000, Training Loss: 4.982356548309326, Validation Loss: 8.004203796386719\n",
      "Epoch 1826/10000, Training Loss: 12.653623580932617, Validation Loss: 12.046398162841797\n",
      "Epoch 1827/10000, Training Loss: 7.230929851531982, Validation Loss: 2.9184505939483643\n",
      "Epoch 1828/10000, Training Loss: 5.227790355682373, Validation Loss: 4.298353672027588\n",
      "Epoch 1829/10000, Training Loss: 4.554343223571777, Validation Loss: 3.8738925457000732\n",
      "Epoch 1830/10000, Training Loss: 4.7609100341796875, Validation Loss: 11.392048835754395\n",
      "Epoch 1831/10000, Training Loss: 4.9092559814453125, Validation Loss: 6.026146411895752\n",
      "Epoch 1832/10000, Training Loss: 5.2250285148620605, Validation Loss: 3.6739742755889893\n",
      "Epoch 1833/10000, Training Loss: 9.089336395263672, Validation Loss: 11.654243469238281\n",
      "Epoch 1834/10000, Training Loss: 4.748392581939697, Validation Loss: 2.1377007961273193\n",
      "Epoch 1835/10000, Training Loss: 5.487912178039551, Validation Loss: 6.644603729248047\n",
      "Epoch 1836/10000, Training Loss: 5.7457194328308105, Validation Loss: 2.622915506362915\n",
      "Epoch 1837/10000, Training Loss: 7.169583320617676, Validation Loss: 7.994182109832764\n",
      "Epoch 1838/10000, Training Loss: 4.569784164428711, Validation Loss: 10.802706718444824\n",
      "Epoch 1839/10000, Training Loss: 7.772810935974121, Validation Loss: 14.306244850158691\n",
      "Epoch 1840/10000, Training Loss: 5.147253036499023, Validation Loss: 17.03705406188965\n",
      "Epoch 1841/10000, Training Loss: 7.775407791137695, Validation Loss: 15.828357696533203\n",
      "Epoch 1842/10000, Training Loss: 5.814507007598877, Validation Loss: 13.752307891845703\n",
      "Epoch 1843/10000, Training Loss: 5.421875953674316, Validation Loss: 3.261575937271118\n",
      "Epoch 1844/10000, Training Loss: 7.185042858123779, Validation Loss: 16.941980361938477\n",
      "Epoch 1845/10000, Training Loss: 5.72643518447876, Validation Loss: 4.674485206604004\n",
      "Epoch 1846/10000, Training Loss: 5.724959850311279, Validation Loss: 1.7476907968521118\n",
      "Epoch 1847/10000, Training Loss: 6.512056827545166, Validation Loss: 11.256477355957031\n",
      "Epoch 1848/10000, Training Loss: 5.611269474029541, Validation Loss: 5.078990459442139\n",
      "Epoch 1849/10000, Training Loss: 4.072448253631592, Validation Loss: 0.9036032557487488\n",
      "Epoch 1850/10000, Training Loss: 9.794657707214355, Validation Loss: 9.154800415039062\n",
      "Epoch 1851/10000, Training Loss: 4.202397346496582, Validation Loss: 5.616654872894287\n",
      "Epoch 1852/10000, Training Loss: 4.882169246673584, Validation Loss: 3.967679977416992\n",
      "Epoch 1853/10000, Training Loss: 6.002505302429199, Validation Loss: 2.4713823795318604\n",
      "Epoch 1854/10000, Training Loss: 5.077452659606934, Validation Loss: 2.9681339263916016\n",
      "Epoch 1855/10000, Training Loss: 11.067717552185059, Validation Loss: 12.95754337310791\n",
      "Epoch 1856/10000, Training Loss: 4.346787452697754, Validation Loss: 2.218904733657837\n",
      "Epoch 1857/10000, Training Loss: 4.28075647354126, Validation Loss: 1.3833560943603516\n",
      "Epoch 1858/10000, Training Loss: 5.718532085418701, Validation Loss: 9.563287734985352\n",
      "Epoch 1859/10000, Training Loss: 5.507300853729248, Validation Loss: 11.231185913085938\n",
      "Epoch 1860/10000, Training Loss: 5.094396114349365, Validation Loss: 10.068208694458008\n",
      "Epoch 1861/10000, Training Loss: 5.167918682098389, Validation Loss: 6.827054500579834\n",
      "Epoch 1862/10000, Training Loss: 5.314709186553955, Validation Loss: 4.780633449554443\n",
      "Epoch 1863/10000, Training Loss: 8.343798637390137, Validation Loss: 3.8346564769744873\n",
      "Epoch 1864/10000, Training Loss: 5.464001178741455, Validation Loss: 5.773160934448242\n",
      "Epoch 1865/10000, Training Loss: 5.3443684577941895, Validation Loss: 4.739128589630127\n",
      "Epoch 1866/10000, Training Loss: 9.751684188842773, Validation Loss: 12.698169708251953\n",
      "Epoch 1867/10000, Training Loss: 5.530893802642822, Validation Loss: 9.945752143859863\n",
      "Epoch 1868/10000, Training Loss: 6.132335186004639, Validation Loss: 3.45570969581604\n",
      "Epoch 1869/10000, Training Loss: 3.4776089191436768, Validation Loss: 9.365724563598633\n",
      "Epoch 1870/10000, Training Loss: 6.911041259765625, Validation Loss: 9.347359657287598\n",
      "Epoch 1871/10000, Training Loss: 7.761128902435303, Validation Loss: 2.3645832538604736\n",
      "Epoch 1872/10000, Training Loss: 7.824979782104492, Validation Loss: 7.879772186279297\n",
      "Epoch 1873/10000, Training Loss: 4.432443618774414, Validation Loss: 14.603412628173828\n",
      "Epoch 1874/10000, Training Loss: 6.926666259765625, Validation Loss: 3.824955940246582\n",
      "Epoch 1875/10000, Training Loss: 4.95820951461792, Validation Loss: 1.3854848146438599\n",
      "Epoch 1876/10000, Training Loss: 8.594216346740723, Validation Loss: 8.57150936126709\n",
      "Epoch 1877/10000, Training Loss: 4.502341270446777, Validation Loss: 4.684052467346191\n",
      "Epoch 1878/10000, Training Loss: 12.14876937866211, Validation Loss: 28.213647842407227\n",
      "Epoch 1879/10000, Training Loss: 6.193327903747559, Validation Loss: 3.003272294998169\n",
      "Epoch 1880/10000, Training Loss: 4.559926509857178, Validation Loss: 7.068231105804443\n",
      "Epoch 1881/10000, Training Loss: 5.201746463775635, Validation Loss: 8.420653343200684\n",
      "Epoch 1882/10000, Training Loss: 7.0905656814575195, Validation Loss: 5.489290237426758\n",
      "Epoch 1883/10000, Training Loss: 4.924583435058594, Validation Loss: 6.126558303833008\n",
      "Epoch 1884/10000, Training Loss: 4.865506172180176, Validation Loss: 2.7149555683135986\n",
      "Epoch 1885/10000, Training Loss: 4.091184139251709, Validation Loss: 3.5155603885650635\n",
      "Epoch 1886/10000, Training Loss: 4.472269058227539, Validation Loss: 7.669016361236572\n",
      "Epoch 1887/10000, Training Loss: 5.596651077270508, Validation Loss: 2.7932159900665283\n",
      "Epoch 1888/10000, Training Loss: 10.129827499389648, Validation Loss: 17.368715286254883\n",
      "Epoch 1889/10000, Training Loss: 6.324387550354004, Validation Loss: 5.58049201965332\n",
      "Epoch 1890/10000, Training Loss: 6.892045974731445, Validation Loss: 4.956451892852783\n",
      "Epoch 1891/10000, Training Loss: 4.422271251678467, Validation Loss: 7.981595993041992\n",
      "Epoch 1892/10000, Training Loss: 6.946166038513184, Validation Loss: 8.215559959411621\n",
      "Epoch 1893/10000, Training Loss: 4.872357368469238, Validation Loss: 2.522294044494629\n",
      "Epoch 1894/10000, Training Loss: 5.15372896194458, Validation Loss: 9.114599227905273\n",
      "Epoch 1895/10000, Training Loss: 8.002043724060059, Validation Loss: 3.821662187576294\n",
      "Epoch 1896/10000, Training Loss: 6.645161151885986, Validation Loss: 4.710515022277832\n",
      "Epoch 1897/10000, Training Loss: 4.423811912536621, Validation Loss: 5.163475513458252\n",
      "Epoch 1898/10000, Training Loss: 7.183259010314941, Validation Loss: 13.359274864196777\n",
      "Epoch 1899/10000, Training Loss: 6.521206855773926, Validation Loss: 5.534027099609375\n",
      "Epoch 1900/10000, Training Loss: 5.929955005645752, Validation Loss: 14.253252983093262\n",
      "Epoch 1901/10000, Training Loss: 5.4437642097473145, Validation Loss: 6.476449489593506\n",
      "Epoch 1902/10000, Training Loss: 4.304710865020752, Validation Loss: 5.840571880340576\n",
      "Epoch 1903/10000, Training Loss: 8.301647186279297, Validation Loss: 4.380454063415527\n",
      "Epoch 1904/10000, Training Loss: 5.41038179397583, Validation Loss: 6.724602222442627\n",
      "Epoch 1905/10000, Training Loss: 6.672952651977539, Validation Loss: 4.1286468505859375\n",
      "Epoch 1906/10000, Training Loss: 5.961255073547363, Validation Loss: 3.3837296962738037\n",
      "Epoch 1907/10000, Training Loss: 7.795141220092773, Validation Loss: 7.828325271606445\n",
      "Epoch 1908/10000, Training Loss: 4.732470989227295, Validation Loss: 9.20529556274414\n",
      "Epoch 1909/10000, Training Loss: 6.07763671875, Validation Loss: 13.17211627960205\n",
      "Epoch 1910/10000, Training Loss: 4.920074939727783, Validation Loss: 7.531608581542969\n",
      "Epoch 1911/10000, Training Loss: 4.33721923828125, Validation Loss: 8.317936897277832\n",
      "Epoch 1912/10000, Training Loss: 6.301763534545898, Validation Loss: 5.700494289398193\n",
      "Epoch 1913/10000, Training Loss: 3.9083926677703857, Validation Loss: 6.911945343017578\n",
      "Epoch 1914/10000, Training Loss: 5.292675971984863, Validation Loss: 3.841548204421997\n",
      "Epoch 1915/10000, Training Loss: 3.6429154872894287, Validation Loss: 5.55159330368042\n",
      "Epoch 1916/10000, Training Loss: 5.642400741577148, Validation Loss: 6.633838653564453\n",
      "Epoch 1917/10000, Training Loss: 6.056963920593262, Validation Loss: 9.5000581741333\n",
      "Epoch 1918/10000, Training Loss: 5.427160263061523, Validation Loss: 3.07112979888916\n",
      "Epoch 1919/10000, Training Loss: 5.950034141540527, Validation Loss: 7.443233489990234\n",
      "Epoch 1920/10000, Training Loss: 3.887073040008545, Validation Loss: 2.259073495864868\n",
      "Epoch 1921/10000, Training Loss: 8.966081619262695, Validation Loss: 16.1960391998291\n",
      "Epoch 1922/10000, Training Loss: 6.224539756774902, Validation Loss: 8.884882926940918\n",
      "Epoch 1923/10000, Training Loss: 4.275997638702393, Validation Loss: 10.435223579406738\n",
      "Epoch 1924/10000, Training Loss: 6.4085259437561035, Validation Loss: 4.408957481384277\n",
      "Epoch 1925/10000, Training Loss: 5.567371845245361, Validation Loss: 3.9914047718048096\n",
      "Epoch 1926/10000, Training Loss: 5.059689044952393, Validation Loss: 3.701110601425171\n",
      "Epoch 1927/10000, Training Loss: 8.32146167755127, Validation Loss: 14.325935363769531\n",
      "Epoch 1928/10000, Training Loss: 5.642479419708252, Validation Loss: 9.583868980407715\n",
      "Epoch 1929/10000, Training Loss: 4.016519546508789, Validation Loss: 19.685510635375977\n",
      "Epoch 1930/10000, Training Loss: 3.864142894744873, Validation Loss: 2.5250403881073\n",
      "Epoch 1931/10000, Training Loss: 6.122860431671143, Validation Loss: 1.3922343254089355\n",
      "Epoch 1932/10000, Training Loss: 4.3703508377075195, Validation Loss: 5.238673686981201\n",
      "Epoch 1933/10000, Training Loss: 5.0963897705078125, Validation Loss: 4.903726577758789\n",
      "Epoch 1934/10000, Training Loss: 6.352215766906738, Validation Loss: 4.040088176727295\n",
      "Epoch 1935/10000, Training Loss: 10.058269500732422, Validation Loss: 4.502529621124268\n",
      "Epoch 1936/10000, Training Loss: 4.7325968742370605, Validation Loss: 3.740617036819458\n",
      "Epoch 1937/10000, Training Loss: 7.540099620819092, Validation Loss: 16.32329750061035\n",
      "Epoch 1938/10000, Training Loss: 4.845515251159668, Validation Loss: 4.2032599449157715\n",
      "Epoch 1939/10000, Training Loss: 7.102428913116455, Validation Loss: 1.209072470664978\n",
      "Epoch 1940/10000, Training Loss: 6.023758888244629, Validation Loss: 2.8061697483062744\n",
      "Epoch 1941/10000, Training Loss: 6.472188949584961, Validation Loss: 5.923561096191406\n",
      "Epoch 1942/10000, Training Loss: 7.5088090896606445, Validation Loss: 3.9170405864715576\n",
      "Epoch 1943/10000, Training Loss: 4.141340255737305, Validation Loss: 27.002660751342773\n",
      "Epoch 1944/10000, Training Loss: 4.8756184577941895, Validation Loss: 4.8377766609191895\n",
      "Epoch 1945/10000, Training Loss: 7.135528564453125, Validation Loss: 4.014181137084961\n",
      "Epoch 1946/10000, Training Loss: 4.1605730056762695, Validation Loss: 4.641136646270752\n",
      "Epoch 1947/10000, Training Loss: 6.38736629486084, Validation Loss: 2.8801333904266357\n",
      "Epoch 1948/10000, Training Loss: 6.043449878692627, Validation Loss: 2.455120801925659\n",
      "Epoch 1949/10000, Training Loss: 5.055211067199707, Validation Loss: 8.008237838745117\n",
      "Epoch 1950/10000, Training Loss: 4.242633819580078, Validation Loss: 5.766632556915283\n",
      "Epoch 1951/10000, Training Loss: 4.649588584899902, Validation Loss: 5.213871479034424\n",
      "Epoch 1952/10000, Training Loss: 3.706305503845215, Validation Loss: 7.282690048217773\n",
      "Epoch 1953/10000, Training Loss: 4.284245014190674, Validation Loss: 7.173521518707275\n",
      "Epoch 1954/10000, Training Loss: 8.256974220275879, Validation Loss: 13.113410949707031\n",
      "Epoch 1955/10000, Training Loss: 5.177671432495117, Validation Loss: 19.66292953491211\n",
      "Epoch 1956/10000, Training Loss: 4.5017194747924805, Validation Loss: 5.9238200187683105\n",
      "Epoch 1957/10000, Training Loss: 6.495417594909668, Validation Loss: 4.206667900085449\n",
      "Epoch 1958/10000, Training Loss: 4.100612163543701, Validation Loss: 8.938258171081543\n",
      "Epoch 1959/10000, Training Loss: 3.8511831760406494, Validation Loss: 3.5498104095458984\n",
      "Epoch 1960/10000, Training Loss: 3.6151020526885986, Validation Loss: 4.551657676696777\n",
      "Epoch 1961/10000, Training Loss: 5.722465515136719, Validation Loss: 13.435839653015137\n",
      "Epoch 1962/10000, Training Loss: 3.28430438041687, Validation Loss: 0.04587783291935921\n",
      "Epoch 1963/10000, Training Loss: 4.382044315338135, Validation Loss: 4.64506196975708\n",
      "Epoch 1964/10000, Training Loss: 7.38771390914917, Validation Loss: 8.134678840637207\n",
      "Epoch 1965/10000, Training Loss: 8.091521263122559, Validation Loss: 10.616093635559082\n",
      "Epoch 1966/10000, Training Loss: 5.84078311920166, Validation Loss: 8.848163604736328\n",
      "Epoch 1967/10000, Training Loss: 6.499953269958496, Validation Loss: 4.662186622619629\n",
      "Epoch 1968/10000, Training Loss: 5.8006415367126465, Validation Loss: 9.045787811279297\n",
      "Epoch 1969/10000, Training Loss: 4.746277809143066, Validation Loss: 2.186659574508667\n",
      "Epoch 1970/10000, Training Loss: 5.231091022491455, Validation Loss: 14.719191551208496\n",
      "Epoch 1971/10000, Training Loss: 7.223187446594238, Validation Loss: 4.8756489753723145\n",
      "Epoch 1972/10000, Training Loss: 5.368112564086914, Validation Loss: 6.459949970245361\n",
      "Epoch 1973/10000, Training Loss: 4.498791217803955, Validation Loss: 13.740080833435059\n",
      "Epoch 1974/10000, Training Loss: 4.410825252532959, Validation Loss: 6.766258716583252\n",
      "Epoch 1975/10000, Training Loss: 4.5532917976379395, Validation Loss: 15.798666954040527\n",
      "Epoch 1976/10000, Training Loss: 4.404839992523193, Validation Loss: 5.07797384262085\n",
      "Epoch 1977/10000, Training Loss: 7.4478864669799805, Validation Loss: 6.0626349449157715\n",
      "Epoch 1978/10000, Training Loss: 3.855915069580078, Validation Loss: 4.469873905181885\n",
      "Epoch 1979/10000, Training Loss: 2.5678248405456543, Validation Loss: 5.160373210906982\n",
      "Epoch 1980/10000, Training Loss: 5.897956371307373, Validation Loss: 3.1576786041259766\n",
      "Epoch 1981/10000, Training Loss: 7.828302383422852, Validation Loss: 7.45881462097168\n",
      "Epoch 1982/10000, Training Loss: 7.11201810836792, Validation Loss: 9.510107040405273\n",
      "Epoch 1983/10000, Training Loss: 7.699339866638184, Validation Loss: 4.632296085357666\n",
      "Epoch 1984/10000, Training Loss: 4.200666427612305, Validation Loss: 5.464481830596924\n",
      "Epoch 1985/10000, Training Loss: 4.948739051818848, Validation Loss: 7.3296966552734375\n",
      "Epoch 1986/10000, Training Loss: 4.789803504943848, Validation Loss: 4.004318714141846\n",
      "Epoch 1987/10000, Training Loss: 6.011291980743408, Validation Loss: 9.078239440917969\n",
      "Epoch 1988/10000, Training Loss: 5.391628742218018, Validation Loss: 4.910511016845703\n",
      "Epoch 1989/10000, Training Loss: 3.420722723007202, Validation Loss: 5.241438865661621\n",
      "Epoch 1990/10000, Training Loss: 7.535723686218262, Validation Loss: 3.19303297996521\n",
      "Epoch 1991/10000, Training Loss: 4.421743392944336, Validation Loss: 6.179884433746338\n",
      "Epoch 1992/10000, Training Loss: 3.9643709659576416, Validation Loss: 3.6789309978485107\n",
      "Epoch 1993/10000, Training Loss: 5.008029460906982, Validation Loss: 0.696706235408783\n",
      "Epoch 1994/10000, Training Loss: 3.9567630290985107, Validation Loss: 3.1364545822143555\n",
      "Epoch 1995/10000, Training Loss: 7.970539569854736, Validation Loss: 17.00447654724121\n",
      "Epoch 1996/10000, Training Loss: 7.0699992179870605, Validation Loss: 9.02858829498291\n",
      "Epoch 1997/10000, Training Loss: 5.51263427734375, Validation Loss: 6.1706929206848145\n",
      "Epoch 1998/10000, Training Loss: 5.7302470207214355, Validation Loss: 9.610494613647461\n",
      "Epoch 1999/10000, Training Loss: 4.3209452629089355, Validation Loss: 4.608524322509766\n",
      "Epoch 2000/10000, Training Loss: 3.858078956604004, Validation Loss: 10.144413948059082\n",
      "Epoch 2001/10000, Training Loss: 6.765401840209961, Validation Loss: 8.08952522277832\n",
      "Epoch 2002/10000, Training Loss: 5.8665642738342285, Validation Loss: 2.4495561122894287\n",
      "Epoch 2003/10000, Training Loss: 5.595409870147705, Validation Loss: 5.673640727996826\n",
      "Epoch 2004/10000, Training Loss: 5.065890789031982, Validation Loss: 2.139089345932007\n",
      "Epoch 2005/10000, Training Loss: 3.598003387451172, Validation Loss: 5.205381870269775\n",
      "Epoch 2006/10000, Training Loss: 4.978741645812988, Validation Loss: 4.408864498138428\n",
      "Epoch 2007/10000, Training Loss: 3.755338430404663, Validation Loss: 6.412809371948242\n",
      "Epoch 2008/10000, Training Loss: 4.474636554718018, Validation Loss: 1.3325144052505493\n",
      "Epoch 2009/10000, Training Loss: 3.7479710578918457, Validation Loss: 8.435372352600098\n",
      "Epoch 2010/10000, Training Loss: 3.9903945922851562, Validation Loss: 10.907695770263672\n",
      "Epoch 2011/10000, Training Loss: 5.179651260375977, Validation Loss: 4.31410026550293\n",
      "Epoch 2012/10000, Training Loss: 4.690657615661621, Validation Loss: 7.547150135040283\n",
      "Epoch 2013/10000, Training Loss: 3.4604649543762207, Validation Loss: 7.44614839553833\n",
      "Epoch 2014/10000, Training Loss: 3.758248805999756, Validation Loss: 4.926486492156982\n",
      "Epoch 2015/10000, Training Loss: 6.074046611785889, Validation Loss: 17.241304397583008\n",
      "Epoch 2016/10000, Training Loss: 5.541402339935303, Validation Loss: 14.327542304992676\n",
      "Epoch 2017/10000, Training Loss: 5.55383825302124, Validation Loss: 12.616451263427734\n",
      "Epoch 2018/10000, Training Loss: 6.350100040435791, Validation Loss: 10.240687370300293\n",
      "Epoch 2019/10000, Training Loss: 4.911294937133789, Validation Loss: 4.5396881103515625\n",
      "Epoch 2020/10000, Training Loss: 4.43255615234375, Validation Loss: 2.045593500137329\n",
      "Epoch 2021/10000, Training Loss: 3.663499355316162, Validation Loss: 7.160821437835693\n",
      "Epoch 2022/10000, Training Loss: 4.974907398223877, Validation Loss: 5.018856048583984\n",
      "Epoch 2023/10000, Training Loss: 9.920815467834473, Validation Loss: 1.7624417543411255\n",
      "Epoch 2024/10000, Training Loss: 6.108920097351074, Validation Loss: 1.523765206336975\n",
      "Epoch 2025/10000, Training Loss: 5.275206565856934, Validation Loss: 8.826051712036133\n",
      "Epoch 2026/10000, Training Loss: 5.655613422393799, Validation Loss: 9.256693840026855\n",
      "Epoch 2027/10000, Training Loss: 3.745474338531494, Validation Loss: 2.2311313152313232\n",
      "Epoch 2028/10000, Training Loss: 8.462349891662598, Validation Loss: 33.462581634521484\n",
      "Epoch 2029/10000, Training Loss: 5.137021064758301, Validation Loss: 8.48587703704834\n",
      "Epoch 2030/10000, Training Loss: 6.201799392700195, Validation Loss: 4.90528678894043\n",
      "Epoch 2031/10000, Training Loss: 4.2606916427612305, Validation Loss: 5.850303649902344\n",
      "Epoch 2032/10000, Training Loss: 5.57114839553833, Validation Loss: 3.741265058517456\n",
      "Epoch 2033/10000, Training Loss: 5.015592575073242, Validation Loss: 9.696981430053711\n",
      "Epoch 2034/10000, Training Loss: 5.397324562072754, Validation Loss: 11.0542631149292\n",
      "Epoch 2035/10000, Training Loss: 6.398853778839111, Validation Loss: 4.7612810134887695\n",
      "Epoch 2036/10000, Training Loss: 5.799785137176514, Validation Loss: 1.6898459196090698\n",
      "Epoch 2037/10000, Training Loss: 3.136751413345337, Validation Loss: 4.594070911407471\n",
      "Epoch 2038/10000, Training Loss: 6.2090654373168945, Validation Loss: 9.165412902832031\n",
      "Epoch 2039/10000, Training Loss: 3.9423482418060303, Validation Loss: 4.365459442138672\n",
      "Epoch 2040/10000, Training Loss: 4.007137775421143, Validation Loss: 1.8397117853164673\n",
      "Epoch 2041/10000, Training Loss: 5.4427361488342285, Validation Loss: 7.674946308135986\n",
      "Epoch 2042/10000, Training Loss: 6.076756954193115, Validation Loss: 13.331954956054688\n",
      "Epoch 2043/10000, Training Loss: 3.3516640663146973, Validation Loss: 1.2329071760177612\n",
      "Epoch 2044/10000, Training Loss: 4.613929748535156, Validation Loss: 6.097092151641846\n",
      "Epoch 2045/10000, Training Loss: 5.605144023895264, Validation Loss: 11.741665840148926\n",
      "Epoch 2046/10000, Training Loss: 4.132974147796631, Validation Loss: 3.6101105213165283\n",
      "Epoch 2047/10000, Training Loss: 6.642269611358643, Validation Loss: 5.86651611328125\n",
      "Epoch 2048/10000, Training Loss: 4.859864234924316, Validation Loss: 8.570842742919922\n",
      "Epoch 2049/10000, Training Loss: 4.111081123352051, Validation Loss: 8.617709159851074\n",
      "Epoch 2050/10000, Training Loss: 5.862248420715332, Validation Loss: 11.864425659179688\n",
      "Epoch 2051/10000, Training Loss: 3.739804267883301, Validation Loss: 1.6293801069259644\n",
      "Epoch 2052/10000, Training Loss: 4.856616020202637, Validation Loss: 4.794620990753174\n",
      "Epoch 2053/10000, Training Loss: 6.057098388671875, Validation Loss: 20.112808227539062\n",
      "Epoch 2054/10000, Training Loss: 4.027089595794678, Validation Loss: 1.9511607885360718\n",
      "Epoch 2055/10000, Training Loss: 6.1109514236450195, Validation Loss: 3.4997332096099854\n",
      "Epoch 2056/10000, Training Loss: 4.904627323150635, Validation Loss: 5.906060695648193\n",
      "Epoch 2057/10000, Training Loss: 4.359125137329102, Validation Loss: 17.00773811340332\n",
      "Epoch 2058/10000, Training Loss: 4.105639457702637, Validation Loss: 3.658574342727661\n",
      "Epoch 2059/10000, Training Loss: 3.408294200897217, Validation Loss: 6.336842060089111\n",
      "Epoch 2060/10000, Training Loss: 4.515637397766113, Validation Loss: 2.339698314666748\n",
      "Epoch 2061/10000, Training Loss: 4.740504741668701, Validation Loss: 2.439291000366211\n",
      "Epoch 2062/10000, Training Loss: 4.135451793670654, Validation Loss: 1.4149855375289917\n",
      "Epoch 2063/10000, Training Loss: 6.312565803527832, Validation Loss: 14.997471809387207\n",
      "Epoch 2064/10000, Training Loss: 7.21321439743042, Validation Loss: 4.659653663635254\n",
      "Epoch 2065/10000, Training Loss: 6.173121929168701, Validation Loss: 8.96937084197998\n",
      "Epoch 2066/10000, Training Loss: 5.883404731750488, Validation Loss: 5.340698719024658\n",
      "Epoch 2067/10000, Training Loss: 3.661498785018921, Validation Loss: 3.5431478023529053\n",
      "Epoch 2068/10000, Training Loss: 5.883225440979004, Validation Loss: 13.09125804901123\n",
      "Epoch 2069/10000, Training Loss: 5.183078289031982, Validation Loss: 9.714616775512695\n",
      "Epoch 2070/10000, Training Loss: 5.505847454071045, Validation Loss: 7.180304050445557\n",
      "Epoch 2071/10000, Training Loss: 2.7574872970581055, Validation Loss: 3.525599241256714\n",
      "Epoch 2072/10000, Training Loss: 7.447011470794678, Validation Loss: 5.453566074371338\n",
      "Epoch 2073/10000, Training Loss: 4.356196880340576, Validation Loss: 2.3134782314300537\n",
      "Epoch 2074/10000, Training Loss: 4.088339328765869, Validation Loss: 2.5182814598083496\n",
      "Epoch 2075/10000, Training Loss: 6.893289566040039, Validation Loss: 9.080365180969238\n",
      "Epoch 2076/10000, Training Loss: 6.436826229095459, Validation Loss: 7.837472438812256\n",
      "Epoch 2077/10000, Training Loss: 3.8762364387512207, Validation Loss: 5.318541049957275\n",
      "Epoch 2078/10000, Training Loss: 4.624457836151123, Validation Loss: 5.5810065269470215\n",
      "Epoch 2079/10000, Training Loss: 3.515458583831787, Validation Loss: 3.1323859691619873\n",
      "Epoch 2080/10000, Training Loss: 4.703120231628418, Validation Loss: 5.273237228393555\n",
      "Epoch 2081/10000, Training Loss: 4.063092231750488, Validation Loss: 6.531877517700195\n",
      "Epoch 2082/10000, Training Loss: 5.832695007324219, Validation Loss: 6.428152561187744\n",
      "Epoch 2083/10000, Training Loss: 5.341697692871094, Validation Loss: 6.3421149253845215\n",
      "Epoch 2084/10000, Training Loss: 4.732959270477295, Validation Loss: 11.931273460388184\n",
      "Epoch 2085/10000, Training Loss: 3.2344775199890137, Validation Loss: 6.28460168838501\n",
      "Epoch 2086/10000, Training Loss: 5.670755863189697, Validation Loss: 4.5096940994262695\n",
      "Epoch 2087/10000, Training Loss: 5.805127143859863, Validation Loss: 8.724935531616211\n",
      "Epoch 2088/10000, Training Loss: 4.839147567749023, Validation Loss: 4.581529140472412\n",
      "Epoch 2089/10000, Training Loss: 6.671440601348877, Validation Loss: 6.846078395843506\n",
      "Epoch 2090/10000, Training Loss: 4.35658073425293, Validation Loss: 3.557360887527466\n",
      "Epoch 2091/10000, Training Loss: 4.45875358581543, Validation Loss: 4.03811502456665\n",
      "Epoch 2092/10000, Training Loss: 3.0055906772613525, Validation Loss: 2.6290581226348877\n",
      "Epoch 2093/10000, Training Loss: 3.8981778621673584, Validation Loss: 4.820854663848877\n",
      "Epoch 2094/10000, Training Loss: 3.7400197982788086, Validation Loss: 2.5131447315216064\n",
      "Epoch 2095/10000, Training Loss: 6.811233043670654, Validation Loss: 6.653362274169922\n",
      "Epoch 2096/10000, Training Loss: 5.474904537200928, Validation Loss: 3.574796676635742\n",
      "Epoch 2097/10000, Training Loss: 4.305400848388672, Validation Loss: 14.989884376525879\n",
      "Epoch 2098/10000, Training Loss: 5.079683780670166, Validation Loss: 5.6041646003723145\n",
      "Epoch 2099/10000, Training Loss: 5.6687116622924805, Validation Loss: 4.686537265777588\n",
      "Epoch 2100/10000, Training Loss: 3.112631320953369, Validation Loss: 1.5552030801773071\n",
      "Epoch 2101/10000, Training Loss: 4.825989246368408, Validation Loss: 6.3849334716796875\n",
      "Epoch 2102/10000, Training Loss: 4.315803050994873, Validation Loss: 9.103965759277344\n",
      "Epoch 2103/10000, Training Loss: 3.1350839138031006, Validation Loss: 4.477715015411377\n",
      "Epoch 2104/10000, Training Loss: 4.50181245803833, Validation Loss: 5.214736461639404\n",
      "Epoch 2105/10000, Training Loss: 5.461797714233398, Validation Loss: 6.504038333892822\n",
      "Epoch 2106/10000, Training Loss: 4.768764019012451, Validation Loss: 8.567859649658203\n",
      "Epoch 2107/10000, Training Loss: 5.0425214767456055, Validation Loss: 4.381824970245361\n",
      "Epoch 2108/10000, Training Loss: 5.123038291931152, Validation Loss: 6.924339294433594\n",
      "Epoch 2109/10000, Training Loss: 3.800034761428833, Validation Loss: 6.72432279586792\n",
      "Epoch 2110/10000, Training Loss: 3.1098287105560303, Validation Loss: 5.536401271820068\n",
      "Epoch 2111/10000, Training Loss: 4.134368419647217, Validation Loss: 1.9709296226501465\n",
      "Epoch 2112/10000, Training Loss: 5.701656818389893, Validation Loss: 8.333539009094238\n",
      "Epoch 2113/10000, Training Loss: 4.656607627868652, Validation Loss: 3.3095366954803467\n",
      "Epoch 2114/10000, Training Loss: 5.049402713775635, Validation Loss: 6.066917419433594\n",
      "Epoch 2115/10000, Training Loss: 3.895535707473755, Validation Loss: 0.7262222170829773\n",
      "Epoch 2116/10000, Training Loss: 3.8152787685394287, Validation Loss: 2.4985742568969727\n",
      "Epoch 2117/10000, Training Loss: 3.6646063327789307, Validation Loss: 1.5282591581344604\n",
      "Epoch 2118/10000, Training Loss: 3.3956923484802246, Validation Loss: 2.283097743988037\n",
      "Epoch 2119/10000, Training Loss: 3.250789165496826, Validation Loss: 6.77003812789917\n",
      "Epoch 2120/10000, Training Loss: 3.3164403438568115, Validation Loss: 6.01803731918335\n",
      "Epoch 2121/10000, Training Loss: 6.071291923522949, Validation Loss: 3.20194935798645\n",
      "Epoch 2122/10000, Training Loss: 3.8008499145507812, Validation Loss: 6.28156042098999\n",
      "Epoch 2123/10000, Training Loss: 5.228538990020752, Validation Loss: 11.044075012207031\n",
      "Epoch 2124/10000, Training Loss: 4.258703231811523, Validation Loss: 1.8821126222610474\n",
      "Epoch 2125/10000, Training Loss: 4.595147132873535, Validation Loss: 5.036859512329102\n",
      "Epoch 2126/10000, Training Loss: 4.882638454437256, Validation Loss: 4.1619062423706055\n",
      "Epoch 2127/10000, Training Loss: 5.839047908782959, Validation Loss: 4.734718322753906\n",
      "Epoch 2128/10000, Training Loss: 3.9116580486297607, Validation Loss: 10.344169616699219\n",
      "Epoch 2129/10000, Training Loss: 5.463968753814697, Validation Loss: 8.355887413024902\n",
      "Epoch 2130/10000, Training Loss: 6.51218843460083, Validation Loss: 10.848372459411621\n",
      "Epoch 2131/10000, Training Loss: 6.7448248863220215, Validation Loss: 4.9972052574157715\n",
      "Epoch 2132/10000, Training Loss: 5.349790096282959, Validation Loss: 3.2889792919158936\n",
      "Epoch 2133/10000, Training Loss: 6.262328624725342, Validation Loss: 8.662812232971191\n",
      "Epoch 2134/10000, Training Loss: 3.633453607559204, Validation Loss: 2.0648090839385986\n",
      "Epoch 2135/10000, Training Loss: 3.951486110687256, Validation Loss: 5.878690719604492\n",
      "Epoch 2136/10000, Training Loss: 2.9366517066955566, Validation Loss: 4.514523029327393\n",
      "Epoch 2137/10000, Training Loss: 3.881955862045288, Validation Loss: 2.306661605834961\n",
      "Epoch 2138/10000, Training Loss: 3.619514226913452, Validation Loss: 3.610732078552246\n",
      "Epoch 2139/10000, Training Loss: 3.221688985824585, Validation Loss: 2.9099502563476562\n",
      "Epoch 2140/10000, Training Loss: 6.351569652557373, Validation Loss: 4.707046985626221\n",
      "Epoch 2141/10000, Training Loss: 3.0127363204956055, Validation Loss: 7.185737609863281\n",
      "Epoch 2142/10000, Training Loss: 4.176337718963623, Validation Loss: 12.723698616027832\n",
      "Epoch 2143/10000, Training Loss: 3.2954561710357666, Validation Loss: 3.6907618045806885\n",
      "Epoch 2144/10000, Training Loss: 4.2891435623168945, Validation Loss: 6.457294464111328\n",
      "Epoch 2145/10000, Training Loss: 4.094481468200684, Validation Loss: 3.347083806991577\n",
      "Epoch 2146/10000, Training Loss: 4.147491931915283, Validation Loss: 2.751532554626465\n",
      "Epoch 2147/10000, Training Loss: 3.3764240741729736, Validation Loss: 8.170148849487305\n",
      "Epoch 2148/10000, Training Loss: 4.222635269165039, Validation Loss: 6.33917236328125\n",
      "Epoch 2149/10000, Training Loss: 7.352114200592041, Validation Loss: 5.020784378051758\n",
      "Epoch 2150/10000, Training Loss: 6.095400333404541, Validation Loss: 9.785717010498047\n",
      "Epoch 2151/10000, Training Loss: 5.026925086975098, Validation Loss: 4.432068347930908\n",
      "Epoch 2152/10000, Training Loss: 4.229280471801758, Validation Loss: 5.6336212158203125\n",
      "Epoch 2153/10000, Training Loss: 4.508399486541748, Validation Loss: 6.6666083335876465\n",
      "Epoch 2154/10000, Training Loss: 3.6864969730377197, Validation Loss: 6.485210418701172\n",
      "Epoch 2155/10000, Training Loss: 6.050169944763184, Validation Loss: 6.9103522300720215\n",
      "Epoch 2156/10000, Training Loss: 6.315706253051758, Validation Loss: 6.264070510864258\n",
      "Epoch 2157/10000, Training Loss: 5.303389549255371, Validation Loss: 8.413134574890137\n",
      "Epoch 2158/10000, Training Loss: 4.814555644989014, Validation Loss: 3.86612868309021\n",
      "Epoch 2159/10000, Training Loss: 4.02067756652832, Validation Loss: 7.362403392791748\n",
      "Epoch 2160/10000, Training Loss: 5.163055419921875, Validation Loss: 6.3718132972717285\n",
      "Epoch 2161/10000, Training Loss: 4.2683939933776855, Validation Loss: 4.476168155670166\n",
      "Epoch 2162/10000, Training Loss: 4.1630353927612305, Validation Loss: 6.07295036315918\n",
      "Epoch 2163/10000, Training Loss: 4.5963921546936035, Validation Loss: 4.243079662322998\n",
      "Epoch 2164/10000, Training Loss: 4.725049018859863, Validation Loss: 3.927093267440796\n",
      "Epoch 2165/10000, Training Loss: 4.451395511627197, Validation Loss: 4.881059169769287\n",
      "Epoch 2166/10000, Training Loss: 6.3428778648376465, Validation Loss: 12.987227439880371\n",
      "Epoch 2167/10000, Training Loss: 4.587118625640869, Validation Loss: 2.3928825855255127\n",
      "Epoch 2168/10000, Training Loss: 4.514963150024414, Validation Loss: 13.238699913024902\n",
      "Epoch 2169/10000, Training Loss: 4.024163246154785, Validation Loss: 3.7969772815704346\n",
      "Epoch 2170/10000, Training Loss: 5.39930534362793, Validation Loss: 5.836842060089111\n",
      "Epoch 2171/10000, Training Loss: 5.74233341217041, Validation Loss: 1.6010990142822266\n",
      "Epoch 2172/10000, Training Loss: 3.5870778560638428, Validation Loss: 3.409860849380493\n",
      "Epoch 2173/10000, Training Loss: 5.877409934997559, Validation Loss: 6.996129512786865\n",
      "Epoch 2174/10000, Training Loss: 2.9650344848632812, Validation Loss: 4.658616542816162\n",
      "Epoch 2175/10000, Training Loss: 5.064459800720215, Validation Loss: 6.5663161277771\n",
      "Epoch 2176/10000, Training Loss: 3.6743712425231934, Validation Loss: 8.579358100891113\n",
      "Epoch 2177/10000, Training Loss: 5.4178338050842285, Validation Loss: 9.822100639343262\n",
      "Epoch 2178/10000, Training Loss: 4.6838603019714355, Validation Loss: 4.427100658416748\n",
      "Epoch 2179/10000, Training Loss: 5.619560718536377, Validation Loss: 2.0863940715789795\n",
      "Epoch 2180/10000, Training Loss: 3.843564748764038, Validation Loss: 6.513922214508057\n",
      "Epoch 2181/10000, Training Loss: 4.048945903778076, Validation Loss: 9.619709968566895\n",
      "Epoch 2182/10000, Training Loss: 4.551684379577637, Validation Loss: 6.076717376708984\n",
      "Epoch 2183/10000, Training Loss: 4.024353504180908, Validation Loss: 6.734401702880859\n",
      "Epoch 2184/10000, Training Loss: 3.988633871078491, Validation Loss: 6.788107395172119\n",
      "Epoch 2185/10000, Training Loss: 4.353327751159668, Validation Loss: 3.1766128540039062\n",
      "Epoch 2186/10000, Training Loss: 3.1171083450317383, Validation Loss: 4.706943035125732\n",
      "Epoch 2187/10000, Training Loss: 5.796467304229736, Validation Loss: 6.6444854736328125\n",
      "Epoch 2188/10000, Training Loss: 4.68913459777832, Validation Loss: 2.4896814823150635\n",
      "Epoch 2189/10000, Training Loss: 4.402827739715576, Validation Loss: 4.391859531402588\n",
      "Epoch 2190/10000, Training Loss: 4.01770544052124, Validation Loss: 1.0750013589859009\n",
      "Epoch 2191/10000, Training Loss: 5.785563945770264, Validation Loss: 4.7221903800964355\n",
      "Epoch 2192/10000, Training Loss: 4.019759654998779, Validation Loss: 4.629199981689453\n",
      "Epoch 2193/10000, Training Loss: 3.753462553024292, Validation Loss: 9.567290306091309\n",
      "Epoch 2194/10000, Training Loss: 3.514953374862671, Validation Loss: 8.119876861572266\n",
      "Epoch 2195/10000, Training Loss: 2.4838545322418213, Validation Loss: 3.490948438644409\n",
      "Epoch 2196/10000, Training Loss: 3.7676889896392822, Validation Loss: 2.176231622695923\n",
      "Epoch 2197/10000, Training Loss: 4.401753902435303, Validation Loss: 6.293119430541992\n",
      "Epoch 2198/10000, Training Loss: 4.824657917022705, Validation Loss: 4.5010552406311035\n",
      "Epoch 2199/10000, Training Loss: 3.3683483600616455, Validation Loss: 3.4872210025787354\n",
      "Epoch 2200/10000, Training Loss: 4.427465438842773, Validation Loss: 3.0644493103027344\n",
      "Epoch 2201/10000, Training Loss: 4.023290634155273, Validation Loss: 8.769913673400879\n",
      "Epoch 2202/10000, Training Loss: 3.651932954788208, Validation Loss: 1.9276930093765259\n",
      "Epoch 2203/10000, Training Loss: 2.406226634979248, Validation Loss: 1.819031834602356\n",
      "Epoch 2204/10000, Training Loss: 3.2089970111846924, Validation Loss: 10.964649200439453\n",
      "Epoch 2205/10000, Training Loss: 4.597835540771484, Validation Loss: 4.83308219909668\n",
      "Epoch 2206/10000, Training Loss: 4.865357875823975, Validation Loss: 1.7784496545791626\n",
      "Epoch 2207/10000, Training Loss: 3.0773229598999023, Validation Loss: 8.25871753692627\n",
      "Epoch 2208/10000, Training Loss: 3.907215118408203, Validation Loss: 7.471063613891602\n",
      "Epoch 2209/10000, Training Loss: 5.532757759094238, Validation Loss: 2.676406145095825\n",
      "Epoch 2210/10000, Training Loss: 5.3782782554626465, Validation Loss: 3.975815773010254\n",
      "Epoch 2211/10000, Training Loss: 4.250609397888184, Validation Loss: 2.133274555206299\n",
      "Epoch 2212/10000, Training Loss: 3.765716552734375, Validation Loss: 3.4713449478149414\n",
      "Epoch 2213/10000, Training Loss: 3.8270275592803955, Validation Loss: 9.352452278137207\n",
      "Epoch 2214/10000, Training Loss: 3.3915021419525146, Validation Loss: 2.9199483394622803\n",
      "Epoch 2215/10000, Training Loss: 3.123563528060913, Validation Loss: 3.374100685119629\n",
      "Epoch 2216/10000, Training Loss: 5.896456241607666, Validation Loss: 5.209424018859863\n",
      "Epoch 2217/10000, Training Loss: 2.9988415241241455, Validation Loss: 2.157195568084717\n",
      "Epoch 2218/10000, Training Loss: 2.9512887001037598, Validation Loss: 4.947005748748779\n",
      "Epoch 2219/10000, Training Loss: 3.290530204772949, Validation Loss: 2.9260504245758057\n",
      "Epoch 2220/10000, Training Loss: 4.6904120445251465, Validation Loss: 3.7162599563598633\n",
      "Epoch 2221/10000, Training Loss: 5.489638328552246, Validation Loss: 5.909507751464844\n",
      "Epoch 2222/10000, Training Loss: 3.509901285171509, Validation Loss: 0.6802811026573181\n",
      "Epoch 2223/10000, Training Loss: 4.575249195098877, Validation Loss: 4.361292362213135\n",
      "Epoch 2224/10000, Training Loss: 4.962503433227539, Validation Loss: 8.65571403503418\n",
      "Epoch 2225/10000, Training Loss: 4.519008636474609, Validation Loss: 8.66062068939209\n",
      "Epoch 2226/10000, Training Loss: 3.301852226257324, Validation Loss: 2.5391693115234375\n",
      "Epoch 2227/10000, Training Loss: 3.8522934913635254, Validation Loss: 4.423252582550049\n",
      "Epoch 2228/10000, Training Loss: 12.851134300231934, Validation Loss: 28.097793579101562\n",
      "Epoch 2229/10000, Training Loss: 4.952926158905029, Validation Loss: 10.704682350158691\n",
      "Epoch 2230/10000, Training Loss: 4.076362609863281, Validation Loss: 5.95869779586792\n",
      "Epoch 2231/10000, Training Loss: 4.142763137817383, Validation Loss: 2.725680351257324\n",
      "Epoch 2232/10000, Training Loss: 2.881157398223877, Validation Loss: 0.4739043712615967\n",
      "Epoch 2233/10000, Training Loss: 6.024201393127441, Validation Loss: 10.015727996826172\n",
      "Epoch 2234/10000, Training Loss: 3.8588013648986816, Validation Loss: 3.5420358180999756\n",
      "Epoch 2235/10000, Training Loss: 2.8203225135803223, Validation Loss: 0.5892975926399231\n",
      "Epoch 2236/10000, Training Loss: 6.242644309997559, Validation Loss: 2.3865857124328613\n",
      "Epoch 2237/10000, Training Loss: 8.70034408569336, Validation Loss: 4.03587007522583\n",
      "Epoch 2238/10000, Training Loss: 3.2604544162750244, Validation Loss: 4.028445243835449\n",
      "Epoch 2239/10000, Training Loss: 4.211472511291504, Validation Loss: 5.914059162139893\n",
      "Epoch 2240/10000, Training Loss: 5.503761291503906, Validation Loss: 9.76941967010498\n",
      "Epoch 2241/10000, Training Loss: 4.6187334060668945, Validation Loss: 6.840854644775391\n",
      "Epoch 2242/10000, Training Loss: 5.761697769165039, Validation Loss: 11.18041706085205\n",
      "Epoch 2243/10000, Training Loss: 3.3824305534362793, Validation Loss: 7.6615824699401855\n",
      "Epoch 2244/10000, Training Loss: 7.7231645584106445, Validation Loss: 20.688749313354492\n",
      "Epoch 2245/10000, Training Loss: 4.381890296936035, Validation Loss: 5.859554767608643\n",
      "Epoch 2246/10000, Training Loss: 3.9277215003967285, Validation Loss: 6.149908065795898\n",
      "Epoch 2247/10000, Training Loss: 5.691993236541748, Validation Loss: 11.325037956237793\n",
      "Epoch 2248/10000, Training Loss: 3.5203967094421387, Validation Loss: 2.630357503890991\n",
      "Epoch 2249/10000, Training Loss: 3.7313833236694336, Validation Loss: 2.7135534286499023\n",
      "Epoch 2250/10000, Training Loss: 4.558108329772949, Validation Loss: 21.04300880432129\n",
      "Epoch 2251/10000, Training Loss: 4.856113910675049, Validation Loss: 9.19847583770752\n",
      "Epoch 2252/10000, Training Loss: 4.158613204956055, Validation Loss: 6.2150349617004395\n",
      "Epoch 2253/10000, Training Loss: 6.2695746421813965, Validation Loss: 2.092491626739502\n",
      "Epoch 2254/10000, Training Loss: 5.931405067443848, Validation Loss: 3.355501890182495\n",
      "Epoch 2255/10000, Training Loss: 5.215426921844482, Validation Loss: 5.012743949890137\n",
      "Epoch 2256/10000, Training Loss: 6.049007892608643, Validation Loss: 9.088065147399902\n",
      "Epoch 2257/10000, Training Loss: 5.3412346839904785, Validation Loss: 3.5421371459960938\n",
      "Epoch 2258/10000, Training Loss: 2.521287202835083, Validation Loss: 5.954178333282471\n",
      "Epoch 2259/10000, Training Loss: 5.08226203918457, Validation Loss: 9.499348640441895\n",
      "Epoch 2260/10000, Training Loss: 3.9589414596557617, Validation Loss: 4.082586765289307\n",
      "Epoch 2261/10000, Training Loss: 3.398226261138916, Validation Loss: 7.495727062225342\n",
      "Epoch 2262/10000, Training Loss: 3.9655098915100098, Validation Loss: 2.669123649597168\n",
      "Epoch 2263/10000, Training Loss: 5.176231861114502, Validation Loss: 4.229001522064209\n",
      "Epoch 2264/10000, Training Loss: 3.6203625202178955, Validation Loss: 9.788156509399414\n",
      "Epoch 2265/10000, Training Loss: 4.573832988739014, Validation Loss: 6.662639617919922\n",
      "Epoch 2266/10000, Training Loss: 3.2144792079925537, Validation Loss: 6.64236307144165\n",
      "Epoch 2267/10000, Training Loss: 3.530431032180786, Validation Loss: 2.6239173412323\n",
      "Epoch 2268/10000, Training Loss: 4.408787250518799, Validation Loss: 6.599844455718994\n",
      "Epoch 2269/10000, Training Loss: 4.646952152252197, Validation Loss: 2.7188422679901123\n",
      "Epoch 2270/10000, Training Loss: 4.664050102233887, Validation Loss: 6.653728008270264\n",
      "Epoch 2271/10000, Training Loss: 3.448822498321533, Validation Loss: 1.7788766622543335\n",
      "Epoch 2272/10000, Training Loss: 3.6187291145324707, Validation Loss: 1.2721844911575317\n",
      "Epoch 2273/10000, Training Loss: 4.460452556610107, Validation Loss: 6.434770584106445\n",
      "Epoch 2274/10000, Training Loss: 2.548062562942505, Validation Loss: 3.371213674545288\n",
      "Epoch 2275/10000, Training Loss: 2.6851794719696045, Validation Loss: 2.9621498584747314\n",
      "Epoch 2276/10000, Training Loss: 3.4214248657226562, Validation Loss: 1.3394867181777954\n",
      "Epoch 2277/10000, Training Loss: 3.5595247745513916, Validation Loss: 8.204487800598145\n",
      "Epoch 2278/10000, Training Loss: 4.18186092376709, Validation Loss: 5.792881488800049\n",
      "Epoch 2279/10000, Training Loss: 4.063215255737305, Validation Loss: 4.955056667327881\n",
      "Epoch 2280/10000, Training Loss: 3.558448314666748, Validation Loss: 5.388911724090576\n",
      "Epoch 2281/10000, Training Loss: 3.698127269744873, Validation Loss: 3.6454474925994873\n",
      "Epoch 2282/10000, Training Loss: 4.04031229019165, Validation Loss: 2.957284927368164\n",
      "Epoch 2283/10000, Training Loss: 4.575627326965332, Validation Loss: 3.4387826919555664\n",
      "Epoch 2284/10000, Training Loss: 3.2679812908172607, Validation Loss: 3.5597801208496094\n",
      "Epoch 2285/10000, Training Loss: 4.795287132263184, Validation Loss: 1.1491260528564453\n",
      "Epoch 2286/10000, Training Loss: 3.4084486961364746, Validation Loss: 2.887596368789673\n",
      "Epoch 2287/10000, Training Loss: 4.16059684753418, Validation Loss: 6.239602565765381\n",
      "Epoch 2288/10000, Training Loss: 2.9873223304748535, Validation Loss: 3.6702823638916016\n",
      "Epoch 2289/10000, Training Loss: 4.262786865234375, Validation Loss: 7.443605899810791\n",
      "Epoch 2290/10000, Training Loss: 3.2477049827575684, Validation Loss: 1.337141513824463\n",
      "Epoch 2291/10000, Training Loss: 3.1509182453155518, Validation Loss: 4.556980609893799\n",
      "Epoch 2292/10000, Training Loss: 3.4793827533721924, Validation Loss: 5.814177989959717\n",
      "Epoch 2293/10000, Training Loss: 4.407814979553223, Validation Loss: 9.4561767578125\n",
      "Epoch 2294/10000, Training Loss: 2.887176036834717, Validation Loss: 3.501601219177246\n",
      "Epoch 2295/10000, Training Loss: 3.334355592727661, Validation Loss: 5.172452449798584\n",
      "Epoch 2296/10000, Training Loss: 4.5509819984436035, Validation Loss: 8.76400089263916\n",
      "Epoch 2297/10000, Training Loss: 3.410081386566162, Validation Loss: 1.9231548309326172\n",
      "Epoch 2298/10000, Training Loss: 4.450114727020264, Validation Loss: 6.831940174102783\n",
      "Epoch 2299/10000, Training Loss: 4.149841785430908, Validation Loss: 3.2649242877960205\n",
      "Epoch 2300/10000, Training Loss: 6.114713191986084, Validation Loss: 8.673117637634277\n",
      "Epoch 2301/10000, Training Loss: 4.363041400909424, Validation Loss: 2.9190356731414795\n",
      "Epoch 2302/10000, Training Loss: 3.8369011878967285, Validation Loss: 5.679625988006592\n",
      "Epoch 2303/10000, Training Loss: 4.467251777648926, Validation Loss: 8.984915733337402\n",
      "Epoch 2304/10000, Training Loss: 5.370893478393555, Validation Loss: 5.092462062835693\n",
      "Epoch 2305/10000, Training Loss: 4.783702373504639, Validation Loss: 4.356432914733887\n",
      "Epoch 2306/10000, Training Loss: 7.391836166381836, Validation Loss: 2.4965438842773438\n",
      "Epoch 2307/10000, Training Loss: 3.5150511264801025, Validation Loss: 2.3975002765655518\n",
      "Epoch 2308/10000, Training Loss: 5.126369953155518, Validation Loss: 4.472358703613281\n",
      "Epoch 2309/10000, Training Loss: 2.7700319290161133, Validation Loss: 4.54661750793457\n",
      "Epoch 2310/10000, Training Loss: 2.873396873474121, Validation Loss: 0.4236518144607544\n",
      "Epoch 2311/10000, Training Loss: 5.1632537841796875, Validation Loss: 3.240511655807495\n",
      "Epoch 2312/10000, Training Loss: 4.178226947784424, Validation Loss: 13.069091796875\n",
      "Epoch 2313/10000, Training Loss: 4.507475852966309, Validation Loss: 7.117563724517822\n",
      "Epoch 2314/10000, Training Loss: 2.6360533237457275, Validation Loss: 4.475245952606201\n",
      "Epoch 2315/10000, Training Loss: 3.4668986797332764, Validation Loss: 3.2399723529815674\n",
      "Epoch 2316/10000, Training Loss: 3.405931234359741, Validation Loss: 4.131828308105469\n",
      "Epoch 2317/10000, Training Loss: 4.181347370147705, Validation Loss: 2.7776877880096436\n",
      "Epoch 2318/10000, Training Loss: 3.244241237640381, Validation Loss: 2.4199163913726807\n",
      "Epoch 2319/10000, Training Loss: 2.5387802124023438, Validation Loss: 4.6481852531433105\n",
      "Epoch 2320/10000, Training Loss: 3.9135985374450684, Validation Loss: 2.7953288555145264\n",
      "Epoch 2321/10000, Training Loss: 6.018189430236816, Validation Loss: 5.848278522491455\n",
      "Epoch 2322/10000, Training Loss: 3.4010915756225586, Validation Loss: 9.227897644042969\n",
      "Epoch 2323/10000, Training Loss: 5.483240127563477, Validation Loss: 9.565936088562012\n",
      "Epoch 2324/10000, Training Loss: 5.49925422668457, Validation Loss: 1.2226842641830444\n",
      "Epoch 2325/10000, Training Loss: 3.487647771835327, Validation Loss: 5.579511642456055\n",
      "Epoch 2326/10000, Training Loss: 4.44646692276001, Validation Loss: 7.885402202606201\n",
      "Epoch 2327/10000, Training Loss: 3.8532445430755615, Validation Loss: 1.8608592748641968\n",
      "Epoch 2328/10000, Training Loss: 6.3646159172058105, Validation Loss: 5.186136722564697\n",
      "Epoch 2329/10000, Training Loss: 4.07891845703125, Validation Loss: 3.9524881839752197\n",
      "Epoch 2330/10000, Training Loss: 3.7578039169311523, Validation Loss: 2.320573568344116\n",
      "Epoch 2331/10000, Training Loss: 3.0458483695983887, Validation Loss: 3.10033917427063\n",
      "Epoch 2332/10000, Training Loss: 4.368855953216553, Validation Loss: 5.000667095184326\n",
      "Epoch 2333/10000, Training Loss: 3.1195924282073975, Validation Loss: 1.329277753829956\n",
      "Epoch 2334/10000, Training Loss: 3.663442373275757, Validation Loss: 3.150869131088257\n",
      "Epoch 2335/10000, Training Loss: 10.611316680908203, Validation Loss: 1.5738344192504883\n",
      "Epoch 2336/10000, Training Loss: 4.1552228927612305, Validation Loss: 1.716983675956726\n",
      "Epoch 2337/10000, Training Loss: 3.588137149810791, Validation Loss: 5.73125696182251\n",
      "Epoch 2338/10000, Training Loss: 3.1727664470672607, Validation Loss: 3.498332977294922\n",
      "Epoch 2339/10000, Training Loss: 4.248054504394531, Validation Loss: 5.8267083168029785\n",
      "Epoch 2340/10000, Training Loss: 5.466520309448242, Validation Loss: 6.038692474365234\n",
      "Epoch 2341/10000, Training Loss: 2.7199864387512207, Validation Loss: 4.624125957489014\n",
      "Epoch 2342/10000, Training Loss: 4.637329578399658, Validation Loss: 6.301244735717773\n",
      "Epoch 2343/10000, Training Loss: 3.7436323165893555, Validation Loss: 3.44596266746521\n",
      "Epoch 2344/10000, Training Loss: 4.363687515258789, Validation Loss: 0.5956668257713318\n",
      "Epoch 2345/10000, Training Loss: 4.285004615783691, Validation Loss: 5.10573673248291\n",
      "Epoch 2346/10000, Training Loss: 3.867058753967285, Validation Loss: 2.7698943614959717\n",
      "Epoch 2347/10000, Training Loss: 5.242301940917969, Validation Loss: 7.169783115386963\n",
      "Epoch 2348/10000, Training Loss: 2.2899651527404785, Validation Loss: 4.851933002471924\n",
      "Epoch 2349/10000, Training Loss: 4.683520793914795, Validation Loss: 4.8851776123046875\n",
      "Epoch 2350/10000, Training Loss: 3.8720076084136963, Validation Loss: 10.100159645080566\n",
      "Epoch 2351/10000, Training Loss: 3.0164949893951416, Validation Loss: 2.1233267784118652\n",
      "Epoch 2352/10000, Training Loss: 3.0149171352386475, Validation Loss: 2.692579984664917\n",
      "Epoch 2353/10000, Training Loss: 4.682623863220215, Validation Loss: 10.960543632507324\n",
      "Epoch 2354/10000, Training Loss: 3.5247156620025635, Validation Loss: 10.02798843383789\n",
      "Epoch 2355/10000, Training Loss: 5.327645301818848, Validation Loss: 7.577229022979736\n",
      "Epoch 2356/10000, Training Loss: 4.3802361488342285, Validation Loss: 9.561158180236816\n",
      "Epoch 2357/10000, Training Loss: 4.342059135437012, Validation Loss: 5.589915752410889\n",
      "Epoch 2358/10000, Training Loss: 3.949880361557007, Validation Loss: 4.292186260223389\n",
      "Epoch 2359/10000, Training Loss: 3.2464711666107178, Validation Loss: 2.465803623199463\n",
      "Epoch 2360/10000, Training Loss: 4.713719367980957, Validation Loss: 10.068309783935547\n",
      "Epoch 2361/10000, Training Loss: 4.357213020324707, Validation Loss: 7.037415027618408\n",
      "Epoch 2362/10000, Training Loss: 3.2691142559051514, Validation Loss: 3.810849189758301\n",
      "Epoch 2363/10000, Training Loss: 4.612032890319824, Validation Loss: 3.585649251937866\n",
      "Epoch 2364/10000, Training Loss: 4.275575160980225, Validation Loss: 4.182041645050049\n",
      "Epoch 2365/10000, Training Loss: 3.5151772499084473, Validation Loss: 3.403414011001587\n",
      "Epoch 2366/10000, Training Loss: 4.043230056762695, Validation Loss: 1.2124321460723877\n",
      "Epoch 2367/10000, Training Loss: 2.6564173698425293, Validation Loss: 5.902188777923584\n",
      "Epoch 2368/10000, Training Loss: 3.3550188541412354, Validation Loss: 1.85305655002594\n",
      "Epoch 2369/10000, Training Loss: 5.216552734375, Validation Loss: 1.1857918500900269\n",
      "Epoch 2370/10000, Training Loss: 2.932936429977417, Validation Loss: 6.336986064910889\n",
      "Epoch 2371/10000, Training Loss: 3.3697447776794434, Validation Loss: 3.948134422302246\n",
      "Epoch 2372/10000, Training Loss: 3.553572177886963, Validation Loss: 5.595902919769287\n",
      "Epoch 2373/10000, Training Loss: 5.102657794952393, Validation Loss: 4.928341388702393\n",
      "Epoch 2374/10000, Training Loss: 3.9715323448181152, Validation Loss: 3.8093807697296143\n",
      "Epoch 2375/10000, Training Loss: 4.158359050750732, Validation Loss: 1.1039304733276367\n",
      "Epoch 2376/10000, Training Loss: 4.531637668609619, Validation Loss: 10.944480895996094\n",
      "Epoch 2377/10000, Training Loss: 5.027272701263428, Validation Loss: 9.85457992553711\n",
      "Epoch 2378/10000, Training Loss: 5.176669120788574, Validation Loss: 2.2271225452423096\n",
      "Epoch 2379/10000, Training Loss: 3.7517619132995605, Validation Loss: 5.966053485870361\n",
      "Epoch 2380/10000, Training Loss: 3.030029058456421, Validation Loss: 4.443967819213867\n",
      "Epoch 2381/10000, Training Loss: 3.280684471130371, Validation Loss: 3.018336296081543\n",
      "Epoch 2382/10000, Training Loss: 5.379802227020264, Validation Loss: 4.757266998291016\n",
      "Epoch 2383/10000, Training Loss: 4.647646903991699, Validation Loss: 5.769992828369141\n",
      "Epoch 2384/10000, Training Loss: 3.912611722946167, Validation Loss: 3.6088593006134033\n",
      "Epoch 2385/10000, Training Loss: 4.018651962280273, Validation Loss: 13.260833740234375\n",
      "Epoch 2386/10000, Training Loss: 2.829669713973999, Validation Loss: 5.695091724395752\n",
      "Epoch 2387/10000, Training Loss: 6.34053897857666, Validation Loss: 3.5737295150756836\n",
      "Epoch 2388/10000, Training Loss: 3.505038261413574, Validation Loss: 2.6237804889678955\n",
      "Epoch 2389/10000, Training Loss: 3.574627161026001, Validation Loss: 4.395421981811523\n",
      "Epoch 2390/10000, Training Loss: 3.100918769836426, Validation Loss: 2.670619249343872\n",
      "Epoch 2391/10000, Training Loss: 2.845780849456787, Validation Loss: 5.780063629150391\n",
      "Epoch 2392/10000, Training Loss: 4.129129409790039, Validation Loss: 6.125535488128662\n",
      "Epoch 2393/10000, Training Loss: 5.326167583465576, Validation Loss: 6.0555644035339355\n",
      "Epoch 2394/10000, Training Loss: 3.7763493061065674, Validation Loss: 4.441749095916748\n",
      "Epoch 2395/10000, Training Loss: 3.21437406539917, Validation Loss: 5.565837383270264\n",
      "Epoch 2396/10000, Training Loss: 4.097822666168213, Validation Loss: 2.1644434928894043\n",
      "Epoch 2397/10000, Training Loss: 5.258405685424805, Validation Loss: 14.0855073928833\n",
      "Epoch 2398/10000, Training Loss: 2.657898426055908, Validation Loss: 3.6396591663360596\n",
      "Epoch 2399/10000, Training Loss: 3.5663681030273438, Validation Loss: 3.382138967514038\n",
      "Epoch 2400/10000, Training Loss: 6.557018756866455, Validation Loss: 5.608266353607178\n",
      "Epoch 2401/10000, Training Loss: 3.025418996810913, Validation Loss: 6.026398181915283\n",
      "Epoch 2402/10000, Training Loss: 3.7379696369171143, Validation Loss: 5.901957988739014\n",
      "Epoch 2403/10000, Training Loss: 2.9614198207855225, Validation Loss: 3.6602256298065186\n",
      "Epoch 2404/10000, Training Loss: 5.0675201416015625, Validation Loss: 5.778964996337891\n",
      "Epoch 2405/10000, Training Loss: 4.241405963897705, Validation Loss: 3.4274895191192627\n",
      "Epoch 2406/10000, Training Loss: 3.5745482444763184, Validation Loss: 3.583606719970703\n",
      "Epoch 2407/10000, Training Loss: 5.179886341094971, Validation Loss: 7.6881256103515625\n",
      "Epoch 2408/10000, Training Loss: 5.1051836013793945, Validation Loss: 5.943392276763916\n",
      "Epoch 2409/10000, Training Loss: 6.113358020782471, Validation Loss: 2.977893114089966\n",
      "Epoch 2410/10000, Training Loss: 3.5708913803100586, Validation Loss: 2.826136827468872\n",
      "Epoch 2411/10000, Training Loss: 3.3212316036224365, Validation Loss: 3.858372449874878\n",
      "Epoch 2412/10000, Training Loss: 3.3575210571289062, Validation Loss: 2.888279676437378\n",
      "Epoch 2413/10000, Training Loss: 3.4501748085021973, Validation Loss: 1.6271249055862427\n",
      "Epoch 2414/10000, Training Loss: 4.513876914978027, Validation Loss: 1.465023159980774\n",
      "Epoch 2415/10000, Training Loss: 3.362344980239868, Validation Loss: 7.849822521209717\n",
      "Epoch 2416/10000, Training Loss: 2.518350839614868, Validation Loss: 3.7582881450653076\n",
      "Epoch 2417/10000, Training Loss: 3.1577391624450684, Validation Loss: 1.4553728103637695\n",
      "Epoch 2418/10000, Training Loss: 3.8676838874816895, Validation Loss: 3.465653419494629\n",
      "Epoch 2419/10000, Training Loss: 3.547729015350342, Validation Loss: 0.6695038676261902\n",
      "Epoch 2420/10000, Training Loss: 3.04667329788208, Validation Loss: 1.642295241355896\n",
      "Epoch 2421/10000, Training Loss: 4.294186115264893, Validation Loss: 5.348327159881592\n",
      "Epoch 2422/10000, Training Loss: 3.1439895629882812, Validation Loss: 3.2765934467315674\n",
      "Epoch 2423/10000, Training Loss: 3.508857250213623, Validation Loss: 4.975326061248779\n",
      "Epoch 2424/10000, Training Loss: 4.011878490447998, Validation Loss: 3.338953733444214\n",
      "Epoch 2425/10000, Training Loss: 4.774150371551514, Validation Loss: 2.467677593231201\n",
      "Epoch 2426/10000, Training Loss: 2.2638838291168213, Validation Loss: 4.787909030914307\n",
      "Epoch 2427/10000, Training Loss: 4.888740539550781, Validation Loss: 5.316478252410889\n",
      "Epoch 2428/10000, Training Loss: 3.054194450378418, Validation Loss: 1.7818621397018433\n",
      "Epoch 2429/10000, Training Loss: 3.319119930267334, Validation Loss: 3.1988942623138428\n",
      "Epoch 2430/10000, Training Loss: 3.9231820106506348, Validation Loss: 5.602489948272705\n",
      "Epoch 2431/10000, Training Loss: 3.71903657913208, Validation Loss: 4.855818748474121\n",
      "Epoch 2432/10000, Training Loss: 3.60101318359375, Validation Loss: 5.183017253875732\n",
      "Epoch 2433/10000, Training Loss: 3.018012523651123, Validation Loss: 3.1816980838775635\n",
      "Epoch 2434/10000, Training Loss: 3.554476737976074, Validation Loss: 2.634158134460449\n",
      "Epoch 2435/10000, Training Loss: 2.3008313179016113, Validation Loss: 2.9422271251678467\n",
      "Epoch 2436/10000, Training Loss: 4.032486438751221, Validation Loss: 4.318426609039307\n",
      "Epoch 2437/10000, Training Loss: 4.359226703643799, Validation Loss: 4.374644756317139\n",
      "Epoch 2438/10000, Training Loss: 4.06411075592041, Validation Loss: 2.3930728435516357\n",
      "Epoch 2439/10000, Training Loss: 3.967151165008545, Validation Loss: 6.390600204467773\n",
      "Epoch 2440/10000, Training Loss: 2.8877134323120117, Validation Loss: 3.173253059387207\n",
      "Epoch 2441/10000, Training Loss: 3.044243812561035, Validation Loss: 6.044018268585205\n",
      "Epoch 2442/10000, Training Loss: 4.781485080718994, Validation Loss: 1.5444473028182983\n",
      "Epoch 2443/10000, Training Loss: 5.217890739440918, Validation Loss: 2.4537646770477295\n",
      "Epoch 2444/10000, Training Loss: 3.9005026817321777, Validation Loss: 2.787592887878418\n",
      "Epoch 2445/10000, Training Loss: 2.7322757244110107, Validation Loss: 2.847386360168457\n",
      "Epoch 2446/10000, Training Loss: 5.095947742462158, Validation Loss: 5.397383213043213\n",
      "Epoch 2447/10000, Training Loss: 4.260579586029053, Validation Loss: 12.310540199279785\n",
      "Epoch 2448/10000, Training Loss: 2.941413640975952, Validation Loss: 5.330102920532227\n",
      "Epoch 2449/10000, Training Loss: 3.6500697135925293, Validation Loss: 5.623123645782471\n",
      "Epoch 2450/10000, Training Loss: 3.1151235103607178, Validation Loss: 7.7224297523498535\n",
      "Epoch 2451/10000, Training Loss: 3.835174322128296, Validation Loss: 3.5463085174560547\n",
      "Epoch 2452/10000, Training Loss: 4.102466583251953, Validation Loss: 0.7604658007621765\n",
      "Epoch 2453/10000, Training Loss: 3.748088836669922, Validation Loss: 4.793150424957275\n",
      "Epoch 2454/10000, Training Loss: 6.517285346984863, Validation Loss: 3.491590738296509\n",
      "Epoch 2455/10000, Training Loss: 2.5190978050231934, Validation Loss: 1.7514519691467285\n",
      "Epoch 2456/10000, Training Loss: 2.08927321434021, Validation Loss: 5.179962158203125\n",
      "Epoch 2457/10000, Training Loss: 3.201244354248047, Validation Loss: 4.761768817901611\n",
      "Epoch 2458/10000, Training Loss: 4.2960710525512695, Validation Loss: 6.981672763824463\n",
      "Epoch 2459/10000, Training Loss: 3.1220147609710693, Validation Loss: 3.4346866607666016\n",
      "Epoch 2460/10000, Training Loss: 4.395140647888184, Validation Loss: 7.471120357513428\n",
      "Epoch 2461/10000, Training Loss: 5.0580244064331055, Validation Loss: 10.502828598022461\n",
      "Epoch 2462/10000, Training Loss: 4.396865367889404, Validation Loss: 4.639889240264893\n",
      "Epoch 2463/10000, Training Loss: 2.748715400695801, Validation Loss: 2.2232472896575928\n",
      "Epoch 2464/10000, Training Loss: 4.905956745147705, Validation Loss: 6.254859924316406\n",
      "Epoch 2465/10000, Training Loss: 3.184762477874756, Validation Loss: 6.118823528289795\n",
      "Epoch 2466/10000, Training Loss: 2.6052536964416504, Validation Loss: 3.2561004161834717\n",
      "Epoch 2467/10000, Training Loss: 5.693336009979248, Validation Loss: 3.4067649841308594\n",
      "Epoch 2468/10000, Training Loss: 3.7580959796905518, Validation Loss: 4.422500133514404\n",
      "Epoch 2469/10000, Training Loss: 5.463623046875, Validation Loss: 3.975625991821289\n",
      "Epoch 2470/10000, Training Loss: 4.617679119110107, Validation Loss: 2.8967607021331787\n",
      "Epoch 2471/10000, Training Loss: 3.5657095909118652, Validation Loss: 4.192453861236572\n",
      "Epoch 2472/10000, Training Loss: 4.362264156341553, Validation Loss: 3.3118889331817627\n",
      "Epoch 2473/10000, Training Loss: 4.531713485717773, Validation Loss: 9.02420425415039\n",
      "Epoch 2474/10000, Training Loss: 4.461552143096924, Validation Loss: 3.621093511581421\n",
      "Epoch 2475/10000, Training Loss: 2.5079495906829834, Validation Loss: 2.834702253341675\n",
      "Epoch 2476/10000, Training Loss: 3.380359411239624, Validation Loss: 10.182222366333008\n",
      "Epoch 2477/10000, Training Loss: 3.1382036209106445, Validation Loss: 1.6140609979629517\n",
      "Epoch 2478/10000, Training Loss: 3.424795627593994, Validation Loss: 8.816468238830566\n",
      "Epoch 2479/10000, Training Loss: 2.4117908477783203, Validation Loss: 4.314742565155029\n",
      "Epoch 2480/10000, Training Loss: 3.9874420166015625, Validation Loss: 5.581911563873291\n",
      "Epoch 2481/10000, Training Loss: 3.3007736206054688, Validation Loss: 0.8812830448150635\n",
      "Epoch 2482/10000, Training Loss: 3.7339043617248535, Validation Loss: 3.4224603176116943\n",
      "Epoch 2483/10000, Training Loss: 3.5160481929779053, Validation Loss: 12.657649040222168\n",
      "Epoch 2484/10000, Training Loss: 2.388827323913574, Validation Loss: 0.8951262831687927\n",
      "Epoch 2485/10000, Training Loss: 2.630618095397949, Validation Loss: 9.284274101257324\n",
      "Epoch 2486/10000, Training Loss: 2.7976701259613037, Validation Loss: 2.623990774154663\n",
      "Epoch 2487/10000, Training Loss: 2.926421880722046, Validation Loss: 4.59116792678833\n",
      "Epoch 2488/10000, Training Loss: 3.8526270389556885, Validation Loss: 5.751653671264648\n",
      "Epoch 2489/10000, Training Loss: 4.06356143951416, Validation Loss: 5.264285087585449\n",
      "Epoch 2490/10000, Training Loss: 4.959269046783447, Validation Loss: 7.012216567993164\n",
      "Epoch 2491/10000, Training Loss: 4.444106101989746, Validation Loss: 7.833175182342529\n",
      "Epoch 2492/10000, Training Loss: 3.486302375793457, Validation Loss: 5.455347537994385\n",
      "Epoch 2493/10000, Training Loss: 3.504519462585449, Validation Loss: 6.099380016326904\n",
      "Epoch 2494/10000, Training Loss: 5.472436904907227, Validation Loss: 8.666281700134277\n",
      "Epoch 2495/10000, Training Loss: 2.9422051906585693, Validation Loss: 4.22675085067749\n",
      "Epoch 2496/10000, Training Loss: 3.3951497077941895, Validation Loss: 4.6071457862854\n",
      "Epoch 2497/10000, Training Loss: 4.585107803344727, Validation Loss: 3.961334466934204\n",
      "Epoch 2498/10000, Training Loss: 2.8593506813049316, Validation Loss: 2.7777154445648193\n",
      "Epoch 2499/10000, Training Loss: 2.979992151260376, Validation Loss: 2.2956771850585938\n",
      "Epoch 2500/10000, Training Loss: 2.5052309036254883, Validation Loss: 8.10446548461914\n",
      "Epoch 2501/10000, Training Loss: 3.571035861968994, Validation Loss: 5.635354518890381\n",
      "Epoch 2502/10000, Training Loss: 4.089993953704834, Validation Loss: 7.687900543212891\n",
      "Epoch 2503/10000, Training Loss: 5.0066399574279785, Validation Loss: 6.149136066436768\n",
      "Epoch 2504/10000, Training Loss: 3.585507392883301, Validation Loss: 2.2682511806488037\n",
      "Epoch 2505/10000, Training Loss: 3.7284960746765137, Validation Loss: 8.818819046020508\n",
      "Epoch 2506/10000, Training Loss: 3.990200996398926, Validation Loss: 5.80169153213501\n",
      "Epoch 2507/10000, Training Loss: 5.473027229309082, Validation Loss: 5.430823802947998\n",
      "Epoch 2508/10000, Training Loss: 3.9868905544281006, Validation Loss: 2.2861270904541016\n",
      "Epoch 2509/10000, Training Loss: 4.698944091796875, Validation Loss: 3.2167317867279053\n",
      "Epoch 2510/10000, Training Loss: 2.5093350410461426, Validation Loss: 2.420240879058838\n",
      "Epoch 2511/10000, Training Loss: 3.2493197917938232, Validation Loss: 6.76392126083374\n",
      "Epoch 2512/10000, Training Loss: 4.0405168533325195, Validation Loss: 1.0077604055404663\n",
      "Epoch 2513/10000, Training Loss: 6.7589263916015625, Validation Loss: 14.310458183288574\n",
      "Epoch 2514/10000, Training Loss: 3.260411262512207, Validation Loss: 5.655506134033203\n",
      "Epoch 2515/10000, Training Loss: 3.4372708797454834, Validation Loss: 2.029249668121338\n",
      "Epoch 2516/10000, Training Loss: 2.4373137950897217, Validation Loss: 3.0183651447296143\n",
      "Epoch 2517/10000, Training Loss: 7.026402473449707, Validation Loss: 18.318052291870117\n",
      "Epoch 2518/10000, Training Loss: 3.22279691696167, Validation Loss: 3.4753074645996094\n",
      "Epoch 2519/10000, Training Loss: 5.851092338562012, Validation Loss: 5.091826915740967\n",
      "Epoch 2520/10000, Training Loss: 3.75921893119812, Validation Loss: 1.4922734498977661\n",
      "Epoch 2521/10000, Training Loss: 2.930586099624634, Validation Loss: 3.359489679336548\n",
      "Epoch 2522/10000, Training Loss: 3.814203977584839, Validation Loss: 5.656501293182373\n",
      "Epoch 2523/10000, Training Loss: 3.981734037399292, Validation Loss: 4.703049659729004\n",
      "Epoch 2524/10000, Training Loss: 3.2757198810577393, Validation Loss: 5.389929294586182\n",
      "Epoch 2525/10000, Training Loss: 4.447482109069824, Validation Loss: 8.330487251281738\n",
      "Epoch 2526/10000, Training Loss: 2.943805694580078, Validation Loss: 2.067962408065796\n",
      "Epoch 2527/10000, Training Loss: 2.8342535495758057, Validation Loss: 7.379055500030518\n",
      "Epoch 2528/10000, Training Loss: 3.5135700702667236, Validation Loss: 4.222313404083252\n",
      "Epoch 2529/10000, Training Loss: 2.406085968017578, Validation Loss: 0.9685545563697815\n",
      "Epoch 2530/10000, Training Loss: 3.001286029815674, Validation Loss: 2.5701088905334473\n",
      "Epoch 2531/10000, Training Loss: 3.485332727432251, Validation Loss: 2.194380521774292\n",
      "Epoch 2532/10000, Training Loss: 3.3341000080108643, Validation Loss: 17.850088119506836\n",
      "Epoch 2533/10000, Training Loss: 4.1290740966796875, Validation Loss: 4.775393009185791\n",
      "Epoch 2534/10000, Training Loss: 4.61749792098999, Validation Loss: 6.648980617523193\n",
      "Epoch 2535/10000, Training Loss: 2.931321144104004, Validation Loss: 3.1054036617279053\n",
      "Epoch 2536/10000, Training Loss: 2.7652993202209473, Validation Loss: 3.964559555053711\n",
      "Epoch 2537/10000, Training Loss: 3.2739546298980713, Validation Loss: 4.0054802894592285\n",
      "Epoch 2538/10000, Training Loss: 4.290968894958496, Validation Loss: 4.631801128387451\n",
      "Epoch 2539/10000, Training Loss: 2.56701922416687, Validation Loss: 8.235909461975098\n",
      "Epoch 2540/10000, Training Loss: 2.4580087661743164, Validation Loss: 4.1379594802856445\n",
      "Epoch 2541/10000, Training Loss: 4.129819869995117, Validation Loss: 6.396091938018799\n",
      "Epoch 2542/10000, Training Loss: 3.8732223510742188, Validation Loss: 10.732959747314453\n",
      "Epoch 2543/10000, Training Loss: 2.482203960418701, Validation Loss: 2.329160690307617\n",
      "Epoch 2544/10000, Training Loss: 4.517777442932129, Validation Loss: 5.0169548988342285\n",
      "Epoch 2545/10000, Training Loss: 6.5127973556518555, Validation Loss: 28.966440200805664\n",
      "Epoch 2546/10000, Training Loss: 3.051368236541748, Validation Loss: 7.682455539703369\n",
      "Epoch 2547/10000, Training Loss: 2.9191927909851074, Validation Loss: 0.959261953830719\n",
      "Epoch 2548/10000, Training Loss: 4.056396007537842, Validation Loss: 8.987373352050781\n",
      "Epoch 2549/10000, Training Loss: 2.8603291511535645, Validation Loss: 6.388269901275635\n",
      "Epoch 2550/10000, Training Loss: 2.9839835166931152, Validation Loss: 3.534595489501953\n",
      "Epoch 2551/10000, Training Loss: 3.256157398223877, Validation Loss: 0.6814602017402649\n",
      "Epoch 2552/10000, Training Loss: 3.49076247215271, Validation Loss: 2.4544854164123535\n",
      "Epoch 2553/10000, Training Loss: 4.2424774169921875, Validation Loss: 2.896501302719116\n",
      "Epoch 2554/10000, Training Loss: 3.061187744140625, Validation Loss: 1.532049298286438\n",
      "Epoch 2555/10000, Training Loss: 3.318526029586792, Validation Loss: 1.987257957458496\n",
      "Epoch 2556/10000, Training Loss: 3.9691081047058105, Validation Loss: 2.100860595703125\n",
      "Epoch 2557/10000, Training Loss: 4.044682025909424, Validation Loss: 2.8827598094940186\n",
      "Epoch 2558/10000, Training Loss: 4.5829644203186035, Validation Loss: 4.645710468292236\n",
      "Epoch 2559/10000, Training Loss: 3.563190221786499, Validation Loss: 0.5625588297843933\n",
      "Epoch 2560/10000, Training Loss: 3.6279428005218506, Validation Loss: 7.978992938995361\n",
      "Epoch 2561/10000, Training Loss: 4.450669765472412, Validation Loss: 2.699519157409668\n",
      "Epoch 2562/10000, Training Loss: 4.364765167236328, Validation Loss: 2.346160650253296\n",
      "Epoch 2563/10000, Training Loss: 4.185064315795898, Validation Loss: 3.627950668334961\n",
      "Epoch 2564/10000, Training Loss: 3.3175888061523438, Validation Loss: 5.600412368774414\n",
      "Epoch 2565/10000, Training Loss: 2.9174489974975586, Validation Loss: 2.121385335922241\n",
      "Epoch 2566/10000, Training Loss: 4.29473876953125, Validation Loss: 3.3295862674713135\n",
      "Epoch 2567/10000, Training Loss: 3.5706140995025635, Validation Loss: 3.8209898471832275\n",
      "Epoch 2568/10000, Training Loss: 2.809624195098877, Validation Loss: 2.6246964931488037\n",
      "Epoch 2569/10000, Training Loss: 3.4280407428741455, Validation Loss: 1.5331183671951294\n",
      "Epoch 2570/10000, Training Loss: 3.41330885887146, Validation Loss: 4.339191913604736\n",
      "Epoch 2571/10000, Training Loss: 3.2544708251953125, Validation Loss: 6.182091236114502\n",
      "Epoch 2572/10000, Training Loss: 2.84783673286438, Validation Loss: 1.1504244804382324\n",
      "Epoch 2573/10000, Training Loss: 3.812849283218384, Validation Loss: 5.82532262802124\n",
      "Epoch 2574/10000, Training Loss: 2.5695395469665527, Validation Loss: 4.142649173736572\n",
      "Epoch 2575/10000, Training Loss: 3.4584767818450928, Validation Loss: 4.406082630157471\n",
      "Epoch 2576/10000, Training Loss: 3.726008415222168, Validation Loss: 5.901726245880127\n",
      "Epoch 2577/10000, Training Loss: 4.658254623413086, Validation Loss: 4.042882919311523\n",
      "Epoch 2578/10000, Training Loss: 3.7416088581085205, Validation Loss: 3.426525831222534\n",
      "Epoch 2579/10000, Training Loss: 3.0616025924682617, Validation Loss: 2.4521491527557373\n",
      "Epoch 2580/10000, Training Loss: 3.0514371395111084, Validation Loss: 4.361809253692627\n",
      "Epoch 2581/10000, Training Loss: 3.952129602432251, Validation Loss: 2.55519700050354\n",
      "Epoch 2582/10000, Training Loss: 2.9196715354919434, Validation Loss: 3.404376983642578\n",
      "Epoch 2583/10000, Training Loss: 4.703383445739746, Validation Loss: 2.125556707382202\n",
      "Epoch 2584/10000, Training Loss: 4.891112327575684, Validation Loss: 6.741506576538086\n",
      "Epoch 2585/10000, Training Loss: 4.043209075927734, Validation Loss: 3.0311172008514404\n",
      "Epoch 2586/10000, Training Loss: 3.545684576034546, Validation Loss: 4.856348514556885\n",
      "Epoch 2587/10000, Training Loss: 3.203152656555176, Validation Loss: 4.539237976074219\n",
      "Epoch 2588/10000, Training Loss: 3.144476890563965, Validation Loss: 2.416825771331787\n",
      "Epoch 2589/10000, Training Loss: 2.8158507347106934, Validation Loss: 5.2917327880859375\n",
      "Epoch 2590/10000, Training Loss: 2.8869333267211914, Validation Loss: 4.234286308288574\n",
      "Epoch 2591/10000, Training Loss: 3.8685598373413086, Validation Loss: 7.980721950531006\n",
      "Epoch 2592/10000, Training Loss: 4.247893333435059, Validation Loss: 3.381878137588501\n",
      "Epoch 2593/10000, Training Loss: 3.0280351638793945, Validation Loss: 3.64595103263855\n",
      "Epoch 2594/10000, Training Loss: 2.6376729011535645, Validation Loss: 4.012003421783447\n",
      "Epoch 2595/10000, Training Loss: 2.7541139125823975, Validation Loss: 4.697658061981201\n",
      "Epoch 2596/10000, Training Loss: 3.869452476501465, Validation Loss: 1.5314526557922363\n",
      "Epoch 2597/10000, Training Loss: 1.995727777481079, Validation Loss: 1.156842589378357\n",
      "Epoch 2598/10000, Training Loss: 2.6779239177703857, Validation Loss: 3.5275325775146484\n",
      "Epoch 2599/10000, Training Loss: 2.7816355228424072, Validation Loss: 2.1253631114959717\n",
      "Epoch 2600/10000, Training Loss: 2.6199421882629395, Validation Loss: 4.893511772155762\n",
      "Epoch 2601/10000, Training Loss: 3.131619453430176, Validation Loss: 6.653597354888916\n",
      "Epoch 2602/10000, Training Loss: 3.3852643966674805, Validation Loss: 3.6470792293548584\n",
      "Epoch 2603/10000, Training Loss: 2.4063210487365723, Validation Loss: 1.7557257413864136\n",
      "Epoch 2604/10000, Training Loss: 3.5508298873901367, Validation Loss: 3.828498125076294\n",
      "Epoch 2605/10000, Training Loss: 2.431812286376953, Validation Loss: 2.7814223766326904\n",
      "Epoch 2606/10000, Training Loss: 3.5419671535491943, Validation Loss: 2.6029105186462402\n",
      "Epoch 2607/10000, Training Loss: 2.9478344917297363, Validation Loss: 2.2020983695983887\n",
      "Epoch 2608/10000, Training Loss: 3.75888729095459, Validation Loss: 6.429311752319336\n",
      "Epoch 2609/10000, Training Loss: 4.097203731536865, Validation Loss: 8.867510795593262\n",
      "Epoch 2610/10000, Training Loss: 3.7937817573547363, Validation Loss: 3.440056085586548\n",
      "Epoch 2611/10000, Training Loss: 3.7763619422912598, Validation Loss: 3.2150933742523193\n",
      "Epoch 2612/10000, Training Loss: 3.0109751224517822, Validation Loss: 0.2012733668088913\n",
      "Epoch 2613/10000, Training Loss: 2.641902446746826, Validation Loss: 6.500575542449951\n",
      "Epoch 2614/10000, Training Loss: 4.335064888000488, Validation Loss: 3.9203174114227295\n",
      "Epoch 2615/10000, Training Loss: 4.9616217613220215, Validation Loss: 2.5343551635742188\n",
      "Epoch 2616/10000, Training Loss: 4.74150276184082, Validation Loss: 3.0819671154022217\n",
      "Epoch 2617/10000, Training Loss: 5.248483657836914, Validation Loss: 7.334431171417236\n",
      "Epoch 2618/10000, Training Loss: 3.207420825958252, Validation Loss: 7.998737335205078\n",
      "Epoch 2619/10000, Training Loss: 3.588848114013672, Validation Loss: 3.5833022594451904\n",
      "Epoch 2620/10000, Training Loss: 4.871568202972412, Validation Loss: 5.492410182952881\n",
      "Epoch 2621/10000, Training Loss: 1.8354763984680176, Validation Loss: 3.6293458938598633\n",
      "Epoch 2622/10000, Training Loss: 3.3713009357452393, Validation Loss: 4.354883670806885\n",
      "Epoch 2623/10000, Training Loss: 2.745760440826416, Validation Loss: 2.3279378414154053\n",
      "Epoch 2624/10000, Training Loss: 3.439768075942993, Validation Loss: 6.589120864868164\n",
      "Epoch 2625/10000, Training Loss: 2.9053714275360107, Validation Loss: 2.6849801540374756\n",
      "Epoch 2626/10000, Training Loss: 2.461068630218506, Validation Loss: 3.058084726333618\n",
      "Epoch 2627/10000, Training Loss: 4.290131092071533, Validation Loss: 2.215012311935425\n",
      "Epoch 2628/10000, Training Loss: 3.0393266677856445, Validation Loss: 3.1125080585479736\n",
      "Epoch 2629/10000, Training Loss: 4.619214057922363, Validation Loss: 6.219449996948242\n",
      "Epoch 2630/10000, Training Loss: 2.600417137145996, Validation Loss: 2.7991178035736084\n",
      "Epoch 2631/10000, Training Loss: 2.699212074279785, Validation Loss: 3.0953664779663086\n",
      "Epoch 2632/10000, Training Loss: 2.9325876235961914, Validation Loss: 3.2141952514648438\n",
      "Epoch 2633/10000, Training Loss: 2.4744300842285156, Validation Loss: 7.060612201690674\n",
      "Epoch 2634/10000, Training Loss: 3.242889642715454, Validation Loss: 5.447383880615234\n",
      "Epoch 2635/10000, Training Loss: 2.912871837615967, Validation Loss: 2.584271192550659\n",
      "Epoch 2636/10000, Training Loss: 2.94787859916687, Validation Loss: 1.4642819166183472\n",
      "Epoch 2637/10000, Training Loss: 4.957605838775635, Validation Loss: 7.7216315269470215\n",
      "Epoch 2638/10000, Training Loss: 2.4368839263916016, Validation Loss: 3.8791654109954834\n",
      "Epoch 2639/10000, Training Loss: 4.729675769805908, Validation Loss: 4.634668350219727\n",
      "Epoch 2640/10000, Training Loss: 2.2763774394989014, Validation Loss: 4.065438270568848\n",
      "Epoch 2641/10000, Training Loss: 2.93988037109375, Validation Loss: 2.420714855194092\n",
      "Epoch 2642/10000, Training Loss: 2.5423126220703125, Validation Loss: 1.9790421724319458\n",
      "Epoch 2643/10000, Training Loss: 3.5237956047058105, Validation Loss: 5.624536991119385\n",
      "Epoch 2644/10000, Training Loss: 1.9590396881103516, Validation Loss: 2.486328363418579\n",
      "Epoch 2645/10000, Training Loss: 3.12579345703125, Validation Loss: 4.3831329345703125\n",
      "Epoch 2646/10000, Training Loss: 2.839665651321411, Validation Loss: 1.0672088861465454\n",
      "Epoch 2647/10000, Training Loss: 2.8068928718566895, Validation Loss: 2.8575103282928467\n",
      "Epoch 2648/10000, Training Loss: 3.025923252105713, Validation Loss: 5.187372207641602\n",
      "Epoch 2649/10000, Training Loss: 2.692147731781006, Validation Loss: 5.260265827178955\n",
      "Epoch 2650/10000, Training Loss: 4.342312812805176, Validation Loss: 3.7380847930908203\n",
      "Epoch 2651/10000, Training Loss: 2.736670970916748, Validation Loss: 3.7994587421417236\n",
      "Epoch 2652/10000, Training Loss: 4.494569778442383, Validation Loss: 6.653467655181885\n",
      "Epoch 2653/10000, Training Loss: 2.9879367351531982, Validation Loss: 5.09220552444458\n",
      "Epoch 2654/10000, Training Loss: 3.2043113708496094, Validation Loss: 4.141103267669678\n",
      "Epoch 2655/10000, Training Loss: 2.6791036128997803, Validation Loss: 4.387498378753662\n",
      "Epoch 2656/10000, Training Loss: 3.34149432182312, Validation Loss: 3.241772413253784\n",
      "Epoch 2657/10000, Training Loss: 3.2357611656188965, Validation Loss: 3.711750030517578\n",
      "Epoch 2658/10000, Training Loss: 5.2856245040893555, Validation Loss: 12.301704406738281\n",
      "Epoch 2659/10000, Training Loss: 3.2051429748535156, Validation Loss: 5.227100849151611\n",
      "Epoch 2660/10000, Training Loss: 4.42274808883667, Validation Loss: 2.8380777835845947\n",
      "Epoch 2661/10000, Training Loss: 5.5095415115356445, Validation Loss: 9.373152732849121\n",
      "Epoch 2662/10000, Training Loss: 1.887513518333435, Validation Loss: 2.632547616958618\n",
      "Epoch 2663/10000, Training Loss: 4.039145469665527, Validation Loss: 4.353701114654541\n",
      "Epoch 2664/10000, Training Loss: 2.6172332763671875, Validation Loss: 8.160183906555176\n",
      "Epoch 2665/10000, Training Loss: 3.4088592529296875, Validation Loss: 4.806485652923584\n",
      "Epoch 2666/10000, Training Loss: 3.1253433227539062, Validation Loss: 12.687275886535645\n",
      "Epoch 2667/10000, Training Loss: 3.527247428894043, Validation Loss: 2.5367095470428467\n",
      "Epoch 2668/10000, Training Loss: 3.8132669925689697, Validation Loss: 4.987945079803467\n",
      "Epoch 2669/10000, Training Loss: 3.2913243770599365, Validation Loss: 3.6871416568756104\n",
      "Epoch 2670/10000, Training Loss: 2.827467918395996, Validation Loss: 4.073177337646484\n",
      "Epoch 2671/10000, Training Loss: 3.0769102573394775, Validation Loss: 5.571679592132568\n",
      "Epoch 2672/10000, Training Loss: 6.472877502441406, Validation Loss: 9.013199806213379\n",
      "Epoch 2673/10000, Training Loss: 2.7113003730773926, Validation Loss: 1.8902359008789062\n",
      "Epoch 2674/10000, Training Loss: 3.1924939155578613, Validation Loss: 4.9629364013671875\n",
      "Epoch 2675/10000, Training Loss: 2.489901065826416, Validation Loss: 1.3476098775863647\n",
      "Epoch 2676/10000, Training Loss: 3.05704665184021, Validation Loss: 4.262871742248535\n",
      "Epoch 2677/10000, Training Loss: 3.9619839191436768, Validation Loss: 6.740131378173828\n",
      "Epoch 2678/10000, Training Loss: 2.6255738735198975, Validation Loss: 3.1473963260650635\n",
      "Epoch 2679/10000, Training Loss: 2.0962107181549072, Validation Loss: 0.3748261034488678\n",
      "Epoch 2680/10000, Training Loss: 1.887692928314209, Validation Loss: 0.2843486964702606\n",
      "Epoch 2681/10000, Training Loss: 4.481176853179932, Validation Loss: 8.16787052154541\n",
      "Epoch 2682/10000, Training Loss: 2.9958395957946777, Validation Loss: 5.67067289352417\n",
      "Epoch 2683/10000, Training Loss: 2.24514102935791, Validation Loss: 3.6941111087799072\n",
      "Epoch 2684/10000, Training Loss: 2.6405160427093506, Validation Loss: 5.967103481292725\n",
      "Epoch 2685/10000, Training Loss: 2.8919661045074463, Validation Loss: 6.899784564971924\n",
      "Epoch 2686/10000, Training Loss: 2.8057124614715576, Validation Loss: 5.266790866851807\n",
      "Epoch 2687/10000, Training Loss: 3.253722906112671, Validation Loss: 3.8010129928588867\n",
      "Epoch 2688/10000, Training Loss: 4.111149787902832, Validation Loss: 7.192481517791748\n",
      "Epoch 2689/10000, Training Loss: 3.2746102809906006, Validation Loss: 6.114144802093506\n",
      "Epoch 2690/10000, Training Loss: 2.337897539138794, Validation Loss: 5.603120803833008\n",
      "Epoch 2691/10000, Training Loss: 3.0181667804718018, Validation Loss: 1.9276418685913086\n",
      "Epoch 2692/10000, Training Loss: 3.3743181228637695, Validation Loss: 3.040529251098633\n",
      "Epoch 2693/10000, Training Loss: 3.779386043548584, Validation Loss: 1.6771349906921387\n",
      "Epoch 2694/10000, Training Loss: 3.279952049255371, Validation Loss: 2.686147928237915\n",
      "Epoch 2695/10000, Training Loss: 2.174293041229248, Validation Loss: 4.016956806182861\n",
      "Epoch 2696/10000, Training Loss: 3.6844944953918457, Validation Loss: 4.433332443237305\n",
      "Epoch 2697/10000, Training Loss: 2.2217347621917725, Validation Loss: 6.168910503387451\n",
      "Epoch 2698/10000, Training Loss: 3.875957489013672, Validation Loss: 1.354135513305664\n",
      "Epoch 2699/10000, Training Loss: 6.135388374328613, Validation Loss: 15.374988555908203\n",
      "Epoch 2700/10000, Training Loss: 2.3020217418670654, Validation Loss: 2.5425941944122314\n",
      "Epoch 2701/10000, Training Loss: 2.6347477436065674, Validation Loss: 0.9502429962158203\n",
      "Epoch 2702/10000, Training Loss: 3.2913715839385986, Validation Loss: 3.4849910736083984\n",
      "Epoch 2703/10000, Training Loss: 4.437384128570557, Validation Loss: 1.5775694847106934\n",
      "Epoch 2704/10000, Training Loss: 2.6577951908111572, Validation Loss: 1.7092519998550415\n",
      "Epoch 2705/10000, Training Loss: 2.816540241241455, Validation Loss: 5.623725891113281\n",
      "Epoch 2706/10000, Training Loss: 4.013991832733154, Validation Loss: 3.223858594894409\n",
      "Epoch 2707/10000, Training Loss: 2.6716368198394775, Validation Loss: 1.9572815895080566\n",
      "Epoch 2708/10000, Training Loss: 2.436521053314209, Validation Loss: 1.9446135759353638\n",
      "Epoch 2709/10000, Training Loss: 3.8605856895446777, Validation Loss: 29.856840133666992\n",
      "Epoch 2710/10000, Training Loss: 3.7151412963867188, Validation Loss: 7.619298934936523\n",
      "Epoch 2711/10000, Training Loss: 3.0618796348571777, Validation Loss: 3.3002512454986572\n",
      "Epoch 2712/10000, Training Loss: 2.4889309406280518, Validation Loss: 3.705747604370117\n",
      "Epoch 2713/10000, Training Loss: 3.895805597305298, Validation Loss: 8.897576332092285\n",
      "Epoch 2714/10000, Training Loss: 2.8639047145843506, Validation Loss: 3.0677757263183594\n",
      "Epoch 2715/10000, Training Loss: 2.9619240760803223, Validation Loss: 3.337653398513794\n",
      "Epoch 2716/10000, Training Loss: 2.6032323837280273, Validation Loss: 8.528764724731445\n",
      "Epoch 2717/10000, Training Loss: 2.779031276702881, Validation Loss: 3.3444740772247314\n",
      "Epoch 2718/10000, Training Loss: 2.590364933013916, Validation Loss: 1.9548554420471191\n",
      "Epoch 2719/10000, Training Loss: 3.6682944297790527, Validation Loss: 4.447218418121338\n",
      "Epoch 2720/10000, Training Loss: 3.0698373317718506, Validation Loss: 2.457719326019287\n",
      "Epoch 2721/10000, Training Loss: 2.3817431926727295, Validation Loss: 2.2987990379333496\n",
      "Epoch 2722/10000, Training Loss: 2.7259249687194824, Validation Loss: 0.5701224207878113\n",
      "Epoch 2723/10000, Training Loss: 2.6999545097351074, Validation Loss: 4.611355781555176\n",
      "Epoch 2724/10000, Training Loss: 2.7001261711120605, Validation Loss: 1.065116047859192\n",
      "Epoch 2725/10000, Training Loss: 3.0257937908172607, Validation Loss: 2.623541831970215\n",
      "Epoch 2726/10000, Training Loss: 4.569578170776367, Validation Loss: 4.464406490325928\n",
      "Epoch 2727/10000, Training Loss: 3.150020122528076, Validation Loss: 5.170485973358154\n",
      "Epoch 2728/10000, Training Loss: 2.4244117736816406, Validation Loss: 1.5145158767700195\n",
      "Epoch 2729/10000, Training Loss: 2.6405487060546875, Validation Loss: 0.8679025173187256\n",
      "Epoch 2730/10000, Training Loss: 2.026848077774048, Validation Loss: 1.6092082262039185\n",
      "Epoch 2731/10000, Training Loss: 3.0251924991607666, Validation Loss: 1.5131994485855103\n",
      "Epoch 2732/10000, Training Loss: 2.729685068130493, Validation Loss: 2.9542276859283447\n",
      "Epoch 2733/10000, Training Loss: 2.450078010559082, Validation Loss: 1.9346827268600464\n",
      "Epoch 2734/10000, Training Loss: 3.247472047805786, Validation Loss: 1.178083062171936\n",
      "Epoch 2735/10000, Training Loss: 3.6562414169311523, Validation Loss: 4.059650421142578\n",
      "Epoch 2736/10000, Training Loss: 3.367043972015381, Validation Loss: 7.313696384429932\n",
      "Epoch 2737/10000, Training Loss: 2.468137264251709, Validation Loss: 3.028858184814453\n",
      "Epoch 2738/10000, Training Loss: 2.782975196838379, Validation Loss: 1.4180630445480347\n",
      "Epoch 2739/10000, Training Loss: 2.9792895317077637, Validation Loss: 2.634387254714966\n",
      "Epoch 2740/10000, Training Loss: 3.219249963760376, Validation Loss: 4.778899669647217\n",
      "Epoch 2741/10000, Training Loss: 2.7780723571777344, Validation Loss: 4.958718776702881\n",
      "Epoch 2742/10000, Training Loss: 3.5245678424835205, Validation Loss: 4.090404033660889\n",
      "Epoch 2743/10000, Training Loss: 3.0185365676879883, Validation Loss: 4.291937351226807\n",
      "Epoch 2744/10000, Training Loss: 2.569286823272705, Validation Loss: 3.3225173950195312\n",
      "Epoch 2745/10000, Training Loss: 2.885637044906616, Validation Loss: 2.672511339187622\n",
      "Epoch 2746/10000, Training Loss: 2.290977716445923, Validation Loss: 4.108821392059326\n",
      "Epoch 2747/10000, Training Loss: 3.2477974891662598, Validation Loss: 4.910861015319824\n",
      "Epoch 2748/10000, Training Loss: 3.5546040534973145, Validation Loss: 0.28327852487564087\n",
      "Epoch 2749/10000, Training Loss: 2.722712278366089, Validation Loss: 3.6740944385528564\n",
      "Epoch 2750/10000, Training Loss: 2.678809881210327, Validation Loss: 4.767480373382568\n",
      "Epoch 2751/10000, Training Loss: 3.1496942043304443, Validation Loss: 1.194602370262146\n",
      "Epoch 2752/10000, Training Loss: 2.0633597373962402, Validation Loss: 4.770108222961426\n",
      "Epoch 2753/10000, Training Loss: 4.0286865234375, Validation Loss: 4.2375993728637695\n",
      "Epoch 2754/10000, Training Loss: 2.5240910053253174, Validation Loss: 2.9996612071990967\n",
      "Epoch 2755/10000, Training Loss: 4.232743263244629, Validation Loss: 5.899841785430908\n",
      "Epoch 2756/10000, Training Loss: 2.063244104385376, Validation Loss: 1.899593710899353\n",
      "Epoch 2757/10000, Training Loss: 2.858030080795288, Validation Loss: 3.3670082092285156\n",
      "Epoch 2758/10000, Training Loss: 2.5134623050689697, Validation Loss: 6.389919757843018\n",
      "Epoch 2759/10000, Training Loss: 4.642563343048096, Validation Loss: 3.61191725730896\n",
      "Epoch 2760/10000, Training Loss: 3.579800844192505, Validation Loss: 3.438601493835449\n",
      "Epoch 2761/10000, Training Loss: 2.8822054862976074, Validation Loss: 7.8932037353515625\n",
      "Epoch 2762/10000, Training Loss: 2.5158708095550537, Validation Loss: 2.679184675216675\n",
      "Epoch 2763/10000, Training Loss: 3.164820671081543, Validation Loss: 2.25217866897583\n",
      "Epoch 2764/10000, Training Loss: 3.1660115718841553, Validation Loss: 5.233964443206787\n",
      "Epoch 2765/10000, Training Loss: 3.0030417442321777, Validation Loss: 5.06141996383667\n",
      "Epoch 2766/10000, Training Loss: 3.066779613494873, Validation Loss: 2.0191400051116943\n",
      "Epoch 2767/10000, Training Loss: 2.492168664932251, Validation Loss: 5.406108379364014\n",
      "Epoch 2768/10000, Training Loss: 3.098106622695923, Validation Loss: 3.3500053882598877\n",
      "Epoch 2769/10000, Training Loss: 3.953963279724121, Validation Loss: 3.3094961643218994\n",
      "Epoch 2770/10000, Training Loss: 2.6903042793273926, Validation Loss: 6.842872619628906\n",
      "Epoch 2771/10000, Training Loss: 2.3460640907287598, Validation Loss: 3.401599645614624\n",
      "Epoch 2772/10000, Training Loss: 3.063807964324951, Validation Loss: 2.809236764907837\n",
      "Epoch 2773/10000, Training Loss: 2.271670341491699, Validation Loss: 1.7800507545471191\n",
      "Epoch 2774/10000, Training Loss: 2.615098714828491, Validation Loss: 5.468595504760742\n",
      "Epoch 2775/10000, Training Loss: 3.44573712348938, Validation Loss: 4.161777496337891\n",
      "Epoch 2776/10000, Training Loss: 3.335468292236328, Validation Loss: 1.6817831993103027\n",
      "Epoch 2777/10000, Training Loss: 3.5670981407165527, Validation Loss: 8.113930702209473\n",
      "Epoch 2778/10000, Training Loss: 2.783109188079834, Validation Loss: 7.314453125\n",
      "Epoch 2779/10000, Training Loss: 3.1275672912597656, Validation Loss: 1.394111156463623\n",
      "Epoch 2780/10000, Training Loss: 2.3315703868865967, Validation Loss: 2.230748414993286\n",
      "Epoch 2781/10000, Training Loss: 2.5936617851257324, Validation Loss: 3.3648033142089844\n",
      "Epoch 2782/10000, Training Loss: 3.8797314167022705, Validation Loss: 3.503070831298828\n",
      "Epoch 2783/10000, Training Loss: 3.8329901695251465, Validation Loss: 7.0091633796691895\n",
      "Epoch 2784/10000, Training Loss: 2.9931230545043945, Validation Loss: 1.5883523225784302\n",
      "Epoch 2785/10000, Training Loss: 2.927537441253662, Validation Loss: 5.542211055755615\n",
      "Epoch 2786/10000, Training Loss: 2.4042723178863525, Validation Loss: 6.538698196411133\n",
      "Epoch 2787/10000, Training Loss: 3.893700122833252, Validation Loss: 6.2433648109436035\n",
      "Epoch 2788/10000, Training Loss: 2.6076736450195312, Validation Loss: 0.5498152375221252\n",
      "Epoch 2789/10000, Training Loss: 3.4306674003601074, Validation Loss: 1.6205185651779175\n",
      "Epoch 2790/10000, Training Loss: 2.806330442428589, Validation Loss: 6.264679431915283\n",
      "Epoch 2791/10000, Training Loss: 2.94739031791687, Validation Loss: 5.093848705291748\n",
      "Epoch 2792/10000, Training Loss: 2.5603435039520264, Validation Loss: 2.2793686389923096\n",
      "Epoch 2793/10000, Training Loss: 3.171684741973877, Validation Loss: 3.3366870880126953\n",
      "Epoch 2794/10000, Training Loss: 2.098590612411499, Validation Loss: 2.013963460922241\n",
      "Epoch 2795/10000, Training Loss: 2.800575017929077, Validation Loss: 4.215858459472656\n",
      "Epoch 2796/10000, Training Loss: 3.2107110023498535, Validation Loss: 2.0011236667633057\n",
      "Epoch 2797/10000, Training Loss: 2.9256346225738525, Validation Loss: 2.07865834236145\n",
      "Epoch 2798/10000, Training Loss: 2.7370715141296387, Validation Loss: 2.2550742626190186\n",
      "Epoch 2799/10000, Training Loss: 2.4838969707489014, Validation Loss: 2.422243118286133\n",
      "Epoch 2800/10000, Training Loss: 2.3729100227355957, Validation Loss: 2.7240428924560547\n",
      "Epoch 2801/10000, Training Loss: 2.080780506134033, Validation Loss: 2.67024827003479\n",
      "Epoch 2802/10000, Training Loss: 2.7656569480895996, Validation Loss: 4.981115341186523\n",
      "Epoch 2803/10000, Training Loss: 2.4376769065856934, Validation Loss: 4.415979862213135\n",
      "Epoch 2804/10000, Training Loss: 2.6815130710601807, Validation Loss: 4.031531810760498\n",
      "Epoch 2805/10000, Training Loss: 2.8576061725616455, Validation Loss: 1.1856050491333008\n",
      "Epoch 2806/10000, Training Loss: 3.129185914993286, Validation Loss: 6.5561137199401855\n",
      "Epoch 2807/10000, Training Loss: 4.58902645111084, Validation Loss: 1.0392028093338013\n",
      "Epoch 2808/10000, Training Loss: 2.665783405303955, Validation Loss: 5.8024420738220215\n",
      "Epoch 2809/10000, Training Loss: 2.8190510272979736, Validation Loss: 5.720821380615234\n",
      "Epoch 2810/10000, Training Loss: 2.8342432975769043, Validation Loss: 1.990432858467102\n",
      "Epoch 2811/10000, Training Loss: 3.6522326469421387, Validation Loss: 0.6316959261894226\n",
      "Epoch 2812/10000, Training Loss: 2.806277275085449, Validation Loss: 1.6808525323867798\n",
      "Epoch 2813/10000, Training Loss: 2.4934003353118896, Validation Loss: 6.069211483001709\n",
      "Epoch 2814/10000, Training Loss: 3.075617790222168, Validation Loss: 3.761500358581543\n",
      "Epoch 2815/10000, Training Loss: 2.707061529159546, Validation Loss: 2.00862979888916\n",
      "Epoch 2816/10000, Training Loss: 2.913160562515259, Validation Loss: 3.070000410079956\n",
      "Epoch 2817/10000, Training Loss: 2.4709463119506836, Validation Loss: 2.068932294845581\n",
      "Epoch 2818/10000, Training Loss: 2.586747407913208, Validation Loss: 3.1572964191436768\n",
      "Epoch 2819/10000, Training Loss: 2.0746989250183105, Validation Loss: 0.28295043110847473\n",
      "Epoch 2820/10000, Training Loss: 2.497377872467041, Validation Loss: 3.7440383434295654\n",
      "Epoch 2821/10000, Training Loss: 2.184744358062744, Validation Loss: 4.3605217933654785\n",
      "Epoch 2822/10000, Training Loss: 4.06168794631958, Validation Loss: 4.090528964996338\n",
      "Epoch 2823/10000, Training Loss: 3.324144124984741, Validation Loss: 1.0432405471801758\n",
      "Epoch 2824/10000, Training Loss: 3.8267998695373535, Validation Loss: 4.222084999084473\n",
      "Epoch 2825/10000, Training Loss: 2.4641757011413574, Validation Loss: 2.9363434314727783\n",
      "Epoch 2826/10000, Training Loss: 1.8319687843322754, Validation Loss: 1.5695643424987793\n",
      "Epoch 2827/10000, Training Loss: 3.64628267288208, Validation Loss: 5.389881134033203\n",
      "Epoch 2828/10000, Training Loss: 1.8367825746536255, Validation Loss: 0.9310908317565918\n",
      "Epoch 2829/10000, Training Loss: 2.904674530029297, Validation Loss: 1.5599256753921509\n",
      "Epoch 2830/10000, Training Loss: 3.0914740562438965, Validation Loss: 3.112974166870117\n",
      "Epoch 2831/10000, Training Loss: 3.0522918701171875, Validation Loss: 3.9574692249298096\n",
      "Epoch 2832/10000, Training Loss: 1.9974591732025146, Validation Loss: 3.077238082885742\n",
      "Epoch 2833/10000, Training Loss: 2.486546754837036, Validation Loss: 1.600784182548523\n",
      "Epoch 2834/10000, Training Loss: 2.4783360958099365, Validation Loss: 3.72898268699646\n",
      "Epoch 2835/10000, Training Loss: 2.109323740005493, Validation Loss: 5.650297164916992\n",
      "Epoch 2836/10000, Training Loss: 4.126823902130127, Validation Loss: 3.9363291263580322\n",
      "Epoch 2837/10000, Training Loss: 2.5181894302368164, Validation Loss: 4.631618022918701\n",
      "Epoch 2838/10000, Training Loss: 2.810244560241699, Validation Loss: 3.6105897426605225\n",
      "Epoch 2839/10000, Training Loss: 2.654857635498047, Validation Loss: 0.7507826685905457\n",
      "Epoch 2840/10000, Training Loss: 4.411198616027832, Validation Loss: 7.188330173492432\n",
      "Epoch 2841/10000, Training Loss: 2.920341968536377, Validation Loss: 2.1619656085968018\n",
      "Epoch 2842/10000, Training Loss: 2.2055113315582275, Validation Loss: 5.244250774383545\n",
      "Epoch 2843/10000, Training Loss: 2.399869203567505, Validation Loss: 2.8027002811431885\n",
      "Epoch 2844/10000, Training Loss: 1.857085943222046, Validation Loss: 2.758836507797241\n",
      "Epoch 2845/10000, Training Loss: 1.786675214767456, Validation Loss: 2.074784994125366\n",
      "Epoch 2846/10000, Training Loss: 2.600163698196411, Validation Loss: 5.187130451202393\n",
      "Epoch 2847/10000, Training Loss: 2.413069248199463, Validation Loss: 2.180936336517334\n",
      "Epoch 2848/10000, Training Loss: 2.780933141708374, Validation Loss: 4.6310648918151855\n",
      "Epoch 2849/10000, Training Loss: 1.955904483795166, Validation Loss: 2.3138275146484375\n",
      "Epoch 2850/10000, Training Loss: 3.487914562225342, Validation Loss: 5.950845718383789\n",
      "Epoch 2851/10000, Training Loss: 4.118616580963135, Validation Loss: 10.000643730163574\n",
      "Epoch 2852/10000, Training Loss: 3.8007307052612305, Validation Loss: 2.254544734954834\n",
      "Epoch 2853/10000, Training Loss: 3.2475132942199707, Validation Loss: 4.076449871063232\n",
      "Epoch 2854/10000, Training Loss: 2.4512908458709717, Validation Loss: 1.3776531219482422\n",
      "Epoch 2855/10000, Training Loss: 2.521068572998047, Validation Loss: 3.5700104236602783\n",
      "Epoch 2856/10000, Training Loss: 3.824302911758423, Validation Loss: 7.158313274383545\n",
      "Epoch 2857/10000, Training Loss: 2.6003916263580322, Validation Loss: 2.38407301902771\n",
      "Epoch 2858/10000, Training Loss: 3.298171043395996, Validation Loss: 10.577589988708496\n",
      "Epoch 2859/10000, Training Loss: 2.6959025859832764, Validation Loss: 3.5927743911743164\n",
      "Epoch 2860/10000, Training Loss: 3.1898152828216553, Validation Loss: 1.8060922622680664\n",
      "Epoch 2861/10000, Training Loss: 2.9951982498168945, Validation Loss: 5.791017055511475\n",
      "Epoch 2862/10000, Training Loss: 3.3445138931274414, Validation Loss: 5.868444442749023\n",
      "Epoch 2863/10000, Training Loss: 2.5845947265625, Validation Loss: 4.178568363189697\n",
      "Epoch 2864/10000, Training Loss: 2.2369673252105713, Validation Loss: 1.1396663188934326\n",
      "Epoch 2865/10000, Training Loss: 2.86134934425354, Validation Loss: 4.098080158233643\n",
      "Epoch 2866/10000, Training Loss: 2.690260410308838, Validation Loss: 4.964666843414307\n",
      "Epoch 2867/10000, Training Loss: 4.045966148376465, Validation Loss: 8.22580337524414\n",
      "Epoch 2868/10000, Training Loss: 2.863044500350952, Validation Loss: 4.220287322998047\n",
      "Epoch 2869/10000, Training Loss: 2.257319450378418, Validation Loss: 3.2702529430389404\n",
      "Epoch 2870/10000, Training Loss: 2.560849666595459, Validation Loss: 4.310210227966309\n",
      "Epoch 2871/10000, Training Loss: 2.7056658267974854, Validation Loss: 2.317486047744751\n",
      "Epoch 2872/10000, Training Loss: 2.8096201419830322, Validation Loss: 1.727431297302246\n",
      "Epoch 2873/10000, Training Loss: 3.3510642051696777, Validation Loss: 8.360735893249512\n",
      "Epoch 2874/10000, Training Loss: 3.343531370162964, Validation Loss: 1.5365668535232544\n",
      "Epoch 2875/10000, Training Loss: 3.3876192569732666, Validation Loss: 5.581730365753174\n",
      "Epoch 2876/10000, Training Loss: 3.01088285446167, Validation Loss: 3.5109498500823975\n",
      "Epoch 2877/10000, Training Loss: 2.6146676540374756, Validation Loss: 2.9135310649871826\n",
      "Epoch 2878/10000, Training Loss: 1.6916781663894653, Validation Loss: 1.9826236963272095\n",
      "Epoch 2879/10000, Training Loss: 3.124828577041626, Validation Loss: 2.6295042037963867\n",
      "Epoch 2880/10000, Training Loss: 1.8226901292800903, Validation Loss: 4.095601558685303\n",
      "Epoch 2881/10000, Training Loss: 2.0417351722717285, Validation Loss: 1.4315505027770996\n",
      "Epoch 2882/10000, Training Loss: 1.8857747316360474, Validation Loss: 2.0584452152252197\n",
      "Epoch 2883/10000, Training Loss: 1.541571855545044, Validation Loss: 2.5816280841827393\n",
      "Epoch 2884/10000, Training Loss: 4.137894153594971, Validation Loss: 3.3361756801605225\n",
      "Epoch 2885/10000, Training Loss: 2.3020224571228027, Validation Loss: 1.5405758619308472\n",
      "Epoch 2886/10000, Training Loss: 4.169251441955566, Validation Loss: 9.194504737854004\n",
      "Epoch 2887/10000, Training Loss: 2.3891513347625732, Validation Loss: 6.035879611968994\n",
      "Epoch 2888/10000, Training Loss: 2.9486124515533447, Validation Loss: 3.247178077697754\n",
      "Epoch 2889/10000, Training Loss: 2.4964232444763184, Validation Loss: 2.460080862045288\n",
      "Epoch 2890/10000, Training Loss: 2.666862964630127, Validation Loss: 0.792942464351654\n",
      "Epoch 2891/10000, Training Loss: 3.2114269733428955, Validation Loss: 5.179717063903809\n",
      "Epoch 2892/10000, Training Loss: 3.71289324760437, Validation Loss: 4.079617023468018\n",
      "Epoch 2893/10000, Training Loss: 2.553419828414917, Validation Loss: 1.299420952796936\n",
      "Epoch 2894/10000, Training Loss: 2.7768819332122803, Validation Loss: 1.7359766960144043\n",
      "Epoch 2895/10000, Training Loss: 2.936516284942627, Validation Loss: 1.627977967262268\n",
      "Epoch 2896/10000, Training Loss: 4.06994104385376, Validation Loss: 6.63715934753418\n",
      "Epoch 2897/10000, Training Loss: 3.4218356609344482, Validation Loss: 1.5007117986679077\n",
      "Epoch 2898/10000, Training Loss: 2.4623911380767822, Validation Loss: 8.793076515197754\n",
      "Epoch 2899/10000, Training Loss: 3.283876657485962, Validation Loss: 5.25857400894165\n",
      "Epoch 2900/10000, Training Loss: 2.6147091388702393, Validation Loss: 4.745639801025391\n",
      "Epoch 2901/10000, Training Loss: 1.9780604839324951, Validation Loss: 4.226998805999756\n",
      "Epoch 2902/10000, Training Loss: 3.1278343200683594, Validation Loss: 4.107179164886475\n",
      "Epoch 2903/10000, Training Loss: 2.180835485458374, Validation Loss: 2.5047600269317627\n",
      "Epoch 2904/10000, Training Loss: 3.289973020553589, Validation Loss: 5.250080585479736\n",
      "Epoch 2905/10000, Training Loss: 2.439168930053711, Validation Loss: 2.3838698863983154\n",
      "Epoch 2906/10000, Training Loss: 4.168096542358398, Validation Loss: 2.6490683555603027\n",
      "Epoch 2907/10000, Training Loss: 2.291511058807373, Validation Loss: 3.2158708572387695\n",
      "Epoch 2908/10000, Training Loss: 2.6334657669067383, Validation Loss: 3.283247947692871\n",
      "Epoch 2909/10000, Training Loss: 2.068906784057617, Validation Loss: 1.8795441389083862\n",
      "Epoch 2910/10000, Training Loss: 2.2825639247894287, Validation Loss: 5.2355637550354\n",
      "Epoch 2911/10000, Training Loss: 2.4806125164031982, Validation Loss: 4.181957721710205\n",
      "Epoch 2912/10000, Training Loss: 2.054586172103882, Validation Loss: 1.5407060384750366\n",
      "Epoch 2913/10000, Training Loss: 2.4644620418548584, Validation Loss: 1.2218635082244873\n",
      "Epoch 2914/10000, Training Loss: 2.2150557041168213, Validation Loss: 2.0388314723968506\n",
      "Epoch 2915/10000, Training Loss: 2.359607219696045, Validation Loss: 5.539993762969971\n",
      "Epoch 2916/10000, Training Loss: 1.763062834739685, Validation Loss: 5.402724742889404\n",
      "Epoch 2917/10000, Training Loss: 2.3128609657287598, Validation Loss: 6.042112827301025\n",
      "Epoch 2918/10000, Training Loss: 2.163513422012329, Validation Loss: 3.628904342651367\n",
      "Epoch 2919/10000, Training Loss: 3.0737485885620117, Validation Loss: 1.9401847124099731\n",
      "Epoch 2920/10000, Training Loss: 2.4924800395965576, Validation Loss: 0.9763585925102234\n",
      "Epoch 2921/10000, Training Loss: 1.5125397443771362, Validation Loss: 2.411524534225464\n",
      "Epoch 2922/10000, Training Loss: 2.6356430053710938, Validation Loss: 3.5768258571624756\n",
      "Epoch 2923/10000, Training Loss: 3.3314807415008545, Validation Loss: 4.414520740509033\n",
      "Epoch 2924/10000, Training Loss: 2.680722713470459, Validation Loss: 3.0878970623016357\n",
      "Epoch 2925/10000, Training Loss: 2.219093084335327, Validation Loss: 2.659202814102173\n",
      "Epoch 2926/10000, Training Loss: 3.079805612564087, Validation Loss: 3.902683973312378\n",
      "Epoch 2927/10000, Training Loss: 2.9270098209381104, Validation Loss: 2.118612289428711\n",
      "Epoch 2928/10000, Training Loss: 3.586103677749634, Validation Loss: 2.911912202835083\n",
      "Epoch 2929/10000, Training Loss: 2.517037868499756, Validation Loss: 3.168933868408203\n",
      "Epoch 2930/10000, Training Loss: 2.080596685409546, Validation Loss: 1.8575869798660278\n",
      "Epoch 2931/10000, Training Loss: 2.5955350399017334, Validation Loss: 1.5688825845718384\n",
      "Epoch 2932/10000, Training Loss: 2.6027145385742188, Validation Loss: 3.5430490970611572\n",
      "Epoch 2933/10000, Training Loss: 2.4525680541992188, Validation Loss: 6.9196319580078125\n",
      "Epoch 2934/10000, Training Loss: 2.898329734802246, Validation Loss: 4.583066463470459\n",
      "Epoch 2935/10000, Training Loss: 3.2587404251098633, Validation Loss: 1.667291522026062\n",
      "Epoch 2936/10000, Training Loss: 3.339399814605713, Validation Loss: 0.7600655555725098\n",
      "Epoch 2937/10000, Training Loss: 2.79583740234375, Validation Loss: 6.111885070800781\n",
      "Epoch 2938/10000, Training Loss: 1.747978687286377, Validation Loss: 1.895682692527771\n",
      "Epoch 2939/10000, Training Loss: 2.001251697540283, Validation Loss: 1.1663316488265991\n",
      "Epoch 2940/10000, Training Loss: 2.1533970832824707, Validation Loss: 1.2792648077011108\n",
      "Epoch 2941/10000, Training Loss: 2.441037654876709, Validation Loss: 2.763625383377075\n",
      "Epoch 2942/10000, Training Loss: 3.2655463218688965, Validation Loss: 4.728518486022949\n",
      "Epoch 2943/10000, Training Loss: 2.4329302310943604, Validation Loss: 0.44945085048675537\n",
      "Epoch 2944/10000, Training Loss: 2.3318474292755127, Validation Loss: 0.7678835988044739\n",
      "Epoch 2945/10000, Training Loss: 3.2086312770843506, Validation Loss: 3.3240573406219482\n",
      "Epoch 2946/10000, Training Loss: 3.124549388885498, Validation Loss: 2.7039706707000732\n",
      "Epoch 2947/10000, Training Loss: 2.7831573486328125, Validation Loss: 3.0495615005493164\n",
      "Epoch 2948/10000, Training Loss: 3.0514285564422607, Validation Loss: 6.425416469573975\n",
      "Epoch 2949/10000, Training Loss: 1.7234095335006714, Validation Loss: 1.6633296012878418\n",
      "Epoch 2950/10000, Training Loss: 2.7452776432037354, Validation Loss: 6.0527825355529785\n",
      "Epoch 2951/10000, Training Loss: 3.6756386756896973, Validation Loss: 2.6091701984405518\n",
      "Epoch 2952/10000, Training Loss: 3.040900707244873, Validation Loss: 2.128591537475586\n",
      "Epoch 2953/10000, Training Loss: 2.34513258934021, Validation Loss: 1.4803708791732788\n",
      "Epoch 2954/10000, Training Loss: 2.4549009799957275, Validation Loss: 1.1247190237045288\n",
      "Epoch 2955/10000, Training Loss: 3.501828908920288, Validation Loss: 2.9981422424316406\n",
      "Epoch 2956/10000, Training Loss: 3.6041624546051025, Validation Loss: 3.9076881408691406\n",
      "Epoch 2957/10000, Training Loss: 2.367297649383545, Validation Loss: 1.5681685209274292\n",
      "Epoch 2958/10000, Training Loss: 2.4483025074005127, Validation Loss: 0.6849908828735352\n",
      "Epoch 2959/10000, Training Loss: 2.5394341945648193, Validation Loss: 1.517225742340088\n",
      "Epoch 2960/10000, Training Loss: 2.3712680339813232, Validation Loss: 2.1113598346710205\n",
      "Epoch 2961/10000, Training Loss: 2.121748447418213, Validation Loss: 1.4200760126113892\n",
      "Epoch 2962/10000, Training Loss: 1.6275784969329834, Validation Loss: 2.7025089263916016\n",
      "Epoch 2963/10000, Training Loss: 3.4138996601104736, Validation Loss: 3.4829647541046143\n",
      "Epoch 2964/10000, Training Loss: 2.98260498046875, Validation Loss: 4.003356456756592\n",
      "Epoch 2965/10000, Training Loss: 1.7935664653778076, Validation Loss: 2.422457456588745\n",
      "Epoch 2966/10000, Training Loss: 2.3484785556793213, Validation Loss: 1.6121488809585571\n",
      "Epoch 2967/10000, Training Loss: 1.8718740940093994, Validation Loss: 1.4178258180618286\n",
      "Epoch 2968/10000, Training Loss: 1.98328697681427, Validation Loss: 3.2803261280059814\n",
      "Epoch 2969/10000, Training Loss: 3.8157408237457275, Validation Loss: 4.974565029144287\n",
      "Epoch 2970/10000, Training Loss: 2.309424877166748, Validation Loss: 6.708994388580322\n",
      "Epoch 2971/10000, Training Loss: 1.8925387859344482, Validation Loss: 1.683537483215332\n",
      "Epoch 2972/10000, Training Loss: 2.0697243213653564, Validation Loss: 3.5384442806243896\n",
      "Epoch 2973/10000, Training Loss: 1.941994547843933, Validation Loss: 3.7236435413360596\n",
      "Epoch 2974/10000, Training Loss: 2.573514699935913, Validation Loss: 4.444891452789307\n",
      "Epoch 2975/10000, Training Loss: 3.025536060333252, Validation Loss: 1.1354761123657227\n",
      "Epoch 2976/10000, Training Loss: 2.726938486099243, Validation Loss: 1.5577096939086914\n",
      "Epoch 2977/10000, Training Loss: 2.544553518295288, Validation Loss: 4.933732032775879\n",
      "Epoch 2978/10000, Training Loss: 2.224146604537964, Validation Loss: 2.2087910175323486\n",
      "Epoch 2979/10000, Training Loss: 3.2380728721618652, Validation Loss: 2.6271698474884033\n",
      "Epoch 2980/10000, Training Loss: 3.1650390625, Validation Loss: 3.450977087020874\n",
      "Epoch 2981/10000, Training Loss: 2.1793534755706787, Validation Loss: 2.209770679473877\n",
      "Epoch 2982/10000, Training Loss: 1.7580338716506958, Validation Loss: 4.665781497955322\n",
      "Epoch 2983/10000, Training Loss: 2.218031644821167, Validation Loss: 1.8253024816513062\n",
      "Epoch 2984/10000, Training Loss: 1.6655094623565674, Validation Loss: 2.420081853866577\n",
      "Epoch 2985/10000, Training Loss: 2.7130608558654785, Validation Loss: 6.234513759613037\n",
      "Epoch 2986/10000, Training Loss: 2.5190672874450684, Validation Loss: 1.0665786266326904\n",
      "Epoch 2987/10000, Training Loss: 2.5827105045318604, Validation Loss: 1.998945713043213\n",
      "Epoch 2988/10000, Training Loss: 2.831559658050537, Validation Loss: 1.7690333127975464\n",
      "Epoch 2989/10000, Training Loss: 2.0745463371276855, Validation Loss: 2.1908202171325684\n",
      "Epoch 2990/10000, Training Loss: 2.307894706726074, Validation Loss: 1.6603847742080688\n",
      "Epoch 2991/10000, Training Loss: 2.3057570457458496, Validation Loss: 4.450381755828857\n",
      "Epoch 2992/10000, Training Loss: 2.913984537124634, Validation Loss: 1.4384135007858276\n",
      "Epoch 2993/10000, Training Loss: 2.877298593521118, Validation Loss: 3.134518623352051\n",
      "Epoch 2994/10000, Training Loss: 2.888641119003296, Validation Loss: 4.439083099365234\n",
      "Epoch 2995/10000, Training Loss: 2.64107608795166, Validation Loss: 1.0549410581588745\n",
      "Epoch 2996/10000, Training Loss: 2.8223764896392822, Validation Loss: 1.6104384660720825\n",
      "Epoch 2997/10000, Training Loss: 2.515638589859009, Validation Loss: 1.6660361289978027\n",
      "Epoch 2998/10000, Training Loss: 2.5174639225006104, Validation Loss: 4.312867164611816\n",
      "Epoch 2999/10000, Training Loss: 2.855198860168457, Validation Loss: 1.8091505765914917\n",
      "Epoch 3000/10000, Training Loss: 2.2828352451324463, Validation Loss: 2.302072763442993\n",
      "Epoch 3001/10000, Training Loss: 2.3776402473449707, Validation Loss: 2.2652428150177\n",
      "Epoch 3002/10000, Training Loss: 3.533162832260132, Validation Loss: 6.622044086456299\n",
      "Epoch 3003/10000, Training Loss: 2.4750165939331055, Validation Loss: 1.8861876726150513\n",
      "Epoch 3004/10000, Training Loss: 1.702400803565979, Validation Loss: 2.4147145748138428\n",
      "Epoch 3005/10000, Training Loss: 2.578148126602173, Validation Loss: 3.51652455329895\n",
      "Epoch 3006/10000, Training Loss: 2.296299695968628, Validation Loss: 2.299273729324341\n",
      "Epoch 3007/10000, Training Loss: 2.969815731048584, Validation Loss: 3.0916764736175537\n",
      "Epoch 3008/10000, Training Loss: 2.0039732456207275, Validation Loss: 2.573901891708374\n",
      "Epoch 3009/10000, Training Loss: 2.411677360534668, Validation Loss: 1.30618155002594\n",
      "Epoch 3010/10000, Training Loss: 1.6402533054351807, Validation Loss: 2.350532293319702\n",
      "Epoch 3011/10000, Training Loss: 1.9304754734039307, Validation Loss: 5.056187152862549\n",
      "Epoch 3012/10000, Training Loss: 2.184358835220337, Validation Loss: 1.6106477975845337\n",
      "Epoch 3013/10000, Training Loss: 3.1664910316467285, Validation Loss: 1.3579457998275757\n",
      "Epoch 3014/10000, Training Loss: 2.6843011379241943, Validation Loss: 2.686473846435547\n",
      "Epoch 3015/10000, Training Loss: 2.2573111057281494, Validation Loss: 1.705393671989441\n",
      "Epoch 3016/10000, Training Loss: 3.0840108394622803, Validation Loss: 4.8018388748168945\n",
      "Epoch 3017/10000, Training Loss: 2.1838903427124023, Validation Loss: 1.9005770683288574\n",
      "Epoch 3018/10000, Training Loss: 2.047590732574463, Validation Loss: 4.448166370391846\n",
      "Epoch 3019/10000, Training Loss: 2.997527837753296, Validation Loss: 1.7633534669876099\n",
      "Epoch 3020/10000, Training Loss: 3.2849016189575195, Validation Loss: 2.7078685760498047\n",
      "Epoch 3021/10000, Training Loss: 2.3606607913970947, Validation Loss: 1.909972071647644\n",
      "Epoch 3022/10000, Training Loss: 3.1206133365631104, Validation Loss: 4.43464469909668\n",
      "Epoch 3023/10000, Training Loss: 2.1799583435058594, Validation Loss: 1.3984023332595825\n",
      "Epoch 3024/10000, Training Loss: 1.978829026222229, Validation Loss: 4.379082202911377\n",
      "Epoch 3025/10000, Training Loss: 1.7613308429718018, Validation Loss: 1.9022369384765625\n",
      "Epoch 3026/10000, Training Loss: 2.103523015975952, Validation Loss: 2.0657691955566406\n",
      "Epoch 3027/10000, Training Loss: 2.4221277236938477, Validation Loss: 6.813838958740234\n",
      "Epoch 3028/10000, Training Loss: 1.7700042724609375, Validation Loss: 3.7623960971832275\n",
      "Epoch 3029/10000, Training Loss: 1.7709805965423584, Validation Loss: 2.4834096431732178\n",
      "Epoch 3030/10000, Training Loss: 2.509277582168579, Validation Loss: 2.0381317138671875\n",
      "Epoch 3031/10000, Training Loss: 1.8628846406936646, Validation Loss: 1.2418667078018188\n",
      "Epoch 3032/10000, Training Loss: 2.3636629581451416, Validation Loss: 2.6719274520874023\n",
      "Epoch 3033/10000, Training Loss: 2.032181739807129, Validation Loss: 2.6370930671691895\n",
      "Epoch 3034/10000, Training Loss: 2.5500667095184326, Validation Loss: 4.293525218963623\n",
      "Epoch 3035/10000, Training Loss: 2.296335458755493, Validation Loss: 5.584211826324463\n",
      "Epoch 3036/10000, Training Loss: 2.5142271518707275, Validation Loss: 1.8207101821899414\n",
      "Epoch 3037/10000, Training Loss: 3.562575101852417, Validation Loss: 2.1662662029266357\n",
      "Epoch 3038/10000, Training Loss: 2.654487371444702, Validation Loss: 2.344876527786255\n",
      "Epoch 3039/10000, Training Loss: 2.182657480239868, Validation Loss: 1.3870888948440552\n",
      "Epoch 3040/10000, Training Loss: 2.2749242782592773, Validation Loss: 2.299863576889038\n",
      "Epoch 3041/10000, Training Loss: 2.5204126834869385, Validation Loss: 3.362278699874878\n",
      "Epoch 3042/10000, Training Loss: 1.9712339639663696, Validation Loss: 4.523304462432861\n",
      "Epoch 3043/10000, Training Loss: 2.6304121017456055, Validation Loss: 2.566917896270752\n",
      "Epoch 3044/10000, Training Loss: 3.2427914142608643, Validation Loss: 5.805362224578857\n",
      "Epoch 3045/10000, Training Loss: 2.422823429107666, Validation Loss: 2.8181657791137695\n",
      "Epoch 3046/10000, Training Loss: 2.612752676010132, Validation Loss: 2.0155999660491943\n",
      "Epoch 3047/10000, Training Loss: 2.075622320175171, Validation Loss: 0.7480320930480957\n",
      "Epoch 3048/10000, Training Loss: 3.2600934505462646, Validation Loss: 6.166019439697266\n",
      "Epoch 3049/10000, Training Loss: 2.8133153915405273, Validation Loss: 7.174720287322998\n",
      "Epoch 3050/10000, Training Loss: 1.71589195728302, Validation Loss: 3.9901692867279053\n",
      "Epoch 3051/10000, Training Loss: 2.4241700172424316, Validation Loss: 1.2423808574676514\n",
      "Epoch 3052/10000, Training Loss: 2.438504695892334, Validation Loss: 3.6694796085357666\n",
      "Epoch 3053/10000, Training Loss: 2.4017674922943115, Validation Loss: 3.44854474067688\n",
      "Epoch 3054/10000, Training Loss: 2.0445268154144287, Validation Loss: 3.4919965267181396\n",
      "Epoch 3055/10000, Training Loss: 2.238600254058838, Validation Loss: 3.279698133468628\n",
      "Epoch 3056/10000, Training Loss: 1.9694886207580566, Validation Loss: 2.854344606399536\n",
      "Epoch 3057/10000, Training Loss: 2.0261001586914062, Validation Loss: 7.634603977203369\n",
      "Epoch 3058/10000, Training Loss: 2.2957346439361572, Validation Loss: 5.485250949859619\n",
      "Epoch 3059/10000, Training Loss: 2.2749428749084473, Validation Loss: 3.8856847286224365\n",
      "Epoch 3060/10000, Training Loss: 2.5686838626861572, Validation Loss: 1.3870800733566284\n",
      "Epoch 3061/10000, Training Loss: 1.982706904411316, Validation Loss: 3.466327428817749\n",
      "Epoch 3062/10000, Training Loss: 2.0495729446411133, Validation Loss: 2.328062057495117\n",
      "Epoch 3063/10000, Training Loss: 2.5515897274017334, Validation Loss: 2.1778881549835205\n",
      "Epoch 3064/10000, Training Loss: 1.6455048322677612, Validation Loss: 1.133925199508667\n",
      "Epoch 3065/10000, Training Loss: 3.3185696601867676, Validation Loss: 4.25382137298584\n",
      "Epoch 3066/10000, Training Loss: 2.166317939758301, Validation Loss: 4.0796942710876465\n",
      "Epoch 3067/10000, Training Loss: 2.436234712600708, Validation Loss: 1.7164181470870972\n",
      "Epoch 3068/10000, Training Loss: 2.9153337478637695, Validation Loss: 5.903083801269531\n",
      "Epoch 3069/10000, Training Loss: 1.7502611875534058, Validation Loss: 2.7134573459625244\n",
      "Epoch 3070/10000, Training Loss: 2.2043066024780273, Validation Loss: 5.378814220428467\n",
      "Epoch 3071/10000, Training Loss: 1.763706088066101, Validation Loss: 1.3081692457199097\n",
      "Epoch 3072/10000, Training Loss: 1.9291751384735107, Validation Loss: 1.5188530683517456\n",
      "Epoch 3073/10000, Training Loss: 1.9339351654052734, Validation Loss: 1.4274439811706543\n",
      "Epoch 3074/10000, Training Loss: 2.9321141242980957, Validation Loss: 5.817911624908447\n",
      "Epoch 3075/10000, Training Loss: 3.134185791015625, Validation Loss: 1.4583452939987183\n",
      "Epoch 3076/10000, Training Loss: 2.5349719524383545, Validation Loss: 2.106579065322876\n",
      "Epoch 3077/10000, Training Loss: 1.8506395816802979, Validation Loss: 3.264228582382202\n",
      "Epoch 3078/10000, Training Loss: 2.0476202964782715, Validation Loss: 3.812002182006836\n",
      "Epoch 3079/10000, Training Loss: 4.595992565155029, Validation Loss: 4.372544765472412\n",
      "Epoch 3080/10000, Training Loss: 3.960648536682129, Validation Loss: 10.697074890136719\n",
      "Epoch 3081/10000, Training Loss: 2.017273426055908, Validation Loss: 2.152862548828125\n",
      "Epoch 3082/10000, Training Loss: 2.0994086265563965, Validation Loss: 1.5959173440933228\n",
      "Epoch 3083/10000, Training Loss: 2.097503900527954, Validation Loss: 1.3850513696670532\n",
      "Epoch 3084/10000, Training Loss: 1.934187650680542, Validation Loss: 1.9504772424697876\n",
      "Epoch 3085/10000, Training Loss: 2.899383544921875, Validation Loss: 0.9273566603660583\n",
      "Epoch 3086/10000, Training Loss: 2.0095479488372803, Validation Loss: 4.910004138946533\n",
      "Epoch 3087/10000, Training Loss: 1.6796609163284302, Validation Loss: 2.7793350219726562\n",
      "Epoch 3088/10000, Training Loss: 2.263190269470215, Validation Loss: 4.995016098022461\n",
      "Epoch 3089/10000, Training Loss: 1.6018849611282349, Validation Loss: 0.9488694071769714\n",
      "Epoch 3090/10000, Training Loss: 2.4883406162261963, Validation Loss: 0.8503895401954651\n",
      "Epoch 3091/10000, Training Loss: 1.689765214920044, Validation Loss: 2.144362211227417\n",
      "Epoch 3092/10000, Training Loss: 2.7410199642181396, Validation Loss: 3.6375882625579834\n",
      "Epoch 3093/10000, Training Loss: 1.9144060611724854, Validation Loss: 2.9289748668670654\n",
      "Epoch 3094/10000, Training Loss: 2.209836006164551, Validation Loss: 2.716768264770508\n",
      "Epoch 3095/10000, Training Loss: 4.161823272705078, Validation Loss: 3.239189863204956\n",
      "Epoch 3096/10000, Training Loss: 2.423785924911499, Validation Loss: 2.8531081676483154\n",
      "Epoch 3097/10000, Training Loss: 2.246163845062256, Validation Loss: 3.4469146728515625\n",
      "Epoch 3098/10000, Training Loss: 2.558805227279663, Validation Loss: 1.949497103691101\n",
      "Epoch 3099/10000, Training Loss: 1.9762318134307861, Validation Loss: 1.9290415048599243\n",
      "Epoch 3100/10000, Training Loss: 2.0110251903533936, Validation Loss: 1.7017431259155273\n",
      "Epoch 3101/10000, Training Loss: 2.496943950653076, Validation Loss: 4.043610095977783\n",
      "Epoch 3102/10000, Training Loss: 2.976853370666504, Validation Loss: 4.999486923217773\n",
      "Epoch 3103/10000, Training Loss: 1.6741434335708618, Validation Loss: 3.0649824142456055\n",
      "Epoch 3104/10000, Training Loss: 2.145000696182251, Validation Loss: 3.650498151779175\n",
      "Epoch 3105/10000, Training Loss: 2.6593689918518066, Validation Loss: 2.931655168533325\n",
      "Epoch 3106/10000, Training Loss: 1.9772717952728271, Validation Loss: 1.2723500728607178\n",
      "Epoch 3107/10000, Training Loss: 1.7591902017593384, Validation Loss: 0.5690320134162903\n",
      "Epoch 3108/10000, Training Loss: 2.2711470127105713, Validation Loss: 2.1448371410369873\n",
      "Epoch 3109/10000, Training Loss: 1.8250775337219238, Validation Loss: 2.030721426010132\n",
      "Epoch 3110/10000, Training Loss: 2.6511785984039307, Validation Loss: 2.9542322158813477\n",
      "Epoch 3111/10000, Training Loss: 2.7681851387023926, Validation Loss: 1.3320410251617432\n",
      "Epoch 3112/10000, Training Loss: 2.4571943283081055, Validation Loss: 2.265582323074341\n",
      "Epoch 3113/10000, Training Loss: 2.2871973514556885, Validation Loss: 5.5807576179504395\n",
      "Epoch 3114/10000, Training Loss: 1.6379845142364502, Validation Loss: 3.867341995239258\n",
      "Epoch 3115/10000, Training Loss: 2.56591796875, Validation Loss: 3.5991628170013428\n",
      "Epoch 3116/10000, Training Loss: 2.5731899738311768, Validation Loss: 4.302057266235352\n",
      "Epoch 3117/10000, Training Loss: 1.7595781087875366, Validation Loss: 3.191995620727539\n",
      "Epoch 3118/10000, Training Loss: 1.540813684463501, Validation Loss: 3.0433690547943115\n",
      "Epoch 3119/10000, Training Loss: 1.2746188640594482, Validation Loss: 1.85792076587677\n",
      "Epoch 3120/10000, Training Loss: 1.8979853391647339, Validation Loss: 5.588783264160156\n",
      "Epoch 3121/10000, Training Loss: 1.3102290630340576, Validation Loss: 2.388840913772583\n",
      "Epoch 3122/10000, Training Loss: 2.4304707050323486, Validation Loss: 3.5169572830200195\n",
      "Epoch 3123/10000, Training Loss: 1.3995361328125, Validation Loss: 3.3084795475006104\n",
      "Epoch 3124/10000, Training Loss: 1.8841288089752197, Validation Loss: 3.1539456844329834\n",
      "Epoch 3125/10000, Training Loss: 1.9962188005447388, Validation Loss: 3.0623273849487305\n",
      "Epoch 3126/10000, Training Loss: 1.9632220268249512, Validation Loss: 1.639577031135559\n",
      "Epoch 3127/10000, Training Loss: 2.3826186656951904, Validation Loss: 1.3726463317871094\n",
      "Epoch 3128/10000, Training Loss: 1.723893404006958, Validation Loss: 1.7882280349731445\n",
      "Epoch 3129/10000, Training Loss: 2.474452018737793, Validation Loss: 3.1249992847442627\n",
      "Epoch 3130/10000, Training Loss: 2.4312291145324707, Validation Loss: 5.466209888458252\n",
      "Epoch 3131/10000, Training Loss: 2.193974733352661, Validation Loss: 4.104099750518799\n",
      "Epoch 3132/10000, Training Loss: 1.7566440105438232, Validation Loss: 3.058070182800293\n",
      "Epoch 3133/10000, Training Loss: 4.125186443328857, Validation Loss: 4.292673587799072\n",
      "Epoch 3134/10000, Training Loss: 2.579213857650757, Validation Loss: 3.3073818683624268\n",
      "Epoch 3135/10000, Training Loss: 2.028024673461914, Validation Loss: 4.339785099029541\n",
      "Epoch 3136/10000, Training Loss: 3.349712610244751, Validation Loss: 1.5710684061050415\n",
      "Epoch 3137/10000, Training Loss: 1.586648941040039, Validation Loss: 3.6528356075286865\n",
      "Epoch 3138/10000, Training Loss: 2.41916823387146, Validation Loss: 2.041626214981079\n",
      "Epoch 3139/10000, Training Loss: 1.4445462226867676, Validation Loss: 1.5313715934753418\n",
      "Epoch 3140/10000, Training Loss: 2.1804044246673584, Validation Loss: 1.3932384252548218\n",
      "Epoch 3141/10000, Training Loss: 1.869443655014038, Validation Loss: 3.443452835083008\n",
      "Epoch 3142/10000, Training Loss: 3.1087300777435303, Validation Loss: 7.153041362762451\n",
      "Epoch 3143/10000, Training Loss: 2.585503578186035, Validation Loss: 2.439089298248291\n",
      "Epoch 3144/10000, Training Loss: 2.0048749446868896, Validation Loss: 1.7805147171020508\n",
      "Epoch 3145/10000, Training Loss: 2.1729605197906494, Validation Loss: 1.9136691093444824\n",
      "Epoch 3146/10000, Training Loss: 1.7334929704666138, Validation Loss: 4.474505424499512\n",
      "Epoch 3147/10000, Training Loss: 2.2011773586273193, Validation Loss: 2.0163137912750244\n",
      "Epoch 3148/10000, Training Loss: 2.3504438400268555, Validation Loss: 2.7982959747314453\n",
      "Epoch 3149/10000, Training Loss: 2.1632261276245117, Validation Loss: 1.587221622467041\n",
      "Epoch 3150/10000, Training Loss: 2.222083568572998, Validation Loss: 1.5088577270507812\n",
      "Epoch 3151/10000, Training Loss: 2.5554990768432617, Validation Loss: 3.725393295288086\n",
      "Epoch 3152/10000, Training Loss: 1.6991174221038818, Validation Loss: 2.456766366958618\n",
      "Epoch 3153/10000, Training Loss: 2.2714757919311523, Validation Loss: 2.171787977218628\n",
      "Epoch 3154/10000, Training Loss: 1.850108027458191, Validation Loss: 3.1394355297088623\n",
      "Epoch 3155/10000, Training Loss: 2.3563027381896973, Validation Loss: 6.314392566680908\n",
      "Epoch 3156/10000, Training Loss: 2.8564326763153076, Validation Loss: 2.0678954124450684\n",
      "Epoch 3157/10000, Training Loss: 2.479776620864868, Validation Loss: 1.24793541431427\n",
      "Epoch 3158/10000, Training Loss: 2.109182834625244, Validation Loss: 3.7905025482177734\n",
      "Epoch 3159/10000, Training Loss: 2.8682501316070557, Validation Loss: 3.478773355484009\n",
      "Epoch 3160/10000, Training Loss: 2.0614418983459473, Validation Loss: 2.521813154220581\n",
      "Epoch 3161/10000, Training Loss: 3.6897380352020264, Validation Loss: 4.342146873474121\n",
      "Epoch 3162/10000, Training Loss: 2.220691442489624, Validation Loss: 1.07025945186615\n",
      "Epoch 3163/10000, Training Loss: 2.6309194564819336, Validation Loss: 3.4023869037628174\n",
      "Epoch 3164/10000, Training Loss: 2.6438028812408447, Validation Loss: 1.463129997253418\n",
      "Epoch 3165/10000, Training Loss: 1.7931023836135864, Validation Loss: 2.095905065536499\n",
      "Epoch 3166/10000, Training Loss: 1.589301347732544, Validation Loss: 3.5567283630371094\n",
      "Epoch 3167/10000, Training Loss: 2.4426259994506836, Validation Loss: 8.142836570739746\n",
      "Epoch 3168/10000, Training Loss: 1.652264952659607, Validation Loss: 2.267934799194336\n",
      "Epoch 3169/10000, Training Loss: 1.8520184755325317, Validation Loss: 2.6566078662872314\n",
      "Epoch 3170/10000, Training Loss: 1.8936761617660522, Validation Loss: 1.3396295309066772\n",
      "Epoch 3171/10000, Training Loss: 2.2405848503112793, Validation Loss: 2.677125930786133\n",
      "Epoch 3172/10000, Training Loss: 1.6748230457305908, Validation Loss: 3.3869516849517822\n",
      "Epoch 3173/10000, Training Loss: 3.2568445205688477, Validation Loss: 3.712071418762207\n",
      "Epoch 3174/10000, Training Loss: 1.5647194385528564, Validation Loss: 2.1937880516052246\n",
      "Epoch 3175/10000, Training Loss: 2.888761043548584, Validation Loss: 3.2218849658966064\n",
      "Epoch 3176/10000, Training Loss: 2.853323459625244, Validation Loss: 6.496261119842529\n",
      "Epoch 3177/10000, Training Loss: 2.5809245109558105, Validation Loss: 2.800265073776245\n",
      "Epoch 3178/10000, Training Loss: 2.5606935024261475, Validation Loss: 4.065104007720947\n",
      "Epoch 3179/10000, Training Loss: 2.8227195739746094, Validation Loss: 3.1480062007904053\n",
      "Epoch 3180/10000, Training Loss: 1.9122366905212402, Validation Loss: 1.1937545537948608\n",
      "Epoch 3181/10000, Training Loss: 2.4503679275512695, Validation Loss: 3.1128616333007812\n",
      "Epoch 3182/10000, Training Loss: 1.672276258468628, Validation Loss: 1.8894896507263184\n",
      "Epoch 3183/10000, Training Loss: 1.5909113883972168, Validation Loss: 2.21767258644104\n",
      "Epoch 3184/10000, Training Loss: 1.621187686920166, Validation Loss: 3.958004951477051\n",
      "Epoch 3185/10000, Training Loss: 2.1200315952301025, Validation Loss: 5.038618087768555\n",
      "Epoch 3186/10000, Training Loss: 2.5524230003356934, Validation Loss: 1.3944450616836548\n",
      "Epoch 3187/10000, Training Loss: 1.9061423540115356, Validation Loss: 1.8177632093429565\n",
      "Epoch 3188/10000, Training Loss: 1.8175711631774902, Validation Loss: 3.1484193801879883\n",
      "Epoch 3189/10000, Training Loss: 1.629231333732605, Validation Loss: 2.7336742877960205\n",
      "Epoch 3190/10000, Training Loss: 2.5478978157043457, Validation Loss: 7.360769271850586\n",
      "Epoch 3191/10000, Training Loss: 3.7674953937530518, Validation Loss: 3.1456403732299805\n",
      "Epoch 3192/10000, Training Loss: 1.481170654296875, Validation Loss: 1.9582319259643555\n",
      "Epoch 3193/10000, Training Loss: 1.5485913753509521, Validation Loss: 3.2290942668914795\n",
      "Epoch 3194/10000, Training Loss: 1.6156601905822754, Validation Loss: 4.445324420928955\n",
      "Epoch 3195/10000, Training Loss: 2.378868579864502, Validation Loss: 1.769248604774475\n",
      "Epoch 3196/10000, Training Loss: 2.124647378921509, Validation Loss: 3.4980266094207764\n",
      "Epoch 3197/10000, Training Loss: 1.978540301322937, Validation Loss: 2.00659441947937\n",
      "Epoch 3198/10000, Training Loss: 1.9956294298171997, Validation Loss: 2.9731757640838623\n",
      "Epoch 3199/10000, Training Loss: 2.913360118865967, Validation Loss: 11.956530570983887\n",
      "Epoch 3200/10000, Training Loss: 1.830513834953308, Validation Loss: 3.3321926593780518\n",
      "Epoch 3201/10000, Training Loss: 2.214151382446289, Validation Loss: 3.294043779373169\n",
      "Epoch 3202/10000, Training Loss: 2.5819814205169678, Validation Loss: 4.700967311859131\n",
      "Epoch 3203/10000, Training Loss: 1.8072596788406372, Validation Loss: 3.1080472469329834\n",
      "Epoch 3204/10000, Training Loss: 1.8191660642623901, Validation Loss: 3.4474527835845947\n",
      "Epoch 3205/10000, Training Loss: 1.4395089149475098, Validation Loss: 1.1296061277389526\n",
      "Epoch 3206/10000, Training Loss: 2.0903828144073486, Validation Loss: 2.8598644733428955\n",
      "Epoch 3207/10000, Training Loss: 2.2024407386779785, Validation Loss: 1.7513335943222046\n",
      "Epoch 3208/10000, Training Loss: 2.7890963554382324, Validation Loss: 3.420395612716675\n",
      "Epoch 3209/10000, Training Loss: 2.610908269882202, Validation Loss: 3.762146234512329\n",
      "Epoch 3210/10000, Training Loss: 2.2655556201934814, Validation Loss: 2.1883506774902344\n",
      "Epoch 3211/10000, Training Loss: 1.686594009399414, Validation Loss: 2.9340648651123047\n",
      "Epoch 3212/10000, Training Loss: 2.1728272438049316, Validation Loss: 2.7776854038238525\n",
      "Epoch 3213/10000, Training Loss: 1.7576168775558472, Validation Loss: 1.4250025749206543\n",
      "Epoch 3214/10000, Training Loss: 2.20316743850708, Validation Loss: 5.421220779418945\n",
      "Epoch 3215/10000, Training Loss: 2.0006330013275146, Validation Loss: 2.198439359664917\n",
      "Epoch 3216/10000, Training Loss: 2.357240676879883, Validation Loss: 2.02901291847229\n",
      "Epoch 3217/10000, Training Loss: 2.0859265327453613, Validation Loss: 3.649653196334839\n",
      "Epoch 3218/10000, Training Loss: 2.3289308547973633, Validation Loss: 4.0153093338012695\n",
      "Epoch 3219/10000, Training Loss: 2.0976762771606445, Validation Loss: 2.0399863719940186\n",
      "Epoch 3220/10000, Training Loss: 2.5397462844848633, Validation Loss: 3.4215946197509766\n",
      "Epoch 3221/10000, Training Loss: 1.9964325428009033, Validation Loss: 4.033744812011719\n",
      "Epoch 3222/10000, Training Loss: 1.5800493955612183, Validation Loss: 1.6319953203201294\n",
      "Epoch 3223/10000, Training Loss: 1.775745153427124, Validation Loss: 0.8940861821174622\n",
      "Epoch 3224/10000, Training Loss: 2.399920701980591, Validation Loss: 3.3261301517486572\n",
      "Epoch 3225/10000, Training Loss: 2.4687910079956055, Validation Loss: 2.2686851024627686\n",
      "Epoch 3226/10000, Training Loss: 1.345980167388916, Validation Loss: 0.6888546943664551\n",
      "Epoch 3227/10000, Training Loss: 2.5660433769226074, Validation Loss: 5.544139862060547\n",
      "Epoch 3228/10000, Training Loss: 2.7219319343566895, Validation Loss: 3.984251022338867\n",
      "Epoch 3229/10000, Training Loss: 2.111445903778076, Validation Loss: 5.89286470413208\n",
      "Epoch 3230/10000, Training Loss: 1.462536096572876, Validation Loss: 2.2504265308380127\n",
      "Epoch 3231/10000, Training Loss: 1.9085564613342285, Validation Loss: 2.4512860774993896\n",
      "Epoch 3232/10000, Training Loss: 1.8817816972732544, Validation Loss: 2.0644869804382324\n",
      "Epoch 3233/10000, Training Loss: 1.876146912574768, Validation Loss: 4.025777339935303\n",
      "Epoch 3234/10000, Training Loss: 2.702059745788574, Validation Loss: 4.376983165740967\n",
      "Epoch 3235/10000, Training Loss: 1.6337076425552368, Validation Loss: 2.5689516067504883\n",
      "Epoch 3236/10000, Training Loss: 1.661828875541687, Validation Loss: 0.7883041501045227\n",
      "Epoch 3237/10000, Training Loss: 2.3309450149536133, Validation Loss: 2.408315420150757\n",
      "Epoch 3238/10000, Training Loss: 2.358353614807129, Validation Loss: 2.0615286827087402\n",
      "Epoch 3239/10000, Training Loss: 1.8926537036895752, Validation Loss: 1.831223487854004\n",
      "Epoch 3240/10000, Training Loss: 2.1759848594665527, Validation Loss: 0.9959185123443604\n",
      "Epoch 3241/10000, Training Loss: 1.8210489749908447, Validation Loss: 0.6404582858085632\n",
      "Epoch 3242/10000, Training Loss: 2.216625690460205, Validation Loss: 1.5392680168151855\n",
      "Epoch 3243/10000, Training Loss: 1.9403839111328125, Validation Loss: 1.4782825708389282\n",
      "Epoch 3244/10000, Training Loss: 2.4004006385803223, Validation Loss: 4.231325149536133\n",
      "Epoch 3245/10000, Training Loss: 2.043605089187622, Validation Loss: 3.446122169494629\n",
      "Epoch 3246/10000, Training Loss: 1.602782130241394, Validation Loss: 0.7827877998352051\n",
      "Epoch 3247/10000, Training Loss: 1.7405866384506226, Validation Loss: 1.4118269681930542\n",
      "Epoch 3248/10000, Training Loss: 2.4560534954071045, Validation Loss: 3.1707208156585693\n",
      "Epoch 3249/10000, Training Loss: 1.8036649227142334, Validation Loss: 1.94761323928833\n",
      "Epoch 3250/10000, Training Loss: 1.5196939706802368, Validation Loss: 1.013865351676941\n",
      "Epoch 3251/10000, Training Loss: 1.818027377128601, Validation Loss: 1.808489203453064\n",
      "Epoch 3252/10000, Training Loss: 2.428882360458374, Validation Loss: 3.084705114364624\n",
      "Epoch 3253/10000, Training Loss: 2.955568552017212, Validation Loss: 1.6533159017562866\n",
      "Epoch 3254/10000, Training Loss: 1.5528929233551025, Validation Loss: 2.3340721130371094\n",
      "Epoch 3255/10000, Training Loss: 2.4832959175109863, Validation Loss: 0.9422159194946289\n",
      "Epoch 3256/10000, Training Loss: 1.8481863737106323, Validation Loss: 3.461698293685913\n",
      "Epoch 3257/10000, Training Loss: 1.800352931022644, Validation Loss: 1.1033358573913574\n",
      "Epoch 3258/10000, Training Loss: 2.3879480361938477, Validation Loss: 2.9415814876556396\n",
      "Epoch 3259/10000, Training Loss: 2.3999037742614746, Validation Loss: 0.7354466915130615\n",
      "Epoch 3260/10000, Training Loss: 1.6674593687057495, Validation Loss: 1.6852284669876099\n",
      "Epoch 3261/10000, Training Loss: 1.8953092098236084, Validation Loss: 2.8062150478363037\n",
      "Epoch 3262/10000, Training Loss: 1.6213722229003906, Validation Loss: 2.9436123371124268\n",
      "Epoch 3263/10000, Training Loss: 2.0614712238311768, Validation Loss: 3.159470319747925\n",
      "Epoch 3264/10000, Training Loss: 2.4578540325164795, Validation Loss: 2.3797028064727783\n",
      "Epoch 3265/10000, Training Loss: 1.6744396686553955, Validation Loss: 1.4935803413391113\n",
      "Epoch 3266/10000, Training Loss: 2.2503325939178467, Validation Loss: 3.2773139476776123\n",
      "Epoch 3267/10000, Training Loss: 1.9960594177246094, Validation Loss: 3.5206117630004883\n",
      "Epoch 3268/10000, Training Loss: 2.4145524501800537, Validation Loss: 1.579241156578064\n",
      "Epoch 3269/10000, Training Loss: 2.6030919551849365, Validation Loss: 2.841484785079956\n",
      "Epoch 3270/10000, Training Loss: 2.015807628631592, Validation Loss: 2.7628238201141357\n",
      "Epoch 3271/10000, Training Loss: 1.7683333158493042, Validation Loss: 4.156546592712402\n",
      "Epoch 3272/10000, Training Loss: 1.781535267829895, Validation Loss: 3.037043333053589\n",
      "Epoch 3273/10000, Training Loss: 1.6023656129837036, Validation Loss: 1.5856915712356567\n",
      "Epoch 3274/10000, Training Loss: 2.354318618774414, Validation Loss: 4.072972774505615\n",
      "Epoch 3275/10000, Training Loss: 2.0136702060699463, Validation Loss: 1.846430778503418\n",
      "Epoch 3276/10000, Training Loss: 2.882436990737915, Validation Loss: 7.768764972686768\n",
      "Epoch 3277/10000, Training Loss: 1.80722177028656, Validation Loss: 2.735457420349121\n",
      "Epoch 3278/10000, Training Loss: 1.7381389141082764, Validation Loss: 0.8531045913696289\n",
      "Epoch 3279/10000, Training Loss: 1.830657958984375, Validation Loss: 1.3589798212051392\n",
      "Epoch 3280/10000, Training Loss: 2.613595485687256, Validation Loss: 2.3726446628570557\n",
      "Epoch 3281/10000, Training Loss: 1.8911452293395996, Validation Loss: 3.6296417713165283\n",
      "Epoch 3282/10000, Training Loss: 2.035444498062134, Validation Loss: 1.7215930223464966\n",
      "Epoch 3283/10000, Training Loss: 1.5789159536361694, Validation Loss: 1.070988416671753\n",
      "Epoch 3284/10000, Training Loss: 1.8156174421310425, Validation Loss: 3.8136472702026367\n",
      "Epoch 3285/10000, Training Loss: 1.306321382522583, Validation Loss: 1.7642402648925781\n",
      "Epoch 3286/10000, Training Loss: 1.6549283266067505, Validation Loss: 3.6845576763153076\n",
      "Epoch 3287/10000, Training Loss: 2.3785500526428223, Validation Loss: 1.8383961915969849\n",
      "Epoch 3288/10000, Training Loss: 1.8913315534591675, Validation Loss: 1.9486316442489624\n",
      "Epoch 3289/10000, Training Loss: 2.081505060195923, Validation Loss: 0.8698541522026062\n",
      "Epoch 3290/10000, Training Loss: 2.0149085521698, Validation Loss: 2.1547627449035645\n",
      "Epoch 3291/10000, Training Loss: 2.527118682861328, Validation Loss: 1.4086713790893555\n",
      "Epoch 3292/10000, Training Loss: 1.3707624673843384, Validation Loss: 1.6368433237075806\n",
      "Epoch 3293/10000, Training Loss: 1.7395504713058472, Validation Loss: 2.487200975418091\n",
      "Epoch 3294/10000, Training Loss: 2.4339659214019775, Validation Loss: 2.2214622497558594\n",
      "Epoch 3295/10000, Training Loss: 2.363692283630371, Validation Loss: 0.913020133972168\n",
      "Epoch 3296/10000, Training Loss: 2.689227819442749, Validation Loss: 3.7717835903167725\n",
      "Epoch 3297/10000, Training Loss: 2.6117799282073975, Validation Loss: 1.85293710231781\n",
      "Epoch 3298/10000, Training Loss: 1.7182621955871582, Validation Loss: 2.61228609085083\n",
      "Epoch 3299/10000, Training Loss: 3.1090879440307617, Validation Loss: 4.791732311248779\n",
      "Epoch 3300/10000, Training Loss: 1.609537124633789, Validation Loss: 2.158750534057617\n",
      "Epoch 3301/10000, Training Loss: 2.196357011795044, Validation Loss: 3.0974743366241455\n",
      "Epoch 3302/10000, Training Loss: 2.405912160873413, Validation Loss: 2.0935025215148926\n",
      "Epoch 3303/10000, Training Loss: 1.8745945692062378, Validation Loss: 2.9293110370635986\n",
      "Epoch 3304/10000, Training Loss: 2.1824400424957275, Validation Loss: 2.342374086380005\n",
      "Epoch 3305/10000, Training Loss: 1.9855456352233887, Validation Loss: 1.576018214225769\n",
      "Epoch 3306/10000, Training Loss: 2.003739356994629, Validation Loss: 1.3273051977157593\n",
      "Epoch 3307/10000, Training Loss: 1.7505595684051514, Validation Loss: 2.3457183837890625\n",
      "Epoch 3308/10000, Training Loss: 2.508962392807007, Validation Loss: 3.7329816818237305\n",
      "Epoch 3309/10000, Training Loss: 2.8250365257263184, Validation Loss: 1.2922775745391846\n",
      "Epoch 3310/10000, Training Loss: 2.1049485206604004, Validation Loss: 1.4462400674819946\n",
      "Epoch 3311/10000, Training Loss: 2.1336426734924316, Validation Loss: 2.5461840629577637\n",
      "Epoch 3312/10000, Training Loss: 1.6276925802230835, Validation Loss: 2.6160266399383545\n",
      "Epoch 3313/10000, Training Loss: 1.6169732809066772, Validation Loss: 1.840253233909607\n",
      "Epoch 3314/10000, Training Loss: 1.9796799421310425, Validation Loss: 4.033613681793213\n",
      "Epoch 3315/10000, Training Loss: 2.2808260917663574, Validation Loss: 1.7026729583740234\n",
      "Epoch 3316/10000, Training Loss: 1.7466034889221191, Validation Loss: 2.70121693611145\n",
      "Epoch 3317/10000, Training Loss: 2.002251386642456, Validation Loss: 3.0123231410980225\n",
      "Epoch 3318/10000, Training Loss: 2.122974157333374, Validation Loss: 2.074219226837158\n",
      "Epoch 3319/10000, Training Loss: 2.483942985534668, Validation Loss: 2.519197702407837\n",
      "Epoch 3320/10000, Training Loss: 2.280567169189453, Validation Loss: 1.6006550788879395\n",
      "Epoch 3321/10000, Training Loss: 2.0464699268341064, Validation Loss: 2.2879741191864014\n",
      "Epoch 3322/10000, Training Loss: 1.938728928565979, Validation Loss: 1.3924107551574707\n",
      "Epoch 3323/10000, Training Loss: 2.221123218536377, Validation Loss: 2.814875602722168\n",
      "Epoch 3324/10000, Training Loss: 2.4664297103881836, Validation Loss: 3.8705708980560303\n",
      "Epoch 3325/10000, Training Loss: 1.794572353363037, Validation Loss: 2.6648731231689453\n",
      "Epoch 3326/10000, Training Loss: 2.1743931770324707, Validation Loss: 1.645789623260498\n",
      "Epoch 3327/10000, Training Loss: 2.730255126953125, Validation Loss: 2.1833040714263916\n",
      "Epoch 3328/10000, Training Loss: 1.910971999168396, Validation Loss: 4.72788667678833\n",
      "Epoch 3329/10000, Training Loss: 2.084388256072998, Validation Loss: 1.4928975105285645\n",
      "Epoch 3330/10000, Training Loss: 1.5334993600845337, Validation Loss: 1.7555192708969116\n",
      "Epoch 3331/10000, Training Loss: 2.6213595867156982, Validation Loss: 7.165292263031006\n",
      "Epoch 3332/10000, Training Loss: 1.3066662549972534, Validation Loss: 1.1033028364181519\n",
      "Epoch 3333/10000, Training Loss: 2.0984549522399902, Validation Loss: 3.0955617427825928\n",
      "Epoch 3334/10000, Training Loss: 1.7547996044158936, Validation Loss: 2.094398260116577\n",
      "Epoch 3335/10000, Training Loss: 2.001803398132324, Validation Loss: 2.6589415073394775\n",
      "Epoch 3336/10000, Training Loss: 1.5944453477859497, Validation Loss: 1.452682375907898\n",
      "Epoch 3337/10000, Training Loss: 1.8831126689910889, Validation Loss: 3.2642993927001953\n",
      "Epoch 3338/10000, Training Loss: 1.9905864000320435, Validation Loss: 1.1916465759277344\n",
      "Epoch 3339/10000, Training Loss: 2.6644303798675537, Validation Loss: 2.413119077682495\n",
      "Epoch 3340/10000, Training Loss: 2.2252771854400635, Validation Loss: 1.0149885416030884\n",
      "Epoch 3341/10000, Training Loss: 1.7261494398117065, Validation Loss: 4.302661895751953\n",
      "Epoch 3342/10000, Training Loss: 1.5262404680252075, Validation Loss: 2.8009841442108154\n",
      "Epoch 3343/10000, Training Loss: 1.8287451267242432, Validation Loss: 1.5462093353271484\n",
      "Epoch 3344/10000, Training Loss: 1.5358757972717285, Validation Loss: 1.1468790769577026\n",
      "Epoch 3345/10000, Training Loss: 1.611593246459961, Validation Loss: 2.6735916137695312\n",
      "Epoch 3346/10000, Training Loss: 2.1250758171081543, Validation Loss: 1.5238536596298218\n",
      "Epoch 3347/10000, Training Loss: 1.7842662334442139, Validation Loss: 3.6195545196533203\n",
      "Epoch 3348/10000, Training Loss: 2.1890883445739746, Validation Loss: 4.877530574798584\n",
      "Epoch 3349/10000, Training Loss: 1.6517634391784668, Validation Loss: 3.5205671787261963\n",
      "Epoch 3350/10000, Training Loss: 1.7190594673156738, Validation Loss: 3.3494272232055664\n",
      "Epoch 3351/10000, Training Loss: 1.987446665763855, Validation Loss: 1.5103387832641602\n",
      "Epoch 3352/10000, Training Loss: 1.9081497192382812, Validation Loss: 3.822615623474121\n",
      "Epoch 3353/10000, Training Loss: 2.108381986618042, Validation Loss: 5.54912805557251\n",
      "Epoch 3354/10000, Training Loss: 2.698190450668335, Validation Loss: 3.1398239135742188\n",
      "Epoch 3355/10000, Training Loss: 1.56959867477417, Validation Loss: 3.030228853225708\n",
      "Epoch 3356/10000, Training Loss: 1.6388276815414429, Validation Loss: 1.6756162643432617\n",
      "Epoch 3357/10000, Training Loss: 2.480250358581543, Validation Loss: 4.222006797790527\n",
      "Epoch 3358/10000, Training Loss: 1.7065263986587524, Validation Loss: 3.39380145072937\n",
      "Epoch 3359/10000, Training Loss: 1.7510782480239868, Validation Loss: 0.9734718203544617\n",
      "Epoch 3360/10000, Training Loss: 1.8421096801757812, Validation Loss: 1.2507998943328857\n",
      "Epoch 3361/10000, Training Loss: 1.8446106910705566, Validation Loss: 2.2683868408203125\n",
      "Epoch 3362/10000, Training Loss: 1.859573245048523, Validation Loss: 2.55888295173645\n",
      "Epoch 3363/10000, Training Loss: 1.7983806133270264, Validation Loss: 1.866139531135559\n",
      "Epoch 3364/10000, Training Loss: 1.333723783493042, Validation Loss: 2.310865640640259\n",
      "Epoch 3365/10000, Training Loss: 1.7502861022949219, Validation Loss: 4.178705215454102\n",
      "Epoch 3366/10000, Training Loss: 1.5403512716293335, Validation Loss: 2.6276814937591553\n",
      "Epoch 3367/10000, Training Loss: 1.7970980405807495, Validation Loss: 1.373106598854065\n",
      "Epoch 3368/10000, Training Loss: 1.9729087352752686, Validation Loss: 1.9296530485153198\n",
      "Epoch 3369/10000, Training Loss: 1.1401422023773193, Validation Loss: 2.248642921447754\n",
      "Epoch 3370/10000, Training Loss: 1.3897558450698853, Validation Loss: 2.2641055583953857\n",
      "Epoch 3371/10000, Training Loss: 1.5792186260223389, Validation Loss: 1.2434576749801636\n",
      "Epoch 3372/10000, Training Loss: 1.8749582767486572, Validation Loss: 4.632404327392578\n",
      "Epoch 3373/10000, Training Loss: 2.506106376647949, Validation Loss: 4.873020172119141\n",
      "Epoch 3374/10000, Training Loss: 1.884657382965088, Validation Loss: 2.6804580688476562\n",
      "Epoch 3375/10000, Training Loss: 2.1742844581604004, Validation Loss: 1.569992184638977\n",
      "Epoch 3376/10000, Training Loss: 2.708670139312744, Validation Loss: 1.3625226020812988\n",
      "Epoch 3377/10000, Training Loss: 1.6434519290924072, Validation Loss: 1.9896916151046753\n",
      "Epoch 3378/10000, Training Loss: 1.799216389656067, Validation Loss: 5.036613941192627\n",
      "Epoch 3379/10000, Training Loss: 2.064692497253418, Validation Loss: 2.6661417484283447\n",
      "Epoch 3380/10000, Training Loss: 1.8571733236312866, Validation Loss: 2.3332817554473877\n",
      "Epoch 3381/10000, Training Loss: 1.8821909427642822, Validation Loss: 3.009782075881958\n",
      "Epoch 3382/10000, Training Loss: 2.1617634296417236, Validation Loss: 2.3722097873687744\n",
      "Epoch 3383/10000, Training Loss: 2.0228142738342285, Validation Loss: 2.09948992729187\n",
      "Epoch 3384/10000, Training Loss: 1.614703893661499, Validation Loss: 1.8554911613464355\n",
      "Epoch 3385/10000, Training Loss: 2.4143850803375244, Validation Loss: 1.6052188873291016\n",
      "Epoch 3386/10000, Training Loss: 1.5523715019226074, Validation Loss: 0.7144388556480408\n",
      "Epoch 3387/10000, Training Loss: 1.4161086082458496, Validation Loss: 0.30672159790992737\n",
      "Epoch 3388/10000, Training Loss: 1.8773531913757324, Validation Loss: 4.015544891357422\n",
      "Epoch 3389/10000, Training Loss: 1.4269343614578247, Validation Loss: 2.190155506134033\n",
      "Epoch 3390/10000, Training Loss: 2.322410821914673, Validation Loss: 6.391752243041992\n",
      "Epoch 3391/10000, Training Loss: 1.6930255889892578, Validation Loss: 3.3183631896972656\n",
      "Epoch 3392/10000, Training Loss: 2.723886728286743, Validation Loss: 2.9548370838165283\n",
      "Epoch 3393/10000, Training Loss: 2.301652193069458, Validation Loss: 3.5522215366363525\n",
      "Epoch 3394/10000, Training Loss: 2.4366955757141113, Validation Loss: 0.8896796107292175\n",
      "Epoch 3395/10000, Training Loss: 1.6724902391433716, Validation Loss: 2.300297498703003\n",
      "Epoch 3396/10000, Training Loss: 2.313664674758911, Validation Loss: 4.404252529144287\n",
      "Epoch 3397/10000, Training Loss: 1.7649192810058594, Validation Loss: 1.9958175420761108\n",
      "Epoch 3398/10000, Training Loss: 1.3485395908355713, Validation Loss: 1.9908593893051147\n",
      "Epoch 3399/10000, Training Loss: 1.7553998231887817, Validation Loss: 2.890291929244995\n",
      "Epoch 3400/10000, Training Loss: 1.7227742671966553, Validation Loss: 2.420473337173462\n",
      "Epoch 3401/10000, Training Loss: 1.4330073595046997, Validation Loss: 2.2414348125457764\n",
      "Epoch 3402/10000, Training Loss: 1.944130539894104, Validation Loss: 3.953934669494629\n",
      "Epoch 3403/10000, Training Loss: 1.4537014961242676, Validation Loss: 1.8579391241073608\n",
      "Epoch 3404/10000, Training Loss: 1.3103162050247192, Validation Loss: 1.4830933809280396\n",
      "Epoch 3405/10000, Training Loss: 2.039523124694824, Validation Loss: 4.0532307624816895\n",
      "Epoch 3406/10000, Training Loss: 2.3312559127807617, Validation Loss: 4.418118476867676\n",
      "Epoch 3407/10000, Training Loss: 1.9316856861114502, Validation Loss: 1.4542850255966187\n",
      "Epoch 3408/10000, Training Loss: 1.51461923122406, Validation Loss: 1.4918049573898315\n",
      "Epoch 3409/10000, Training Loss: 1.8572514057159424, Validation Loss: 2.5840072631835938\n",
      "Epoch 3410/10000, Training Loss: 1.2124853134155273, Validation Loss: 2.637343645095825\n",
      "Epoch 3411/10000, Training Loss: 2.108335018157959, Validation Loss: 1.4268184900283813\n",
      "Epoch 3412/10000, Training Loss: 2.064274311065674, Validation Loss: 3.561253547668457\n",
      "Epoch 3413/10000, Training Loss: 1.9195741415023804, Validation Loss: 3.4641714096069336\n",
      "Epoch 3414/10000, Training Loss: 1.4424331188201904, Validation Loss: 2.6128010749816895\n",
      "Epoch 3415/10000, Training Loss: 1.256555438041687, Validation Loss: 1.1626044511795044\n",
      "Epoch 3416/10000, Training Loss: 1.9793510437011719, Validation Loss: 1.050111174583435\n",
      "Epoch 3417/10000, Training Loss: 1.7440119981765747, Validation Loss: 3.21855092048645\n",
      "Epoch 3418/10000, Training Loss: 2.3684091567993164, Validation Loss: 1.155077338218689\n",
      "Epoch 3419/10000, Training Loss: 2.0087077617645264, Validation Loss: 1.9205913543701172\n",
      "Epoch 3420/10000, Training Loss: 1.5127450227737427, Validation Loss: 1.1013418436050415\n",
      "Epoch 3421/10000, Training Loss: 1.6639869213104248, Validation Loss: 2.775472402572632\n",
      "Epoch 3422/10000, Training Loss: 1.4428743124008179, Validation Loss: 1.5661007165908813\n",
      "Epoch 3423/10000, Training Loss: 1.5340688228607178, Validation Loss: 1.4212497472763062\n",
      "Epoch 3424/10000, Training Loss: 1.6845958232879639, Validation Loss: 0.9894011616706848\n",
      "Epoch 3425/10000, Training Loss: 2.059512138366699, Validation Loss: 4.4466352462768555\n",
      "Epoch 3426/10000, Training Loss: 1.7731276750564575, Validation Loss: 2.834998369216919\n",
      "Epoch 3427/10000, Training Loss: 2.0173144340515137, Validation Loss: 1.9815120697021484\n",
      "Epoch 3428/10000, Training Loss: 2.421630859375, Validation Loss: 3.3343937397003174\n",
      "Epoch 3429/10000, Training Loss: 1.9410666227340698, Validation Loss: 0.8835368156433105\n",
      "Epoch 3430/10000, Training Loss: 1.8089566230773926, Validation Loss: 2.923995018005371\n",
      "Epoch 3431/10000, Training Loss: 1.6185498237609863, Validation Loss: 0.7732554078102112\n",
      "Epoch 3432/10000, Training Loss: 2.218752861022949, Validation Loss: 5.132421493530273\n",
      "Epoch 3433/10000, Training Loss: 2.490182638168335, Validation Loss: 3.5084705352783203\n",
      "Epoch 3434/10000, Training Loss: 1.817805528640747, Validation Loss: 1.3295623064041138\n",
      "Epoch 3435/10000, Training Loss: 2.375516176223755, Validation Loss: 6.221402645111084\n",
      "Epoch 3436/10000, Training Loss: 1.250739336013794, Validation Loss: 2.375020742416382\n",
      "Epoch 3437/10000, Training Loss: 2.38177490234375, Validation Loss: 4.1114583015441895\n",
      "Epoch 3438/10000, Training Loss: 2.0102627277374268, Validation Loss: 1.320420265197754\n",
      "Epoch 3439/10000, Training Loss: 0.9664242267608643, Validation Loss: 0.7155110239982605\n",
      "Epoch 3440/10000, Training Loss: 1.8057183027267456, Validation Loss: 3.1798369884490967\n",
      "Epoch 3441/10000, Training Loss: 2.922630548477173, Validation Loss: 5.683019161224365\n",
      "Epoch 3442/10000, Training Loss: 1.5794081687927246, Validation Loss: 0.24777847528457642\n",
      "Epoch 3443/10000, Training Loss: 1.270627498626709, Validation Loss: 0.7640653252601624\n",
      "Epoch 3444/10000, Training Loss: 1.6021596193313599, Validation Loss: 0.7516505718231201\n",
      "Epoch 3445/10000, Training Loss: 1.433381199836731, Validation Loss: 1.4375284910202026\n",
      "Epoch 3446/10000, Training Loss: 1.4831063747406006, Validation Loss: 1.33338463306427\n",
      "Epoch 3447/10000, Training Loss: 2.0075066089630127, Validation Loss: 3.0393495559692383\n",
      "Epoch 3448/10000, Training Loss: 2.2005372047424316, Validation Loss: 1.3161568641662598\n",
      "Epoch 3449/10000, Training Loss: 1.7259916067123413, Validation Loss: 5.571737289428711\n",
      "Epoch 3450/10000, Training Loss: 2.2685720920562744, Validation Loss: 3.0573179721832275\n",
      "Epoch 3451/10000, Training Loss: 2.39174485206604, Validation Loss: 3.307844877243042\n",
      "Epoch 3452/10000, Training Loss: 2.0033726692199707, Validation Loss: 1.3615363836288452\n",
      "Epoch 3453/10000, Training Loss: 1.7889659404754639, Validation Loss: 1.8470864295959473\n",
      "Epoch 3454/10000, Training Loss: 1.4493365287780762, Validation Loss: 2.0474605560302734\n",
      "Epoch 3455/10000, Training Loss: 1.6573752164840698, Validation Loss: 1.339424967765808\n",
      "Epoch 3456/10000, Training Loss: 1.2510310411453247, Validation Loss: 1.2230135202407837\n",
      "Epoch 3457/10000, Training Loss: 1.7016932964324951, Validation Loss: 1.4128938913345337\n",
      "Epoch 3458/10000, Training Loss: 1.8071620464324951, Validation Loss: 1.3268176317214966\n",
      "Epoch 3459/10000, Training Loss: 1.7443530559539795, Validation Loss: 3.1011171340942383\n",
      "Epoch 3460/10000, Training Loss: 2.2691798210144043, Validation Loss: 3.8580610752105713\n",
      "Epoch 3461/10000, Training Loss: 1.5474307537078857, Validation Loss: 1.7226444482803345\n",
      "Epoch 3462/10000, Training Loss: 2.0455517768859863, Validation Loss: 3.313339948654175\n",
      "Epoch 3463/10000, Training Loss: 1.3572485446929932, Validation Loss: 1.1243865489959717\n",
      "Epoch 3464/10000, Training Loss: 1.8562273979187012, Validation Loss: 3.71269154548645\n",
      "Epoch 3465/10000, Training Loss: 1.7788256406784058, Validation Loss: 2.1320416927337646\n",
      "Epoch 3466/10000, Training Loss: 1.7802162170410156, Validation Loss: 2.04685640335083\n",
      "Epoch 3467/10000, Training Loss: 1.439504861831665, Validation Loss: 2.2921254634857178\n",
      "Epoch 3468/10000, Training Loss: 1.8159314393997192, Validation Loss: 1.998491883277893\n",
      "Epoch 3469/10000, Training Loss: 1.6350104808807373, Validation Loss: 1.8000526428222656\n",
      "Epoch 3470/10000, Training Loss: 1.5243780612945557, Validation Loss: 1.1202317476272583\n",
      "Epoch 3471/10000, Training Loss: 1.4787166118621826, Validation Loss: 1.5522111654281616\n",
      "Epoch 3472/10000, Training Loss: 1.251068115234375, Validation Loss: 1.1819679737091064\n",
      "Epoch 3473/10000, Training Loss: 3.446305513381958, Validation Loss: 4.614299774169922\n",
      "Epoch 3474/10000, Training Loss: 1.828185796737671, Validation Loss: 3.488368034362793\n",
      "Epoch 3475/10000, Training Loss: 1.658558964729309, Validation Loss: 1.7186473608016968\n",
      "Epoch 3476/10000, Training Loss: 1.5686509609222412, Validation Loss: 2.4248175621032715\n",
      "Epoch 3477/10000, Training Loss: 2.2195611000061035, Validation Loss: 1.3858124017715454\n",
      "Epoch 3478/10000, Training Loss: 1.7187143564224243, Validation Loss: 2.423426628112793\n",
      "Epoch 3479/10000, Training Loss: 2.1189956665039062, Validation Loss: 2.895077705383301\n",
      "Epoch 3480/10000, Training Loss: 1.4608733654022217, Validation Loss: 1.3772540092468262\n",
      "Epoch 3481/10000, Training Loss: 1.5210473537445068, Validation Loss: 1.990099549293518\n",
      "Epoch 3482/10000, Training Loss: 1.5791988372802734, Validation Loss: 1.762175440788269\n",
      "Epoch 3483/10000, Training Loss: 1.7236131429672241, Validation Loss: 1.3920539617538452\n",
      "Epoch 3484/10000, Training Loss: 2.815182685852051, Validation Loss: 5.497498035430908\n",
      "Epoch 3485/10000, Training Loss: 1.4055001735687256, Validation Loss: 1.8094934225082397\n",
      "Epoch 3486/10000, Training Loss: 1.5293047428131104, Validation Loss: 2.664567232131958\n",
      "Epoch 3487/10000, Training Loss: 1.7236219644546509, Validation Loss: 1.6107478141784668\n",
      "Epoch 3488/10000, Training Loss: 1.96896231174469, Validation Loss: 3.0127174854278564\n",
      "Epoch 3489/10000, Training Loss: 1.5944018363952637, Validation Loss: 2.530480146408081\n",
      "Epoch 3490/10000, Training Loss: 1.7267221212387085, Validation Loss: 1.9806116819381714\n",
      "Epoch 3491/10000, Training Loss: 1.979595422744751, Validation Loss: 1.7718364000320435\n",
      "Epoch 3492/10000, Training Loss: 2.0589795112609863, Validation Loss: 1.3232406377792358\n",
      "Epoch 3493/10000, Training Loss: 1.5662972927093506, Validation Loss: 3.5343005657196045\n",
      "Epoch 3494/10000, Training Loss: 1.8167212009429932, Validation Loss: 4.598904609680176\n",
      "Epoch 3495/10000, Training Loss: 1.8949854373931885, Validation Loss: 3.889601469039917\n",
      "Epoch 3496/10000, Training Loss: 2.041912317276001, Validation Loss: 1.3639284372329712\n",
      "Epoch 3497/10000, Training Loss: 1.6078951358795166, Validation Loss: 2.122875928878784\n",
      "Epoch 3498/10000, Training Loss: 1.8055928945541382, Validation Loss: 2.0259904861450195\n",
      "Epoch 3499/10000, Training Loss: 2.0103020668029785, Validation Loss: 2.449336051940918\n",
      "Epoch 3500/10000, Training Loss: 1.8895035982131958, Validation Loss: 2.8992364406585693\n",
      "Epoch 3501/10000, Training Loss: 2.163388729095459, Validation Loss: 2.2862308025360107\n",
      "Epoch 3502/10000, Training Loss: 1.4704419374465942, Validation Loss: 4.0553059577941895\n",
      "Epoch 3503/10000, Training Loss: 2.07206130027771, Validation Loss: 2.0619471073150635\n",
      "Epoch 3504/10000, Training Loss: 1.7029988765716553, Validation Loss: 3.6623315811157227\n",
      "Epoch 3505/10000, Training Loss: 1.5433828830718994, Validation Loss: 2.1870622634887695\n",
      "Epoch 3506/10000, Training Loss: 1.6104316711425781, Validation Loss: 1.756657600402832\n",
      "Epoch 3507/10000, Training Loss: 1.793790578842163, Validation Loss: 2.5286803245544434\n",
      "Epoch 3508/10000, Training Loss: 1.7145682573318481, Validation Loss: 1.5848265886306763\n",
      "Epoch 3509/10000, Training Loss: 1.8193128108978271, Validation Loss: 3.4971139430999756\n",
      "Epoch 3510/10000, Training Loss: 1.9149060249328613, Validation Loss: 0.8620259165763855\n",
      "Epoch 3511/10000, Training Loss: 1.5033318996429443, Validation Loss: 2.1544153690338135\n",
      "Epoch 3512/10000, Training Loss: 1.4449330568313599, Validation Loss: 1.8752840757369995\n",
      "Epoch 3513/10000, Training Loss: 1.6944105625152588, Validation Loss: 1.4632567167282104\n",
      "Epoch 3514/10000, Training Loss: 1.4912850856781006, Validation Loss: 3.0183897018432617\n",
      "Epoch 3515/10000, Training Loss: 1.6821913719177246, Validation Loss: 1.607473373413086\n",
      "Epoch 3516/10000, Training Loss: 1.7478886842727661, Validation Loss: 3.076029062271118\n",
      "Epoch 3517/10000, Training Loss: 1.327239990234375, Validation Loss: 1.5712915658950806\n",
      "Epoch 3518/10000, Training Loss: 1.2527506351470947, Validation Loss: 1.261931300163269\n",
      "Epoch 3519/10000, Training Loss: 1.5134536027908325, Validation Loss: 2.0134899616241455\n",
      "Epoch 3520/10000, Training Loss: 2.0228185653686523, Validation Loss: 2.6532070636749268\n",
      "Epoch 3521/10000, Training Loss: 2.103771209716797, Validation Loss: 2.9265434741973877\n",
      "Epoch 3522/10000, Training Loss: 1.5323213338851929, Validation Loss: 2.118359088897705\n",
      "Epoch 3523/10000, Training Loss: 2.4159469604492188, Validation Loss: 3.0414164066314697\n",
      "Epoch 3524/10000, Training Loss: 1.6205552816390991, Validation Loss: 0.846611738204956\n",
      "Epoch 3525/10000, Training Loss: 1.5383706092834473, Validation Loss: 3.1192550659179688\n",
      "Epoch 3526/10000, Training Loss: 1.7167657613754272, Validation Loss: 2.407240152359009\n",
      "Epoch 3527/10000, Training Loss: 2.0329360961914062, Validation Loss: 3.593008041381836\n",
      "Epoch 3528/10000, Training Loss: 1.8837839365005493, Validation Loss: 2.198146343231201\n",
      "Epoch 3529/10000, Training Loss: 1.9697799682617188, Validation Loss: 5.244743824005127\n",
      "Epoch 3530/10000, Training Loss: 1.7952752113342285, Validation Loss: 1.0769954919815063\n",
      "Epoch 3531/10000, Training Loss: 1.1347922086715698, Validation Loss: 1.192988395690918\n",
      "Epoch 3532/10000, Training Loss: 2.0484464168548584, Validation Loss: 2.9129180908203125\n",
      "Epoch 3533/10000, Training Loss: 1.8083446025848389, Validation Loss: 2.287170171737671\n",
      "Epoch 3534/10000, Training Loss: 1.9333451986312866, Validation Loss: 2.1160101890563965\n",
      "Epoch 3535/10000, Training Loss: 1.9218827486038208, Validation Loss: 2.961951971054077\n",
      "Epoch 3536/10000, Training Loss: 1.5935566425323486, Validation Loss: 1.4594882726669312\n",
      "Epoch 3537/10000, Training Loss: 1.9833232164382935, Validation Loss: 2.696667432785034\n",
      "Epoch 3538/10000, Training Loss: 1.2821807861328125, Validation Loss: 0.513193666934967\n",
      "Epoch 3539/10000, Training Loss: 1.668666958808899, Validation Loss: 3.1152307987213135\n",
      "Epoch 3540/10000, Training Loss: 1.7669997215270996, Validation Loss: 2.652472496032715\n",
      "Epoch 3541/10000, Training Loss: 1.977500081062317, Validation Loss: 2.6085727214813232\n",
      "Epoch 3542/10000, Training Loss: 1.8600903749465942, Validation Loss: 0.9869082570075989\n",
      "Epoch 3543/10000, Training Loss: 1.3760355710983276, Validation Loss: 1.0559121370315552\n",
      "Epoch 3544/10000, Training Loss: 1.9301167726516724, Validation Loss: 0.9734281897544861\n",
      "Epoch 3545/10000, Training Loss: 1.6857457160949707, Validation Loss: 1.7931040525436401\n",
      "Epoch 3546/10000, Training Loss: 1.704785704612732, Validation Loss: 3.4869182109832764\n",
      "Epoch 3547/10000, Training Loss: 1.453531265258789, Validation Loss: 2.4554338455200195\n",
      "Epoch 3548/10000, Training Loss: 1.2709187269210815, Validation Loss: 1.6415047645568848\n",
      "Epoch 3549/10000, Training Loss: 1.887312650680542, Validation Loss: 2.361541986465454\n",
      "Epoch 3550/10000, Training Loss: 1.976744294166565, Validation Loss: 1.2126824855804443\n",
      "Epoch 3551/10000, Training Loss: 1.389143466949463, Validation Loss: 1.8953913450241089\n",
      "Epoch 3552/10000, Training Loss: 1.7306145429611206, Validation Loss: 2.425936222076416\n",
      "Epoch 3553/10000, Training Loss: 1.8962552547454834, Validation Loss: 3.46639084815979\n",
      "Epoch 3554/10000, Training Loss: 1.7984284162521362, Validation Loss: 3.190620183944702\n",
      "Epoch 3555/10000, Training Loss: 1.6112685203552246, Validation Loss: 2.5709145069122314\n",
      "Epoch 3556/10000, Training Loss: 1.6973838806152344, Validation Loss: 1.594950556755066\n",
      "Epoch 3557/10000, Training Loss: 1.5588304996490479, Validation Loss: 1.3889204263687134\n",
      "Epoch 3558/10000, Training Loss: 1.5190707445144653, Validation Loss: 2.1297366619110107\n",
      "Epoch 3559/10000, Training Loss: 1.196519136428833, Validation Loss: 1.6562355756759644\n",
      "Epoch 3560/10000, Training Loss: 2.431004524230957, Validation Loss: 1.2325353622436523\n",
      "Epoch 3561/10000, Training Loss: 2.393514633178711, Validation Loss: 1.9045677185058594\n",
      "Epoch 3562/10000, Training Loss: 1.4066455364227295, Validation Loss: 1.2976528406143188\n",
      "Epoch 3563/10000, Training Loss: 1.9533025026321411, Validation Loss: 1.9015790224075317\n",
      "Epoch 3564/10000, Training Loss: 1.5691912174224854, Validation Loss: 1.9531100988388062\n",
      "Epoch 3565/10000, Training Loss: 1.693135142326355, Validation Loss: 1.4375019073486328\n",
      "Epoch 3566/10000, Training Loss: 2.326206922531128, Validation Loss: 4.211795330047607\n",
      "Epoch 3567/10000, Training Loss: 1.564415454864502, Validation Loss: 2.70711350440979\n",
      "Epoch 3568/10000, Training Loss: 1.7555358409881592, Validation Loss: 2.2200610637664795\n",
      "Epoch 3569/10000, Training Loss: 1.7440434694290161, Validation Loss: 2.218488931655884\n",
      "Epoch 3570/10000, Training Loss: 1.3224157094955444, Validation Loss: 3.810002565383911\n",
      "Epoch 3571/10000, Training Loss: 1.6237748861312866, Validation Loss: 1.2637872695922852\n",
      "Epoch 3572/10000, Training Loss: 1.7188761234283447, Validation Loss: 1.3464921712875366\n",
      "Epoch 3573/10000, Training Loss: 1.9385738372802734, Validation Loss: 3.8622653484344482\n",
      "Epoch 3574/10000, Training Loss: 1.6308218240737915, Validation Loss: 1.950107455253601\n",
      "Epoch 3575/10000, Training Loss: 1.6535694599151611, Validation Loss: 2.428623676300049\n",
      "Epoch 3576/10000, Training Loss: 1.7825396060943604, Validation Loss: 3.1456925868988037\n",
      "Epoch 3577/10000, Training Loss: 2.0649359226226807, Validation Loss: 2.9415295124053955\n",
      "Epoch 3578/10000, Training Loss: 1.5917550325393677, Validation Loss: 1.9835084676742554\n",
      "Epoch 3579/10000, Training Loss: 1.5156174898147583, Validation Loss: 1.1819835901260376\n",
      "Epoch 3580/10000, Training Loss: 1.794259786605835, Validation Loss: 3.4041316509246826\n",
      "Epoch 3581/10000, Training Loss: 1.411077618598938, Validation Loss: 1.1235076189041138\n",
      "Epoch 3582/10000, Training Loss: 1.4565999507904053, Validation Loss: 0.6916890144348145\n",
      "Epoch 3583/10000, Training Loss: 1.697383165359497, Validation Loss: 3.3230934143066406\n",
      "Epoch 3584/10000, Training Loss: 2.1120798587799072, Validation Loss: 4.848835468292236\n",
      "Epoch 3585/10000, Training Loss: 1.5755810737609863, Validation Loss: 3.3684451580047607\n",
      "Epoch 3586/10000, Training Loss: 1.4176716804504395, Validation Loss: 2.924057722091675\n",
      "Epoch 3587/10000, Training Loss: 1.216821551322937, Validation Loss: 1.6607764959335327\n",
      "Epoch 3588/10000, Training Loss: 1.467830777168274, Validation Loss: 2.06296706199646\n",
      "Epoch 3589/10000, Training Loss: 1.642540693283081, Validation Loss: 1.3904372453689575\n",
      "Epoch 3590/10000, Training Loss: 1.954828143119812, Validation Loss: 1.6871341466903687\n",
      "Epoch 3591/10000, Training Loss: 1.4573962688446045, Validation Loss: 4.942507743835449\n",
      "Epoch 3592/10000, Training Loss: 1.625069260597229, Validation Loss: 2.5329089164733887\n",
      "Epoch 3593/10000, Training Loss: 1.5948824882507324, Validation Loss: 2.831470251083374\n",
      "Epoch 3594/10000, Training Loss: 1.6271309852600098, Validation Loss: 2.7992029190063477\n",
      "Epoch 3595/10000, Training Loss: 1.229804277420044, Validation Loss: 1.4288997650146484\n",
      "Epoch 3596/10000, Training Loss: 1.8710767030715942, Validation Loss: 1.1989398002624512\n",
      "Epoch 3597/10000, Training Loss: 1.3798127174377441, Validation Loss: 1.272053837776184\n",
      "Epoch 3598/10000, Training Loss: 1.1198644638061523, Validation Loss: 0.9941449761390686\n",
      "Epoch 3599/10000, Training Loss: 1.2605112791061401, Validation Loss: 1.1361535787582397\n",
      "Epoch 3600/10000, Training Loss: 1.4383989572525024, Validation Loss: 0.897468090057373\n",
      "Epoch 3601/10000, Training Loss: 1.4439233541488647, Validation Loss: 1.4955925941467285\n",
      "Epoch 3602/10000, Training Loss: 1.9709117412567139, Validation Loss: 2.4585254192352295\n",
      "Epoch 3603/10000, Training Loss: 1.726335048675537, Validation Loss: 1.174953818321228\n",
      "Epoch 3604/10000, Training Loss: 1.5837498903274536, Validation Loss: 1.4166244268417358\n",
      "Epoch 3605/10000, Training Loss: 1.1845396757125854, Validation Loss: 0.6617291569709778\n",
      "Epoch 3606/10000, Training Loss: 1.7653062343597412, Validation Loss: 1.881484031677246\n",
      "Epoch 3607/10000, Training Loss: 1.8886120319366455, Validation Loss: 6.209005832672119\n",
      "Epoch 3608/10000, Training Loss: 2.1217169761657715, Validation Loss: 4.92754602432251\n",
      "Epoch 3609/10000, Training Loss: 1.766562819480896, Validation Loss: 2.819399833679199\n",
      "Epoch 3610/10000, Training Loss: 1.9447306394577026, Validation Loss: 0.9884983897209167\n",
      "Epoch 3611/10000, Training Loss: 1.5149792432785034, Validation Loss: 2.07458758354187\n",
      "Epoch 3612/10000, Training Loss: 1.864504337310791, Validation Loss: 1.5408474206924438\n",
      "Epoch 3613/10000, Training Loss: 1.3534963130950928, Validation Loss: 2.234689712524414\n",
      "Epoch 3614/10000, Training Loss: 1.5677777528762817, Validation Loss: 3.480487823486328\n",
      "Epoch 3615/10000, Training Loss: 1.222801685333252, Validation Loss: 1.643826961517334\n",
      "Epoch 3616/10000, Training Loss: 1.3498750925064087, Validation Loss: 0.5115814805030823\n",
      "Epoch 3617/10000, Training Loss: 1.635277509689331, Validation Loss: 1.6529273986816406\n",
      "Epoch 3618/10000, Training Loss: 1.4468700885772705, Validation Loss: 1.6118770837783813\n",
      "Epoch 3619/10000, Training Loss: 2.0465798377990723, Validation Loss: 1.4544481039047241\n",
      "Epoch 3620/10000, Training Loss: 1.3009247779846191, Validation Loss: 1.2676786184310913\n",
      "Epoch 3621/10000, Training Loss: 1.544563889503479, Validation Loss: 1.8441108465194702\n",
      "Epoch 3622/10000, Training Loss: 1.8553550243377686, Validation Loss: 3.005033254623413\n",
      "Epoch 3623/10000, Training Loss: 1.3702290058135986, Validation Loss: 1.086271047592163\n",
      "Epoch 3624/10000, Training Loss: 1.2898972034454346, Validation Loss: 1.8868799209594727\n",
      "Epoch 3625/10000, Training Loss: 1.5672811269760132, Validation Loss: 0.8361017107963562\n",
      "Epoch 3626/10000, Training Loss: 1.9134668111801147, Validation Loss: 1.2203079462051392\n",
      "Epoch 3627/10000, Training Loss: 1.4020048379898071, Validation Loss: 1.3729816675186157\n",
      "Epoch 3628/10000, Training Loss: 1.3884550333023071, Validation Loss: 1.959903359413147\n",
      "Epoch 3629/10000, Training Loss: 1.7715483903884888, Validation Loss: 2.5423951148986816\n",
      "Epoch 3630/10000, Training Loss: 1.251396656036377, Validation Loss: 2.1444199085235596\n",
      "Epoch 3631/10000, Training Loss: 2.4716742038726807, Validation Loss: 0.9857695698738098\n",
      "Epoch 3632/10000, Training Loss: 1.5471019744873047, Validation Loss: 3.066366195678711\n",
      "Epoch 3633/10000, Training Loss: 1.6154109239578247, Validation Loss: 3.7168474197387695\n",
      "Epoch 3634/10000, Training Loss: 1.264867901802063, Validation Loss: 1.095923900604248\n",
      "Epoch 3635/10000, Training Loss: 1.530600666999817, Validation Loss: 2.6079776287078857\n",
      "Epoch 3636/10000, Training Loss: 1.6958460807800293, Validation Loss: 2.1855480670928955\n",
      "Epoch 3637/10000, Training Loss: 1.43577241897583, Validation Loss: 3.0652875900268555\n",
      "Epoch 3638/10000, Training Loss: 1.3832472562789917, Validation Loss: 0.7655184864997864\n",
      "Epoch 3639/10000, Training Loss: 1.4940247535705566, Validation Loss: 4.31466817855835\n",
      "Epoch 3640/10000, Training Loss: 1.376307725906372, Validation Loss: 1.8414198160171509\n",
      "Epoch 3641/10000, Training Loss: 1.3475632667541504, Validation Loss: 1.757400631904602\n",
      "Epoch 3642/10000, Training Loss: 1.2922242879867554, Validation Loss: 1.9190250635147095\n",
      "Epoch 3643/10000, Training Loss: 1.1736584901809692, Validation Loss: 1.0790263414382935\n",
      "Epoch 3644/10000, Training Loss: 1.6889610290527344, Validation Loss: 2.2093191146850586\n",
      "Epoch 3645/10000, Training Loss: 1.518404245376587, Validation Loss: 2.0138065814971924\n",
      "Epoch 3646/10000, Training Loss: 1.8067569732666016, Validation Loss: 1.6977134943008423\n",
      "Epoch 3647/10000, Training Loss: 2.014084577560425, Validation Loss: 2.881781578063965\n",
      "Epoch 3648/10000, Training Loss: 1.327284574508667, Validation Loss: 1.502109408378601\n",
      "Epoch 3649/10000, Training Loss: 1.2949663400650024, Validation Loss: 0.6020280718803406\n",
      "Epoch 3650/10000, Training Loss: 1.7194396257400513, Validation Loss: 3.3832876682281494\n",
      "Epoch 3651/10000, Training Loss: 1.1873672008514404, Validation Loss: 1.0850380659103394\n",
      "Epoch 3652/10000, Training Loss: 1.8763922452926636, Validation Loss: 2.0183956623077393\n",
      "Epoch 3653/10000, Training Loss: 2.0549545288085938, Validation Loss: 1.0232075452804565\n",
      "Epoch 3654/10000, Training Loss: 1.2914150953292847, Validation Loss: 1.938949465751648\n",
      "Epoch 3655/10000, Training Loss: 2.123093605041504, Validation Loss: 2.911987543106079\n",
      "Epoch 3656/10000, Training Loss: 1.6850275993347168, Validation Loss: 1.0944353342056274\n",
      "Epoch 3657/10000, Training Loss: 1.4107710123062134, Validation Loss: 0.7853977084159851\n",
      "Epoch 3658/10000, Training Loss: 1.8891050815582275, Validation Loss: 3.4048492908477783\n",
      "Epoch 3659/10000, Training Loss: 1.2468804121017456, Validation Loss: 1.516493320465088\n",
      "Epoch 3660/10000, Training Loss: 1.4879218339920044, Validation Loss: 1.5592178106307983\n",
      "Epoch 3661/10000, Training Loss: 1.4114142656326294, Validation Loss: 0.6891977190971375\n",
      "Epoch 3662/10000, Training Loss: 1.1920002698898315, Validation Loss: 1.483811378479004\n",
      "Epoch 3663/10000, Training Loss: 1.40474534034729, Validation Loss: 0.3835400640964508\n",
      "Epoch 3664/10000, Training Loss: 1.5199525356292725, Validation Loss: 0.9198467135429382\n",
      "Epoch 3665/10000, Training Loss: 1.3906816244125366, Validation Loss: 2.879804849624634\n",
      "Epoch 3666/10000, Training Loss: 1.4454470872879028, Validation Loss: 1.2263898849487305\n",
      "Epoch 3667/10000, Training Loss: 1.2930806875228882, Validation Loss: 1.5275789499282837\n",
      "Epoch 3668/10000, Training Loss: 1.2300747632980347, Validation Loss: 1.550460934638977\n",
      "Epoch 3669/10000, Training Loss: 1.128857970237732, Validation Loss: 1.6564313173294067\n",
      "Epoch 3670/10000, Training Loss: 1.4710131883621216, Validation Loss: 1.3730745315551758\n",
      "Epoch 3671/10000, Training Loss: 1.5096869468688965, Validation Loss: 2.8934097290039062\n",
      "Epoch 3672/10000, Training Loss: 1.4056366682052612, Validation Loss: 2.9648616313934326\n",
      "Epoch 3673/10000, Training Loss: 1.46465003490448, Validation Loss: 2.5081112384796143\n",
      "Epoch 3674/10000, Training Loss: 1.3302580118179321, Validation Loss: 0.987255334854126\n",
      "Epoch 3675/10000, Training Loss: 1.3493469953536987, Validation Loss: 2.904916763305664\n",
      "Epoch 3676/10000, Training Loss: 1.1651384830474854, Validation Loss: 1.953560709953308\n",
      "Epoch 3677/10000, Training Loss: 1.3611557483673096, Validation Loss: 0.9639351963996887\n",
      "Epoch 3678/10000, Training Loss: 1.6082171201705933, Validation Loss: 1.233176589012146\n",
      "Epoch 3679/10000, Training Loss: 1.3702932596206665, Validation Loss: 2.492133378982544\n",
      "Epoch 3680/10000, Training Loss: 1.7703711986541748, Validation Loss: 1.7504717111587524\n",
      "Epoch 3681/10000, Training Loss: 1.5249546766281128, Validation Loss: 1.7946199178695679\n",
      "Epoch 3682/10000, Training Loss: 1.3380436897277832, Validation Loss: 1.9913654327392578\n",
      "Epoch 3683/10000, Training Loss: 1.3072750568389893, Validation Loss: 1.0358210802078247\n",
      "Epoch 3684/10000, Training Loss: 1.2208715677261353, Validation Loss: 1.1310161352157593\n",
      "Epoch 3685/10000, Training Loss: 1.5196316242218018, Validation Loss: 2.2946484088897705\n",
      "Epoch 3686/10000, Training Loss: 1.1543190479278564, Validation Loss: 3.173603057861328\n",
      "Epoch 3687/10000, Training Loss: 1.5597140789031982, Validation Loss: 1.9324783086776733\n",
      "Epoch 3688/10000, Training Loss: 1.3354448080062866, Validation Loss: 0.5480931997299194\n",
      "Epoch 3689/10000, Training Loss: 1.3948826789855957, Validation Loss: 1.3600822687149048\n",
      "Epoch 3690/10000, Training Loss: 1.280448079109192, Validation Loss: 1.964638352394104\n",
      "Epoch 3691/10000, Training Loss: 1.2449849843978882, Validation Loss: 2.1784651279449463\n",
      "Epoch 3692/10000, Training Loss: 1.3320279121398926, Validation Loss: 1.5840626955032349\n",
      "Epoch 3693/10000, Training Loss: 1.36211097240448, Validation Loss: 0.7917872071266174\n",
      "Epoch 3694/10000, Training Loss: 1.6492667198181152, Validation Loss: 2.4706363677978516\n",
      "Epoch 3695/10000, Training Loss: 1.445501446723938, Validation Loss: 2.394618272781372\n",
      "Epoch 3696/10000, Training Loss: 1.7332987785339355, Validation Loss: 2.542782783508301\n",
      "Epoch 3697/10000, Training Loss: 1.68747878074646, Validation Loss: 1.5266441106796265\n",
      "Epoch 3698/10000, Training Loss: 1.1288551092147827, Validation Loss: 0.9813871383666992\n",
      "Epoch 3699/10000, Training Loss: 1.8270691633224487, Validation Loss: 3.3662045001983643\n",
      "Epoch 3700/10000, Training Loss: 1.3481091260910034, Validation Loss: 3.256779432296753\n",
      "Epoch 3701/10000, Training Loss: 1.9230940341949463, Validation Loss: 2.663841485977173\n",
      "Epoch 3702/10000, Training Loss: 1.4670618772506714, Validation Loss: 2.2191338539123535\n",
      "Epoch 3703/10000, Training Loss: 1.9149935245513916, Validation Loss: 1.7963358163833618\n",
      "Epoch 3704/10000, Training Loss: 1.3564426898956299, Validation Loss: 1.3755277395248413\n",
      "Epoch 3705/10000, Training Loss: 1.3936034440994263, Validation Loss: 0.7366973757743835\n",
      "Epoch 3706/10000, Training Loss: 1.5277050733566284, Validation Loss: 1.73062264919281\n",
      "Epoch 3707/10000, Training Loss: 1.4781723022460938, Validation Loss: 2.42987322807312\n",
      "Epoch 3708/10000, Training Loss: 1.5881787538528442, Validation Loss: 1.9288291931152344\n",
      "Epoch 3709/10000, Training Loss: 1.4289809465408325, Validation Loss: 2.094287872314453\n",
      "Epoch 3710/10000, Training Loss: 1.6733813285827637, Validation Loss: 1.0155693292617798\n",
      "Epoch 3711/10000, Training Loss: 1.1832425594329834, Validation Loss: 2.3451836109161377\n",
      "Epoch 3712/10000, Training Loss: 1.2899878025054932, Validation Loss: 1.3252557516098022\n",
      "Epoch 3713/10000, Training Loss: 1.473494052886963, Validation Loss: 0.7775802612304688\n",
      "Epoch 3714/10000, Training Loss: 1.873481273651123, Validation Loss: 3.578152656555176\n",
      "Epoch 3715/10000, Training Loss: 1.2675126791000366, Validation Loss: 2.649566888809204\n",
      "Epoch 3716/10000, Training Loss: 1.5576424598693848, Validation Loss: 1.9169682264328003\n",
      "Epoch 3717/10000, Training Loss: 1.580780029296875, Validation Loss: 1.4473682641983032\n",
      "Epoch 3718/10000, Training Loss: 1.3119736909866333, Validation Loss: 1.4171814918518066\n",
      "Epoch 3719/10000, Training Loss: 1.274411916732788, Validation Loss: 1.6650830507278442\n",
      "Epoch 3720/10000, Training Loss: 1.4239366054534912, Validation Loss: 1.2070485353469849\n",
      "Epoch 3721/10000, Training Loss: 1.4150924682617188, Validation Loss: 0.49257752299308777\n",
      "Epoch 3722/10000, Training Loss: 1.1216524839401245, Validation Loss: 1.0471608638763428\n",
      "Epoch 3723/10000, Training Loss: 1.3772094249725342, Validation Loss: 1.6290968656539917\n",
      "Epoch 3724/10000, Training Loss: 1.235410213470459, Validation Loss: 0.5886315107345581\n",
      "Epoch 3725/10000, Training Loss: 1.757750153541565, Validation Loss: 1.627720832824707\n",
      "Epoch 3726/10000, Training Loss: 1.2878233194351196, Validation Loss: 2.404057025909424\n",
      "Epoch 3727/10000, Training Loss: 1.575117588043213, Validation Loss: 3.619964599609375\n",
      "Epoch 3728/10000, Training Loss: 1.5646597146987915, Validation Loss: 3.2313787937164307\n",
      "Epoch 3729/10000, Training Loss: 1.145837664604187, Validation Loss: 1.5104578733444214\n",
      "Epoch 3730/10000, Training Loss: 1.702636957168579, Validation Loss: 1.7541230916976929\n",
      "Epoch 3731/10000, Training Loss: 1.6621171236038208, Validation Loss: 1.4442663192749023\n",
      "Epoch 3732/10000, Training Loss: 1.315053105354309, Validation Loss: 1.4963537454605103\n",
      "Epoch 3733/10000, Training Loss: 1.1537768840789795, Validation Loss: 1.8252352476119995\n",
      "Epoch 3734/10000, Training Loss: 1.2804725170135498, Validation Loss: 0.6527833342552185\n",
      "Epoch 3735/10000, Training Loss: 1.5888358354568481, Validation Loss: 1.390653133392334\n",
      "Epoch 3736/10000, Training Loss: 1.3063355684280396, Validation Loss: 2.719538927078247\n",
      "Epoch 3737/10000, Training Loss: 1.6545881032943726, Validation Loss: 0.6470787525177002\n",
      "Epoch 3738/10000, Training Loss: 2.0121731758117676, Validation Loss: 2.523794412612915\n",
      "Epoch 3739/10000, Training Loss: 1.4869017601013184, Validation Loss: 1.8871999979019165\n",
      "Epoch 3740/10000, Training Loss: 1.3261762857437134, Validation Loss: 2.449586868286133\n",
      "Epoch 3741/10000, Training Loss: 1.4098095893859863, Validation Loss: 1.7206381559371948\n",
      "Epoch 3742/10000, Training Loss: 1.3045780658721924, Validation Loss: 0.7471718192100525\n",
      "Epoch 3743/10000, Training Loss: 1.8304227590560913, Validation Loss: 2.30167293548584\n",
      "Epoch 3744/10000, Training Loss: 1.0005180835723877, Validation Loss: 0.9623532891273499\n",
      "Epoch 3745/10000, Training Loss: 1.293537974357605, Validation Loss: 0.9253256320953369\n",
      "Epoch 3746/10000, Training Loss: 1.3315098285675049, Validation Loss: 4.197989463806152\n",
      "Epoch 3747/10000, Training Loss: 1.3220088481903076, Validation Loss: 0.5497351288795471\n",
      "Epoch 3748/10000, Training Loss: 1.4424599409103394, Validation Loss: 1.0214147567749023\n",
      "Epoch 3749/10000, Training Loss: 1.2816859483718872, Validation Loss: 0.6420937776565552\n",
      "Epoch 3750/10000, Training Loss: 1.4403353929519653, Validation Loss: 1.9117684364318848\n",
      "Epoch 3751/10000, Training Loss: 1.2554086446762085, Validation Loss: 1.9269920587539673\n",
      "Epoch 3752/10000, Training Loss: 1.4757660627365112, Validation Loss: 1.9389972686767578\n",
      "Epoch 3753/10000, Training Loss: 1.3184702396392822, Validation Loss: 1.5866607427597046\n",
      "Epoch 3754/10000, Training Loss: 1.8253562450408936, Validation Loss: 1.3097456693649292\n",
      "Epoch 3755/10000, Training Loss: 1.5647345781326294, Validation Loss: 1.602447509765625\n",
      "Epoch 3756/10000, Training Loss: 1.728905439376831, Validation Loss: 0.8591058850288391\n",
      "Epoch 3757/10000, Training Loss: 1.6170077323913574, Validation Loss: 1.260567545890808\n",
      "Epoch 3758/10000, Training Loss: 1.4250112771987915, Validation Loss: 1.1457535028457642\n",
      "Epoch 3759/10000, Training Loss: 1.2841345071792603, Validation Loss: 1.136454701423645\n",
      "Epoch 3760/10000, Training Loss: 1.6926460266113281, Validation Loss: 2.2057220935821533\n",
      "Epoch 3761/10000, Training Loss: 1.3387434482574463, Validation Loss: 1.562220573425293\n",
      "Epoch 3762/10000, Training Loss: 1.2251229286193848, Validation Loss: 1.362433910369873\n",
      "Epoch 3763/10000, Training Loss: 1.2935153245925903, Validation Loss: 2.376948118209839\n",
      "Epoch 3764/10000, Training Loss: 1.290440320968628, Validation Loss: 2.033021926879883\n",
      "Epoch 3765/10000, Training Loss: 1.466104507446289, Validation Loss: 4.534469127655029\n",
      "Epoch 3766/10000, Training Loss: 1.5154662132263184, Validation Loss: 1.5326036214828491\n",
      "Epoch 3767/10000, Training Loss: 1.749132752418518, Validation Loss: 1.4921393394470215\n",
      "Epoch 3768/10000, Training Loss: 1.0849194526672363, Validation Loss: 1.7240854501724243\n",
      "Epoch 3769/10000, Training Loss: 1.3091424703598022, Validation Loss: 1.1816937923431396\n",
      "Epoch 3770/10000, Training Loss: 1.1364470720291138, Validation Loss: 1.7369180917739868\n",
      "Epoch 3771/10000, Training Loss: 1.3772927522659302, Validation Loss: 0.8117034435272217\n",
      "Epoch 3772/10000, Training Loss: 1.4181488752365112, Validation Loss: 1.1489161252975464\n",
      "Epoch 3773/10000, Training Loss: 1.5489909648895264, Validation Loss: 1.2410846948623657\n",
      "Epoch 3774/10000, Training Loss: 1.1257296800613403, Validation Loss: 0.8543604016304016\n",
      "Epoch 3775/10000, Training Loss: 1.3497467041015625, Validation Loss: 2.0725021362304688\n",
      "Epoch 3776/10000, Training Loss: 1.7411504983901978, Validation Loss: 1.3249129056930542\n",
      "Epoch 3777/10000, Training Loss: 1.7561228275299072, Validation Loss: 2.238996982574463\n",
      "Epoch 3778/10000, Training Loss: 2.190638780593872, Validation Loss: 2.058319330215454\n",
      "Epoch 3779/10000, Training Loss: 1.3921555280685425, Validation Loss: 2.486973524093628\n",
      "Epoch 3780/10000, Training Loss: 1.3100147247314453, Validation Loss: 1.7817744016647339\n",
      "Epoch 3781/10000, Training Loss: 1.2005723714828491, Validation Loss: 0.9705474972724915\n",
      "Epoch 3782/10000, Training Loss: 1.6885735988616943, Validation Loss: 4.6311936378479\n",
      "Epoch 3783/10000, Training Loss: 1.5855753421783447, Validation Loss: 1.7316280603408813\n",
      "Epoch 3784/10000, Training Loss: 1.3794912099838257, Validation Loss: 1.5090605020523071\n",
      "Epoch 3785/10000, Training Loss: 1.109311580657959, Validation Loss: 1.7928743362426758\n",
      "Epoch 3786/10000, Training Loss: 1.4107252359390259, Validation Loss: 1.0005625486373901\n",
      "Epoch 3787/10000, Training Loss: 1.3897039890289307, Validation Loss: 2.1324613094329834\n",
      "Epoch 3788/10000, Training Loss: 1.575950026512146, Validation Loss: 2.4208908081054688\n",
      "Epoch 3789/10000, Training Loss: 1.3413575887680054, Validation Loss: 1.3199995756149292\n",
      "Epoch 3790/10000, Training Loss: 1.882066011428833, Validation Loss: 2.044398546218872\n",
      "Epoch 3791/10000, Training Loss: 1.0861574411392212, Validation Loss: 1.2015548944473267\n",
      "Epoch 3792/10000, Training Loss: 1.3427802324295044, Validation Loss: 1.907976508140564\n",
      "Epoch 3793/10000, Training Loss: 2.0098154544830322, Validation Loss: 2.3938426971435547\n",
      "Epoch 3794/10000, Training Loss: 1.4816442728042603, Validation Loss: 2.449873685836792\n",
      "Epoch 3795/10000, Training Loss: 1.4193063974380493, Validation Loss: 0.6010071635246277\n",
      "Epoch 3796/10000, Training Loss: 1.2326470613479614, Validation Loss: 2.2223503589630127\n",
      "Epoch 3797/10000, Training Loss: 1.295422911643982, Validation Loss: 2.0551414489746094\n",
      "Epoch 3798/10000, Training Loss: 1.121813416481018, Validation Loss: 0.548333466053009\n",
      "Epoch 3799/10000, Training Loss: 1.5830020904541016, Validation Loss: 1.444216251373291\n",
      "Epoch 3800/10000, Training Loss: 1.6029818058013916, Validation Loss: 0.6078288555145264\n",
      "Epoch 3801/10000, Training Loss: 1.5394492149353027, Validation Loss: 0.840812623500824\n",
      "Epoch 3802/10000, Training Loss: 1.2001888751983643, Validation Loss: 1.5024758577346802\n",
      "Epoch 3803/10000, Training Loss: 1.250584363937378, Validation Loss: 0.6977065205574036\n",
      "Epoch 3804/10000, Training Loss: 1.4269033670425415, Validation Loss: 1.0320513248443604\n",
      "Epoch 3805/10000, Training Loss: 1.8136966228485107, Validation Loss: 2.113398551940918\n",
      "Epoch 3806/10000, Training Loss: 1.6095339059829712, Validation Loss: 2.103853225708008\n",
      "Epoch 3807/10000, Training Loss: 1.1486504077911377, Validation Loss: 0.9142894148826599\n",
      "Epoch 3808/10000, Training Loss: 1.5632007122039795, Validation Loss: 1.0068405866622925\n",
      "Epoch 3809/10000, Training Loss: 1.488311529159546, Validation Loss: 1.2032734155654907\n",
      "Epoch 3810/10000, Training Loss: 1.6334742307662964, Validation Loss: 2.4722023010253906\n",
      "Epoch 3811/10000, Training Loss: 1.9959237575531006, Validation Loss: 1.7316083908081055\n",
      "Epoch 3812/10000, Training Loss: 1.3964532613754272, Validation Loss: 3.4102907180786133\n",
      "Epoch 3813/10000, Training Loss: 0.9574843049049377, Validation Loss: 0.8623238205909729\n",
      "Epoch 3814/10000, Training Loss: 1.6606879234313965, Validation Loss: 2.2401695251464844\n",
      "Epoch 3815/10000, Training Loss: 1.182985544204712, Validation Loss: 0.6369177103042603\n",
      "Epoch 3816/10000, Training Loss: 2.0830588340759277, Validation Loss: 2.278815984725952\n",
      "Epoch 3817/10000, Training Loss: 1.51124906539917, Validation Loss: 1.5359363555908203\n",
      "Epoch 3818/10000, Training Loss: 1.2799656391143799, Validation Loss: 2.3517110347747803\n",
      "Epoch 3819/10000, Training Loss: 1.2988656759262085, Validation Loss: 1.240140676498413\n",
      "Epoch 3820/10000, Training Loss: 1.1821776628494263, Validation Loss: 1.1039913892745972\n",
      "Epoch 3821/10000, Training Loss: 1.865364670753479, Validation Loss: 1.2355448007583618\n",
      "Epoch 3822/10000, Training Loss: 1.1701302528381348, Validation Loss: 1.19599187374115\n",
      "Epoch 3823/10000, Training Loss: 1.3257838487625122, Validation Loss: 1.9183273315429688\n",
      "Epoch 3824/10000, Training Loss: 1.5021395683288574, Validation Loss: 0.550041913986206\n",
      "Epoch 3825/10000, Training Loss: 1.4727815389633179, Validation Loss: 2.0766255855560303\n",
      "Epoch 3826/10000, Training Loss: 0.9958381652832031, Validation Loss: 1.9566577672958374\n",
      "Epoch 3827/10000, Training Loss: 1.7768330574035645, Validation Loss: 1.1016819477081299\n",
      "Epoch 3828/10000, Training Loss: 1.5670440196990967, Validation Loss: 1.3943758010864258\n",
      "Epoch 3829/10000, Training Loss: 1.5360770225524902, Validation Loss: 1.1460925340652466\n",
      "Epoch 3830/10000, Training Loss: 1.2653677463531494, Validation Loss: 1.2721912860870361\n",
      "Epoch 3831/10000, Training Loss: 1.5556222200393677, Validation Loss: 2.421868085861206\n",
      "Epoch 3832/10000, Training Loss: 1.5660830736160278, Validation Loss: 0.9309802055358887\n",
      "Epoch 3833/10000, Training Loss: 1.4628225564956665, Validation Loss: 1.477095603942871\n",
      "Epoch 3834/10000, Training Loss: 1.3037444353103638, Validation Loss: 1.0323704481124878\n",
      "Epoch 3835/10000, Training Loss: 1.5087966918945312, Validation Loss: 1.4013704061508179\n",
      "Epoch 3836/10000, Training Loss: 1.0161484479904175, Validation Loss: 3.123086929321289\n",
      "Epoch 3837/10000, Training Loss: 1.305751085281372, Validation Loss: 0.9560549855232239\n",
      "Epoch 3838/10000, Training Loss: 1.346771240234375, Validation Loss: 1.5705255270004272\n",
      "Epoch 3839/10000, Training Loss: 1.0921965837478638, Validation Loss: 1.3461098670959473\n",
      "Epoch 3840/10000, Training Loss: 1.434713363647461, Validation Loss: 1.9522753953933716\n",
      "Epoch 3841/10000, Training Loss: 1.3033944368362427, Validation Loss: 1.5234432220458984\n",
      "Epoch 3842/10000, Training Loss: 2.2880938053131104, Validation Loss: 1.7109421491622925\n",
      "Epoch 3843/10000, Training Loss: 1.4214587211608887, Validation Loss: 0.6804106831550598\n",
      "Epoch 3844/10000, Training Loss: 1.2567156553268433, Validation Loss: 0.9592016339302063\n",
      "Epoch 3845/10000, Training Loss: 1.8930233716964722, Validation Loss: 1.3188226222991943\n",
      "Epoch 3846/10000, Training Loss: 1.348152995109558, Validation Loss: 3.2447011470794678\n",
      "Epoch 3847/10000, Training Loss: 1.4310133457183838, Validation Loss: 2.261599540710449\n",
      "Epoch 3848/10000, Training Loss: 1.2150461673736572, Validation Loss: 2.2914063930511475\n",
      "Epoch 3849/10000, Training Loss: 1.3387356996536255, Validation Loss: 6.397449970245361\n",
      "Epoch 3850/10000, Training Loss: 1.1157841682434082, Validation Loss: 1.159116268157959\n",
      "Epoch 3851/10000, Training Loss: 1.5181679725646973, Validation Loss: 1.0541226863861084\n",
      "Epoch 3852/10000, Training Loss: 1.115185022354126, Validation Loss: 2.0685927867889404\n",
      "Epoch 3853/10000, Training Loss: 1.4284160137176514, Validation Loss: 0.6977694034576416\n",
      "Epoch 3854/10000, Training Loss: 1.074627161026001, Validation Loss: 1.852262020111084\n",
      "Epoch 3855/10000, Training Loss: 1.1865819692611694, Validation Loss: 1.8624268770217896\n",
      "Epoch 3856/10000, Training Loss: 1.4618453979492188, Validation Loss: 1.1642364263534546\n",
      "Epoch 3857/10000, Training Loss: 1.3756121397018433, Validation Loss: 1.4104775190353394\n",
      "Epoch 3858/10000, Training Loss: 1.1966975927352905, Validation Loss: 3.5016772747039795\n",
      "Epoch 3859/10000, Training Loss: 1.7938005924224854, Validation Loss: 2.3891685009002686\n",
      "Epoch 3860/10000, Training Loss: 1.6824501752853394, Validation Loss: 1.0374029874801636\n",
      "Epoch 3861/10000, Training Loss: 1.684204339981079, Validation Loss: 1.136000633239746\n",
      "Epoch 3862/10000, Training Loss: 1.1152307987213135, Validation Loss: 1.0958484411239624\n",
      "Epoch 3863/10000, Training Loss: 1.1978325843811035, Validation Loss: 1.2702773809432983\n",
      "Epoch 3864/10000, Training Loss: 1.5414791107177734, Validation Loss: 0.828469455242157\n",
      "Epoch 3865/10000, Training Loss: 1.305403232574463, Validation Loss: 1.8032485246658325\n",
      "Epoch 3866/10000, Training Loss: 1.4656085968017578, Validation Loss: 2.194780111312866\n",
      "Epoch 3867/10000, Training Loss: 1.1650817394256592, Validation Loss: 1.2470422983169556\n",
      "Epoch 3868/10000, Training Loss: 1.751886248588562, Validation Loss: 1.4765881299972534\n",
      "Epoch 3869/10000, Training Loss: 1.238506555557251, Validation Loss: 1.969907283782959\n",
      "Epoch 3870/10000, Training Loss: 1.6799172163009644, Validation Loss: 2.6932926177978516\n",
      "Epoch 3871/10000, Training Loss: 1.4262726306915283, Validation Loss: 1.4136203527450562\n",
      "Epoch 3872/10000, Training Loss: 1.805189847946167, Validation Loss: 1.6675809621810913\n",
      "Epoch 3873/10000, Training Loss: 1.7962702512741089, Validation Loss: 1.5665054321289062\n",
      "Epoch 3874/10000, Training Loss: 1.2636040449142456, Validation Loss: 1.9891648292541504\n",
      "Epoch 3875/10000, Training Loss: 1.4700226783752441, Validation Loss: 1.498019814491272\n",
      "Epoch 3876/10000, Training Loss: 1.6000741720199585, Validation Loss: 1.4307974576950073\n",
      "Epoch 3877/10000, Training Loss: 1.5020414590835571, Validation Loss: 1.9623314142227173\n",
      "Epoch 3878/10000, Training Loss: 1.5196998119354248, Validation Loss: 0.9845349192619324\n",
      "Epoch 3879/10000, Training Loss: 1.1036922931671143, Validation Loss: 0.8047866225242615\n",
      "Epoch 3880/10000, Training Loss: 1.6169047355651855, Validation Loss: 2.2472968101501465\n",
      "Epoch 3881/10000, Training Loss: 1.362400770187378, Validation Loss: 2.9156837463378906\n",
      "Epoch 3882/10000, Training Loss: 1.7009779214859009, Validation Loss: 2.774820327758789\n",
      "Epoch 3883/10000, Training Loss: 1.3254566192626953, Validation Loss: 3.1348021030426025\n",
      "Epoch 3884/10000, Training Loss: 1.1817351579666138, Validation Loss: 1.4998270273208618\n",
      "Epoch 3885/10000, Training Loss: 1.1706639528274536, Validation Loss: 1.2307629585266113\n",
      "Epoch 3886/10000, Training Loss: 1.4259308576583862, Validation Loss: 1.9790865182876587\n",
      "Epoch 3887/10000, Training Loss: 1.2454180717468262, Validation Loss: 1.073278546333313\n",
      "Epoch 3888/10000, Training Loss: 1.2442072629928589, Validation Loss: 1.2552508115768433\n",
      "Epoch 3889/10000, Training Loss: 0.9348517656326294, Validation Loss: 0.8707042336463928\n",
      "Epoch 3890/10000, Training Loss: 1.335050106048584, Validation Loss: 1.1988413333892822\n",
      "Epoch 3891/10000, Training Loss: 1.2391330003738403, Validation Loss: 2.1916344165802\n",
      "Epoch 3892/10000, Training Loss: 1.3067818880081177, Validation Loss: 0.8423026204109192\n",
      "Epoch 3893/10000, Training Loss: 1.1737706661224365, Validation Loss: 1.956344723701477\n",
      "Epoch 3894/10000, Training Loss: 1.4730370044708252, Validation Loss: 1.4661270380020142\n",
      "Epoch 3895/10000, Training Loss: 1.220765233039856, Validation Loss: 1.8919767141342163\n",
      "Epoch 3896/10000, Training Loss: 1.5721806287765503, Validation Loss: 1.2258249521255493\n",
      "Epoch 3897/10000, Training Loss: 1.2763688564300537, Validation Loss: 1.3314833641052246\n",
      "Epoch 3898/10000, Training Loss: 1.2781392335891724, Validation Loss: 0.7593128681182861\n",
      "Epoch 3899/10000, Training Loss: 1.1634031534194946, Validation Loss: 1.8069394826889038\n",
      "Epoch 3900/10000, Training Loss: 1.2368927001953125, Validation Loss: 1.515568733215332\n",
      "Epoch 3901/10000, Training Loss: 0.8912731409072876, Validation Loss: 1.4807807207107544\n",
      "Epoch 3902/10000, Training Loss: 1.8855572938919067, Validation Loss: 2.455533981323242\n",
      "Epoch 3903/10000, Training Loss: 1.4506396055221558, Validation Loss: 2.1761248111724854\n",
      "Epoch 3904/10000, Training Loss: 1.3920838832855225, Validation Loss: 1.4979286193847656\n",
      "Epoch 3905/10000, Training Loss: 1.2242237329483032, Validation Loss: 2.410102128982544\n",
      "Epoch 3906/10000, Training Loss: 1.1155273914337158, Validation Loss: 1.1388479471206665\n",
      "Epoch 3907/10000, Training Loss: 1.2215325832366943, Validation Loss: 0.5943741202354431\n",
      "Epoch 3908/10000, Training Loss: 1.5027250051498413, Validation Loss: 1.3532034158706665\n",
      "Epoch 3909/10000, Training Loss: 1.1155894994735718, Validation Loss: 0.43505391478538513\n",
      "Epoch 3910/10000, Training Loss: 1.67525053024292, Validation Loss: 1.3284162282943726\n",
      "Epoch 3911/10000, Training Loss: 1.541974425315857, Validation Loss: 2.073136568069458\n",
      "Epoch 3912/10000, Training Loss: 0.9921920299530029, Validation Loss: 1.3613063097000122\n",
      "Epoch 3913/10000, Training Loss: 1.4523494243621826, Validation Loss: 2.218425989151001\n",
      "Epoch 3914/10000, Training Loss: 1.3256372213363647, Validation Loss: 1.5099760293960571\n",
      "Epoch 3915/10000, Training Loss: 0.9453675746917725, Validation Loss: 1.7181228399276733\n",
      "Epoch 3916/10000, Training Loss: 1.730766773223877, Validation Loss: 1.6438168287277222\n",
      "Epoch 3917/10000, Training Loss: 1.4913060665130615, Validation Loss: 2.9815709590911865\n",
      "Epoch 3918/10000, Training Loss: 1.2863914966583252, Validation Loss: 0.6511050462722778\n",
      "Epoch 3919/10000, Training Loss: 1.1469471454620361, Validation Loss: 1.5845789909362793\n",
      "Epoch 3920/10000, Training Loss: 1.874259114265442, Validation Loss: 3.3878018856048584\n",
      "Epoch 3921/10000, Training Loss: 1.2239595651626587, Validation Loss: 3.2665841579437256\n",
      "Epoch 3922/10000, Training Loss: 1.0186151266098022, Validation Loss: 1.2185441255569458\n",
      "Epoch 3923/10000, Training Loss: 1.1079583168029785, Validation Loss: 1.041047215461731\n",
      "Epoch 3924/10000, Training Loss: 1.1644542217254639, Validation Loss: 0.9854065775871277\n",
      "Epoch 3925/10000, Training Loss: 1.4741122722625732, Validation Loss: 1.3885191679000854\n",
      "Epoch 3926/10000, Training Loss: 1.3504817485809326, Validation Loss: 1.0347926616668701\n",
      "Epoch 3927/10000, Training Loss: 1.7102445363998413, Validation Loss: 1.2001796960830688\n",
      "Epoch 3928/10000, Training Loss: 1.1912250518798828, Validation Loss: 1.451677918434143\n",
      "Epoch 3929/10000, Training Loss: 1.683552622795105, Validation Loss: 3.071378707885742\n",
      "Epoch 3930/10000, Training Loss: 1.2467331886291504, Validation Loss: 1.108970046043396\n",
      "Epoch 3931/10000, Training Loss: 1.0577499866485596, Validation Loss: 1.0915154218673706\n",
      "Epoch 3932/10000, Training Loss: 1.1153099536895752, Validation Loss: 1.4488433599472046\n",
      "Epoch 3933/10000, Training Loss: 1.3163070678710938, Validation Loss: 0.9146353602409363\n",
      "Epoch 3934/10000, Training Loss: 1.2619200944900513, Validation Loss: 1.6007741689682007\n",
      "Epoch 3935/10000, Training Loss: 1.1555418968200684, Validation Loss: 0.8542389273643494\n",
      "Epoch 3936/10000, Training Loss: 1.4950847625732422, Validation Loss: 2.17741322517395\n",
      "Epoch 3937/10000, Training Loss: 1.389393925666809, Validation Loss: 1.1022230386734009\n",
      "Epoch 3938/10000, Training Loss: 0.9306434988975525, Validation Loss: 1.9227927923202515\n",
      "Epoch 3939/10000, Training Loss: 1.323779821395874, Validation Loss: 1.2221020460128784\n",
      "Epoch 3940/10000, Training Loss: 1.3602690696716309, Validation Loss: 1.517811894416809\n",
      "Epoch 3941/10000, Training Loss: 1.0262941122055054, Validation Loss: 1.5690007209777832\n",
      "Epoch 3942/10000, Training Loss: 1.2372225522994995, Validation Loss: 1.3110541105270386\n",
      "Epoch 3943/10000, Training Loss: 1.3939236402511597, Validation Loss: 1.608911156654358\n",
      "Epoch 3944/10000, Training Loss: 1.0218452215194702, Validation Loss: 0.4810295104980469\n",
      "Epoch 3945/10000, Training Loss: 1.2930960655212402, Validation Loss: 1.1025437116622925\n",
      "Epoch 3946/10000, Training Loss: 1.2536327838897705, Validation Loss: 1.540631890296936\n",
      "Epoch 3947/10000, Training Loss: 1.0804563760757446, Validation Loss: 1.0360323190689087\n",
      "Epoch 3948/10000, Training Loss: 1.7294737100601196, Validation Loss: 3.1415958404541016\n",
      "Epoch 3949/10000, Training Loss: 1.2738350629806519, Validation Loss: 0.8201478123664856\n",
      "Epoch 3950/10000, Training Loss: 1.2979882955551147, Validation Loss: 2.1902246475219727\n",
      "Epoch 3951/10000, Training Loss: 1.1666290760040283, Validation Loss: 1.1969295740127563\n",
      "Epoch 3952/10000, Training Loss: 1.3491644859313965, Validation Loss: 2.2729475498199463\n",
      "Epoch 3953/10000, Training Loss: 1.0183167457580566, Validation Loss: 1.0293471813201904\n",
      "Epoch 3954/10000, Training Loss: 1.292013168334961, Validation Loss: 1.668394684791565\n",
      "Epoch 3955/10000, Training Loss: 0.9146435260772705, Validation Loss: 1.170300841331482\n",
      "Epoch 3956/10000, Training Loss: 1.422241449356079, Validation Loss: 0.673272430896759\n",
      "Epoch 3957/10000, Training Loss: 1.6023824214935303, Validation Loss: 2.596954822540283\n",
      "Epoch 3958/10000, Training Loss: 1.7621899843215942, Validation Loss: 1.169870376586914\n",
      "Epoch 3959/10000, Training Loss: 1.687872052192688, Validation Loss: 1.4062820672988892\n",
      "Epoch 3960/10000, Training Loss: 1.1980115175247192, Validation Loss: 1.8484630584716797\n",
      "Epoch 3961/10000, Training Loss: 1.3506077527999878, Validation Loss: 1.7244654893875122\n",
      "Epoch 3962/10000, Training Loss: 1.3548882007598877, Validation Loss: 1.6217812299728394\n",
      "Epoch 3963/10000, Training Loss: 1.1988343000411987, Validation Loss: 1.0355714559555054\n",
      "Epoch 3964/10000, Training Loss: 1.188329815864563, Validation Loss: 1.1714781522750854\n",
      "Epoch 3965/10000, Training Loss: 1.1345138549804688, Validation Loss: 2.128390073776245\n",
      "Epoch 3966/10000, Training Loss: 1.353361964225769, Validation Loss: 1.9973276853561401\n",
      "Epoch 3967/10000, Training Loss: 1.238391637802124, Validation Loss: 0.6130630970001221\n",
      "Epoch 3968/10000, Training Loss: 1.1108894348144531, Validation Loss: 1.350908637046814\n",
      "Epoch 3969/10000, Training Loss: 1.312255859375, Validation Loss: 2.5948946475982666\n",
      "Epoch 3970/10000, Training Loss: 1.401094913482666, Validation Loss: 1.7291935682296753\n",
      "Epoch 3971/10000, Training Loss: 1.1901764869689941, Validation Loss: 1.375050663948059\n",
      "Epoch 3972/10000, Training Loss: 1.2258894443511963, Validation Loss: 1.2442458868026733\n",
      "Epoch 3973/10000, Training Loss: 1.0702340602874756, Validation Loss: 0.5021464228630066\n",
      "Epoch 3974/10000, Training Loss: 1.0246847867965698, Validation Loss: 1.7034050226211548\n",
      "Epoch 3975/10000, Training Loss: 1.3368809223175049, Validation Loss: 1.316753625869751\n",
      "Epoch 3976/10000, Training Loss: 1.2597683668136597, Validation Loss: 1.552263855934143\n",
      "Epoch 3977/10000, Training Loss: 1.2496144771575928, Validation Loss: 1.4008294343948364\n",
      "Epoch 3978/10000, Training Loss: 1.4597642421722412, Validation Loss: 1.924850583076477\n",
      "Epoch 3979/10000, Training Loss: 1.4331504106521606, Validation Loss: 2.273768186569214\n",
      "Epoch 3980/10000, Training Loss: 1.1849112510681152, Validation Loss: 1.073494791984558\n",
      "Epoch 3981/10000, Training Loss: 1.462915301322937, Validation Loss: 0.6625309586524963\n",
      "Epoch 3982/10000, Training Loss: 1.2052885293960571, Validation Loss: 1.556870460510254\n",
      "Epoch 3983/10000, Training Loss: 1.3922593593597412, Validation Loss: 1.6398237943649292\n",
      "Epoch 3984/10000, Training Loss: 1.2242491245269775, Validation Loss: 1.577093482017517\n",
      "Epoch 3985/10000, Training Loss: 1.7666840553283691, Validation Loss: 1.8018442392349243\n",
      "Epoch 3986/10000, Training Loss: 1.1376625299453735, Validation Loss: 0.6504570841789246\n",
      "Epoch 3987/10000, Training Loss: 1.253340244293213, Validation Loss: 2.1418721675872803\n",
      "Epoch 3988/10000, Training Loss: 1.4567903280258179, Validation Loss: 0.7754898071289062\n",
      "Epoch 3989/10000, Training Loss: 1.1702443361282349, Validation Loss: 0.7367758750915527\n",
      "Epoch 3990/10000, Training Loss: 1.3225640058517456, Validation Loss: 1.6598230600357056\n",
      "Epoch 3991/10000, Training Loss: 1.088243007659912, Validation Loss: 2.4771881103515625\n",
      "Epoch 3992/10000, Training Loss: 1.1161932945251465, Validation Loss: 0.9915395379066467\n",
      "Epoch 3993/10000, Training Loss: 1.424027681350708, Validation Loss: 1.2793607711791992\n",
      "Epoch 3994/10000, Training Loss: 1.1905436515808105, Validation Loss: 1.500257134437561\n",
      "Epoch 3995/10000, Training Loss: 1.0050830841064453, Validation Loss: 0.9394122958183289\n",
      "Epoch 3996/10000, Training Loss: 1.3681715726852417, Validation Loss: 1.0412825345993042\n",
      "Epoch 3997/10000, Training Loss: 1.4715956449508667, Validation Loss: 1.3752573728561401\n",
      "Epoch 3998/10000, Training Loss: 1.1587402820587158, Validation Loss: 0.7756031155586243\n",
      "Epoch 3999/10000, Training Loss: 1.0016833543777466, Validation Loss: 0.8204779624938965\n",
      "Epoch 4000/10000, Training Loss: 1.2084732055664062, Validation Loss: 2.3292791843414307\n",
      "Epoch 4001/10000, Training Loss: 1.0617271661758423, Validation Loss: 1.355576992034912\n",
      "Epoch 4002/10000, Training Loss: 1.4091925621032715, Validation Loss: 1.2842663526535034\n",
      "Epoch 4003/10000, Training Loss: 1.3225092887878418, Validation Loss: 1.2367793321609497\n",
      "Epoch 4004/10000, Training Loss: 1.3138234615325928, Validation Loss: 1.031658411026001\n",
      "Epoch 4005/10000, Training Loss: 1.3477110862731934, Validation Loss: 2.183640956878662\n",
      "Epoch 4006/10000, Training Loss: 1.1100512742996216, Validation Loss: 1.608091950416565\n",
      "Epoch 4007/10000, Training Loss: 1.181027889251709, Validation Loss: 0.9259945750236511\n",
      "Epoch 4008/10000, Training Loss: 0.9936536550521851, Validation Loss: 0.9817420840263367\n",
      "Epoch 4009/10000, Training Loss: 1.0623741149902344, Validation Loss: 1.3305059671401978\n",
      "Epoch 4010/10000, Training Loss: 1.1540467739105225, Validation Loss: 1.5477275848388672\n",
      "Epoch 4011/10000, Training Loss: 1.064542293548584, Validation Loss: 1.2567778825759888\n",
      "Epoch 4012/10000, Training Loss: 1.2965483665466309, Validation Loss: 1.6760482788085938\n",
      "Epoch 4013/10000, Training Loss: 1.1986820697784424, Validation Loss: 0.6612145900726318\n",
      "Epoch 4014/10000, Training Loss: 1.4625296592712402, Validation Loss: 1.0798648595809937\n",
      "Epoch 4015/10000, Training Loss: 1.4863433837890625, Validation Loss: 3.3853466510772705\n",
      "Epoch 4016/10000, Training Loss: 1.204886794090271, Validation Loss: 1.5405302047729492\n",
      "Epoch 4017/10000, Training Loss: 1.1160085201263428, Validation Loss: 0.7953673005104065\n",
      "Epoch 4018/10000, Training Loss: 1.2567543983459473, Validation Loss: 1.9271026849746704\n",
      "Epoch 4019/10000, Training Loss: 1.437601923942566, Validation Loss: 0.7175316214561462\n",
      "Epoch 4020/10000, Training Loss: 1.0375088453292847, Validation Loss: 0.8920185565948486\n",
      "Epoch 4021/10000, Training Loss: 0.9689022898674011, Validation Loss: 2.1044881343841553\n",
      "Epoch 4022/10000, Training Loss: 1.2967413663864136, Validation Loss: 1.6255172491073608\n",
      "Epoch 4023/10000, Training Loss: 1.0641918182373047, Validation Loss: 1.3862766027450562\n",
      "Epoch 4024/10000, Training Loss: 0.9693823456764221, Validation Loss: 0.7411556243896484\n",
      "Epoch 4025/10000, Training Loss: 0.9546304941177368, Validation Loss: 2.123322010040283\n",
      "Epoch 4026/10000, Training Loss: 0.9756091237068176, Validation Loss: 0.7924917340278625\n",
      "Epoch 4027/10000, Training Loss: 1.2424582242965698, Validation Loss: 0.9635510444641113\n",
      "Epoch 4028/10000, Training Loss: 1.1173017024993896, Validation Loss: 1.7953577041625977\n",
      "Epoch 4029/10000, Training Loss: 1.1323386430740356, Validation Loss: 1.0816923379898071\n",
      "Epoch 4030/10000, Training Loss: 1.4610778093338013, Validation Loss: 0.8701053261756897\n",
      "Epoch 4031/10000, Training Loss: 1.5354676246643066, Validation Loss: 1.6554979085922241\n",
      "Epoch 4032/10000, Training Loss: 1.2622381448745728, Validation Loss: 1.0665382146835327\n",
      "Epoch 4033/10000, Training Loss: 1.6266049146652222, Validation Loss: 1.8359454870224\n",
      "Epoch 4034/10000, Training Loss: 1.113995909690857, Validation Loss: 2.493112564086914\n",
      "Epoch 4035/10000, Training Loss: 1.4866021871566772, Validation Loss: 3.7339394092559814\n",
      "Epoch 4036/10000, Training Loss: 1.2979680299758911, Validation Loss: 1.7661563158035278\n",
      "Epoch 4037/10000, Training Loss: 1.1730256080627441, Validation Loss: 0.7263224720954895\n",
      "Epoch 4038/10000, Training Loss: 1.1349196434020996, Validation Loss: 0.6664111018180847\n",
      "Epoch 4039/10000, Training Loss: 1.3002848625183105, Validation Loss: 1.6807724237442017\n",
      "Epoch 4040/10000, Training Loss: 1.0239911079406738, Validation Loss: 1.1141091585159302\n",
      "Epoch 4041/10000, Training Loss: 1.3181298971176147, Validation Loss: 2.0441782474517822\n",
      "Epoch 4042/10000, Training Loss: 1.1845406293869019, Validation Loss: 1.678329348564148\n",
      "Epoch 4043/10000, Training Loss: 1.3303476572036743, Validation Loss: 1.2603660821914673\n",
      "Epoch 4044/10000, Training Loss: 1.353721022605896, Validation Loss: 2.9228477478027344\n",
      "Epoch 4045/10000, Training Loss: 1.317613124847412, Validation Loss: 2.349478006362915\n",
      "Epoch 4046/10000, Training Loss: 1.2041653394699097, Validation Loss: 1.7377065420150757\n",
      "Epoch 4047/10000, Training Loss: 1.269197940826416, Validation Loss: 1.8525046110153198\n",
      "Epoch 4048/10000, Training Loss: 1.020528793334961, Validation Loss: 1.7137094736099243\n",
      "Epoch 4049/10000, Training Loss: 1.0479822158813477, Validation Loss: 1.177817702293396\n",
      "Epoch 4050/10000, Training Loss: 1.3195164203643799, Validation Loss: 1.5289913415908813\n",
      "Epoch 4051/10000, Training Loss: 1.3666386604309082, Validation Loss: 1.8028043508529663\n",
      "Epoch 4052/10000, Training Loss: 1.496778964996338, Validation Loss: 0.7071871757507324\n",
      "Epoch 4053/10000, Training Loss: 1.1719528436660767, Validation Loss: 1.6161192655563354\n",
      "Epoch 4054/10000, Training Loss: 1.2149745225906372, Validation Loss: 0.9003362655639648\n",
      "Epoch 4055/10000, Training Loss: 1.286521077156067, Validation Loss: 1.6668349504470825\n",
      "Epoch 4056/10000, Training Loss: 1.7988865375518799, Validation Loss: 1.5891375541687012\n",
      "Epoch 4057/10000, Training Loss: 1.3955748081207275, Validation Loss: 1.0251657962799072\n",
      "Epoch 4058/10000, Training Loss: 1.0138593912124634, Validation Loss: 1.0993667840957642\n",
      "Epoch 4059/10000, Training Loss: 1.2292762994766235, Validation Loss: 0.7488253712654114\n",
      "Epoch 4060/10000, Training Loss: 1.3877009153366089, Validation Loss: 2.193866491317749\n",
      "Epoch 4061/10000, Training Loss: 1.0608675479888916, Validation Loss: 1.4607939720153809\n",
      "Epoch 4062/10000, Training Loss: 1.5480821132659912, Validation Loss: 2.6068367958068848\n",
      "Epoch 4063/10000, Training Loss: 0.9032257795333862, Validation Loss: 1.3877077102661133\n",
      "Epoch 4064/10000, Training Loss: 1.3797026872634888, Validation Loss: 1.2448877096176147\n",
      "Epoch 4065/10000, Training Loss: 1.2524899244308472, Validation Loss: 1.3248811960220337\n",
      "Epoch 4066/10000, Training Loss: 1.7342950105667114, Validation Loss: 0.9851037859916687\n",
      "Epoch 4067/10000, Training Loss: 0.9813368916511536, Validation Loss: 1.8943620920181274\n",
      "Epoch 4068/10000, Training Loss: 1.6062853336334229, Validation Loss: 1.5112628936767578\n",
      "Epoch 4069/10000, Training Loss: 1.5666499137878418, Validation Loss: 0.833566427230835\n",
      "Epoch 4070/10000, Training Loss: 1.3835365772247314, Validation Loss: 1.2000294923782349\n",
      "Epoch 4071/10000, Training Loss: 1.164654016494751, Validation Loss: 2.6915464401245117\n",
      "Epoch 4072/10000, Training Loss: 1.3045397996902466, Validation Loss: 1.1423945426940918\n",
      "Epoch 4073/10000, Training Loss: 1.109619379043579, Validation Loss: 0.959469735622406\n",
      "Epoch 4074/10000, Training Loss: 1.0542638301849365, Validation Loss: 0.6107168197631836\n",
      "Epoch 4075/10000, Training Loss: 1.2856916189193726, Validation Loss: 0.9044535756111145\n",
      "Epoch 4076/10000, Training Loss: 1.4571681022644043, Validation Loss: 1.8399277925491333\n",
      "Epoch 4077/10000, Training Loss: 1.3014819622039795, Validation Loss: 2.444610834121704\n",
      "Epoch 4078/10000, Training Loss: 1.4504802227020264, Validation Loss: 0.665614664554596\n",
      "Epoch 4079/10000, Training Loss: 0.9802451729774475, Validation Loss: 2.554189920425415\n",
      "Epoch 4080/10000, Training Loss: 1.0115559101104736, Validation Loss: 1.7616100311279297\n",
      "Epoch 4081/10000, Training Loss: 0.9827661514282227, Validation Loss: 0.6040235161781311\n",
      "Epoch 4082/10000, Training Loss: 1.2244813442230225, Validation Loss: 2.0329973697662354\n",
      "Epoch 4083/10000, Training Loss: 1.0942893028259277, Validation Loss: 0.5241196751594543\n",
      "Epoch 4084/10000, Training Loss: 1.7669404745101929, Validation Loss: 1.6067298650741577\n",
      "Epoch 4085/10000, Training Loss: 1.0712049007415771, Validation Loss: 0.7939916253089905\n",
      "Epoch 4086/10000, Training Loss: 1.6224746704101562, Validation Loss: 1.5427217483520508\n",
      "Epoch 4087/10000, Training Loss: 1.0110938549041748, Validation Loss: 1.1245354413986206\n",
      "Epoch 4088/10000, Training Loss: 1.291663646697998, Validation Loss: 1.8659471273422241\n",
      "Epoch 4089/10000, Training Loss: 0.8934608697891235, Validation Loss: 0.7540342807769775\n",
      "Epoch 4090/10000, Training Loss: 1.1815850734710693, Validation Loss: 1.256474256515503\n",
      "Epoch 4091/10000, Training Loss: 1.0396831035614014, Validation Loss: 0.9456455111503601\n",
      "Epoch 4092/10000, Training Loss: 1.2417643070220947, Validation Loss: 2.201110363006592\n",
      "Epoch 4093/10000, Training Loss: 1.0179033279418945, Validation Loss: 2.526475667953491\n",
      "Epoch 4094/10000, Training Loss: 1.1590641736984253, Validation Loss: 1.4092079401016235\n",
      "Epoch 4095/10000, Training Loss: 1.0993015766143799, Validation Loss: 0.9213847517967224\n",
      "Epoch 4096/10000, Training Loss: 1.2426375150680542, Validation Loss: 2.154700994491577\n",
      "Epoch 4097/10000, Training Loss: 1.3990737199783325, Validation Loss: 1.8292189836502075\n",
      "Epoch 4098/10000, Training Loss: 1.2453227043151855, Validation Loss: 1.998477578163147\n",
      "Epoch 4099/10000, Training Loss: 1.4187755584716797, Validation Loss: 1.738951325416565\n",
      "Epoch 4100/10000, Training Loss: 1.2776010036468506, Validation Loss: 1.2734336853027344\n",
      "Epoch 4101/10000, Training Loss: 1.1568220853805542, Validation Loss: 2.12937331199646\n",
      "Epoch 4102/10000, Training Loss: 1.2462342977523804, Validation Loss: 0.8784346580505371\n",
      "Epoch 4103/10000, Training Loss: 1.1074155569076538, Validation Loss: 2.5385732650756836\n",
      "Epoch 4104/10000, Training Loss: 1.158434271812439, Validation Loss: 1.0690149068832397\n",
      "Epoch 4105/10000, Training Loss: 0.9598357081413269, Validation Loss: 0.5625982880592346\n",
      "Epoch 4106/10000, Training Loss: 1.3231526613235474, Validation Loss: 2.2412614822387695\n",
      "Epoch 4107/10000, Training Loss: 1.2697468996047974, Validation Loss: 1.5843849182128906\n",
      "Epoch 4108/10000, Training Loss: 0.9812116622924805, Validation Loss: 1.3960040807724\n",
      "Epoch 4109/10000, Training Loss: 0.8695124387741089, Validation Loss: 0.9849449992179871\n",
      "Epoch 4110/10000, Training Loss: 1.327555537223816, Validation Loss: 1.7027000188827515\n",
      "Epoch 4111/10000, Training Loss: 1.2873724699020386, Validation Loss: 1.3440619707107544\n",
      "Epoch 4112/10000, Training Loss: 1.3624610900878906, Validation Loss: 1.3161641359329224\n",
      "Epoch 4113/10000, Training Loss: 1.3838914632797241, Validation Loss: 1.2107542753219604\n",
      "Epoch 4114/10000, Training Loss: 1.0329807996749878, Validation Loss: 2.162810802459717\n",
      "Epoch 4115/10000, Training Loss: 1.3536720275878906, Validation Loss: 1.7448352575302124\n",
      "Epoch 4116/10000, Training Loss: 1.589894413948059, Validation Loss: 0.5128077864646912\n",
      "Epoch 4117/10000, Training Loss: 1.0413752794265747, Validation Loss: 1.9316786527633667\n",
      "Epoch 4118/10000, Training Loss: 1.1359100341796875, Validation Loss: 1.8401237726211548\n",
      "Epoch 4119/10000, Training Loss: 1.1433775424957275, Validation Loss: 1.006960391998291\n",
      "Epoch 4120/10000, Training Loss: 1.200661301612854, Validation Loss: 1.2104945182800293\n",
      "Epoch 4121/10000, Training Loss: 1.1356524229049683, Validation Loss: 1.4384708404541016\n",
      "Epoch 4122/10000, Training Loss: 1.1544525623321533, Validation Loss: 0.7739205360412598\n",
      "Epoch 4123/10000, Training Loss: 1.1029798984527588, Validation Loss: 1.2535496950149536\n",
      "Epoch 4124/10000, Training Loss: 1.1896941661834717, Validation Loss: 0.5290154814720154\n",
      "Epoch 4125/10000, Training Loss: 1.0534119606018066, Validation Loss: 2.455988883972168\n",
      "Epoch 4126/10000, Training Loss: 1.0635435581207275, Validation Loss: 1.4064903259277344\n",
      "Epoch 4127/10000, Training Loss: 1.1033045053482056, Validation Loss: 0.6624680161476135\n",
      "Epoch 4128/10000, Training Loss: 1.070703387260437, Validation Loss: 2.0726773738861084\n",
      "Epoch 4129/10000, Training Loss: 0.9492795467376709, Validation Loss: 0.8946514129638672\n",
      "Epoch 4130/10000, Training Loss: 1.1209633350372314, Validation Loss: 1.604382872581482\n",
      "Epoch 4131/10000, Training Loss: 0.9765356779098511, Validation Loss: 1.2107750177383423\n",
      "Epoch 4132/10000, Training Loss: 1.372848629951477, Validation Loss: 1.5439997911453247\n",
      "Epoch 4133/10000, Training Loss: 1.2162667512893677, Validation Loss: 1.2988187074661255\n",
      "Epoch 4134/10000, Training Loss: 1.4116666316986084, Validation Loss: 3.4370346069335938\n",
      "Epoch 4135/10000, Training Loss: 1.2554365396499634, Validation Loss: 0.8954094052314758\n",
      "Epoch 4136/10000, Training Loss: 1.012328028678894, Validation Loss: 0.9158965945243835\n",
      "Epoch 4137/10000, Training Loss: 1.2836984395980835, Validation Loss: 2.371283769607544\n",
      "Epoch 4138/10000, Training Loss: 1.0324937105178833, Validation Loss: 1.1266322135925293\n",
      "Epoch 4139/10000, Training Loss: 1.1731914281845093, Validation Loss: 2.0271880626678467\n",
      "Epoch 4140/10000, Training Loss: 1.2553343772888184, Validation Loss: 0.7802990078926086\n",
      "Epoch 4141/10000, Training Loss: 1.2524700164794922, Validation Loss: 0.5183585286140442\n",
      "Epoch 4142/10000, Training Loss: 1.1892507076263428, Validation Loss: 1.3228627443313599\n",
      "Epoch 4143/10000, Training Loss: 1.0803086757659912, Validation Loss: 0.5648862719535828\n",
      "Epoch 4144/10000, Training Loss: 1.112968921661377, Validation Loss: 1.894982933998108\n",
      "Epoch 4145/10000, Training Loss: 0.9446153044700623, Validation Loss: 0.3179011046886444\n",
      "Epoch 4146/10000, Training Loss: 1.0005345344543457, Validation Loss: 1.938233733177185\n",
      "Epoch 4147/10000, Training Loss: 1.1333129405975342, Validation Loss: 0.8744552731513977\n",
      "Epoch 4148/10000, Training Loss: 0.8965839743614197, Validation Loss: 1.3626790046691895\n",
      "Epoch 4149/10000, Training Loss: 1.065490961074829, Validation Loss: 0.9844537377357483\n",
      "Epoch 4150/10000, Training Loss: 1.581071376800537, Validation Loss: 0.5484282374382019\n",
      "Epoch 4151/10000, Training Loss: 1.118942379951477, Validation Loss: 1.0968207120895386\n",
      "Epoch 4152/10000, Training Loss: 0.8362869024276733, Validation Loss: 1.7178090810775757\n",
      "Epoch 4153/10000, Training Loss: 1.2657614946365356, Validation Loss: 1.045877456665039\n",
      "Epoch 4154/10000, Training Loss: 1.2091081142425537, Validation Loss: 0.9573934674263\n",
      "Epoch 4155/10000, Training Loss: 1.1783291101455688, Validation Loss: 1.9587959051132202\n",
      "Epoch 4156/10000, Training Loss: 1.3211137056350708, Validation Loss: 1.3417863845825195\n",
      "Epoch 4157/10000, Training Loss: 1.664853572845459, Validation Loss: 1.3106175661087036\n",
      "Epoch 4158/10000, Training Loss: 1.3428107500076294, Validation Loss: 1.7087897062301636\n",
      "Epoch 4159/10000, Training Loss: 1.0821623802185059, Validation Loss: 0.9265129566192627\n",
      "Epoch 4160/10000, Training Loss: 1.319571852684021, Validation Loss: 1.2302662134170532\n",
      "Epoch 4161/10000, Training Loss: 1.8798846006393433, Validation Loss: 2.473236083984375\n",
      "Epoch 4162/10000, Training Loss: 1.178433895111084, Validation Loss: 1.1332495212554932\n",
      "Epoch 4163/10000, Training Loss: 1.2867686748504639, Validation Loss: 0.9556366801261902\n",
      "Epoch 4164/10000, Training Loss: 1.020623803138733, Validation Loss: 1.0795438289642334\n",
      "Epoch 4165/10000, Training Loss: 1.1009554862976074, Validation Loss: 2.1929917335510254\n",
      "Epoch 4166/10000, Training Loss: 0.9831267595291138, Validation Loss: 1.199679970741272\n",
      "Epoch 4167/10000, Training Loss: 1.07602858543396, Validation Loss: 1.0697795152664185\n",
      "Epoch 4168/10000, Training Loss: 0.8687152862548828, Validation Loss: 1.3564518690109253\n",
      "Epoch 4169/10000, Training Loss: 1.1915984153747559, Validation Loss: 1.4873393774032593\n",
      "Epoch 4170/10000, Training Loss: 1.1376596689224243, Validation Loss: 0.8318953514099121\n",
      "Epoch 4171/10000, Training Loss: 1.223924160003662, Validation Loss: 1.5919955968856812\n",
      "Epoch 4172/10000, Training Loss: 1.2875927686691284, Validation Loss: 1.2063149213790894\n",
      "Epoch 4173/10000, Training Loss: 1.040247917175293, Validation Loss: 2.431889295578003\n",
      "Epoch 4174/10000, Training Loss: 1.0820728540420532, Validation Loss: 1.3588112592697144\n",
      "Epoch 4175/10000, Training Loss: 1.1334810256958008, Validation Loss: 0.8097168803215027\n",
      "Epoch 4176/10000, Training Loss: 1.061527967453003, Validation Loss: 1.6122498512268066\n",
      "Epoch 4177/10000, Training Loss: 1.063547134399414, Validation Loss: 1.0756405591964722\n",
      "Epoch 4178/10000, Training Loss: 1.2290239334106445, Validation Loss: 1.836621880531311\n",
      "Epoch 4179/10000, Training Loss: 1.1383731365203857, Validation Loss: 2.478440046310425\n",
      "Epoch 4180/10000, Training Loss: 1.0788521766662598, Validation Loss: 1.039754033088684\n",
      "Epoch 4181/10000, Training Loss: 0.9653237462043762, Validation Loss: 1.428800106048584\n",
      "Epoch 4182/10000, Training Loss: 1.16847825050354, Validation Loss: 0.809368908405304\n",
      "Epoch 4183/10000, Training Loss: 1.0881050825119019, Validation Loss: 1.9846404790878296\n",
      "Epoch 4184/10000, Training Loss: 0.9229182004928589, Validation Loss: 1.194118857383728\n",
      "Epoch 4185/10000, Training Loss: 1.1262943744659424, Validation Loss: 1.5266284942626953\n",
      "Epoch 4186/10000, Training Loss: 1.1086748838424683, Validation Loss: 2.710437536239624\n",
      "Epoch 4187/10000, Training Loss: 1.0750542879104614, Validation Loss: 0.5908949971199036\n",
      "Epoch 4188/10000, Training Loss: 1.2022204399108887, Validation Loss: 1.3067461252212524\n",
      "Epoch 4189/10000, Training Loss: 1.1278202533721924, Validation Loss: 0.9685819149017334\n",
      "Epoch 4190/10000, Training Loss: 1.1512984037399292, Validation Loss: 0.7585332989692688\n",
      "Epoch 4191/10000, Training Loss: 1.4484200477600098, Validation Loss: 1.6906566619873047\n",
      "Epoch 4192/10000, Training Loss: 0.9033691883087158, Validation Loss: 1.308719515800476\n",
      "Epoch 4193/10000, Training Loss: 0.9475430250167847, Validation Loss: 1.699415683746338\n",
      "Epoch 4194/10000, Training Loss: 0.9734667539596558, Validation Loss: 0.7974052429199219\n",
      "Epoch 4195/10000, Training Loss: 1.014933705329895, Validation Loss: 0.6426054239273071\n",
      "Epoch 4196/10000, Training Loss: 1.0229616165161133, Validation Loss: 1.2272230386734009\n",
      "Epoch 4197/10000, Training Loss: 1.1835081577301025, Validation Loss: 0.9679471850395203\n",
      "Epoch 4198/10000, Training Loss: 1.098481297492981, Validation Loss: 1.317521095275879\n",
      "Epoch 4199/10000, Training Loss: 0.9111645221710205, Validation Loss: 0.42335668206214905\n",
      "Epoch 4200/10000, Training Loss: 1.0778266191482544, Validation Loss: 1.4575538635253906\n",
      "Epoch 4201/10000, Training Loss: 1.4039137363433838, Validation Loss: 1.9979225397109985\n",
      "Epoch 4202/10000, Training Loss: 0.9952837228775024, Validation Loss: 1.0843373537063599\n",
      "Epoch 4203/10000, Training Loss: 1.0583765506744385, Validation Loss: 0.5258944630622864\n",
      "Epoch 4204/10000, Training Loss: 1.1811819076538086, Validation Loss: 0.6216462850570679\n",
      "Epoch 4205/10000, Training Loss: 1.2853748798370361, Validation Loss: 1.2556854486465454\n",
      "Epoch 4206/10000, Training Loss: 1.3424371480941772, Validation Loss: 1.4823652505874634\n",
      "Epoch 4207/10000, Training Loss: 1.1468085050582886, Validation Loss: 2.1167008876800537\n",
      "Epoch 4208/10000, Training Loss: 0.988959550857544, Validation Loss: 0.5984241366386414\n",
      "Epoch 4209/10000, Training Loss: 1.0491681098937988, Validation Loss: 1.0158485174179077\n",
      "Epoch 4210/10000, Training Loss: 1.114790678024292, Validation Loss: 1.4245620965957642\n",
      "Epoch 4211/10000, Training Loss: 1.2271449565887451, Validation Loss: 2.267812490463257\n",
      "Epoch 4212/10000, Training Loss: 1.0690302848815918, Validation Loss: 0.8679654598236084\n",
      "Epoch 4213/10000, Training Loss: 1.063082218170166, Validation Loss: 1.3227185010910034\n",
      "Epoch 4214/10000, Training Loss: 0.9371971487998962, Validation Loss: 1.8491787910461426\n",
      "Epoch 4215/10000, Training Loss: 1.3260356187820435, Validation Loss: 0.4417557418346405\n",
      "Epoch 4216/10000, Training Loss: 0.7743517756462097, Validation Loss: 0.7854456901550293\n",
      "Epoch 4217/10000, Training Loss: 0.9105635285377502, Validation Loss: 2.4354665279388428\n",
      "Epoch 4218/10000, Training Loss: 1.1002625226974487, Validation Loss: 1.0725709199905396\n",
      "Epoch 4219/10000, Training Loss: 1.2314238548278809, Validation Loss: 1.621775507926941\n",
      "Epoch 4220/10000, Training Loss: 0.9731663465499878, Validation Loss: 0.7710481286048889\n",
      "Epoch 4221/10000, Training Loss: 0.8948503732681274, Validation Loss: 0.8813750743865967\n",
      "Epoch 4222/10000, Training Loss: 0.8602167367935181, Validation Loss: 1.0654025077819824\n",
      "Epoch 4223/10000, Training Loss: 2.0055220127105713, Validation Loss: 2.6642472743988037\n",
      "Epoch 4224/10000, Training Loss: 1.0152995586395264, Validation Loss: 0.7817080020904541\n",
      "Epoch 4225/10000, Training Loss: 0.9854601621627808, Validation Loss: 1.2201396226882935\n",
      "Epoch 4226/10000, Training Loss: 0.8891632556915283, Validation Loss: 1.3443468809127808\n",
      "Epoch 4227/10000, Training Loss: 1.0142741203308105, Validation Loss: 2.016580820083618\n",
      "Epoch 4228/10000, Training Loss: 1.2431565523147583, Validation Loss: 1.4855523109436035\n",
      "Epoch 4229/10000, Training Loss: 1.071279764175415, Validation Loss: 0.9330331683158875\n",
      "Epoch 4230/10000, Training Loss: 1.098953366279602, Validation Loss: 1.441412329673767\n",
      "Epoch 4231/10000, Training Loss: 1.2854995727539062, Validation Loss: 2.2211380004882812\n",
      "Epoch 4232/10000, Training Loss: 1.0369842052459717, Validation Loss: 0.6517539620399475\n",
      "Epoch 4233/10000, Training Loss: 1.0338776111602783, Validation Loss: 0.7687166333198547\n",
      "Epoch 4234/10000, Training Loss: 1.13076651096344, Validation Loss: 0.937396764755249\n",
      "Epoch 4235/10000, Training Loss: 1.3641902208328247, Validation Loss: 1.1234840154647827\n",
      "Epoch 4236/10000, Training Loss: 1.088983178138733, Validation Loss: 1.2028436660766602\n",
      "Epoch 4237/10000, Training Loss: 1.018533706665039, Validation Loss: 1.013205885887146\n",
      "Epoch 4238/10000, Training Loss: 1.0120126008987427, Validation Loss: 1.3654974699020386\n",
      "Epoch 4239/10000, Training Loss: 0.9669607877731323, Validation Loss: 1.354434847831726\n",
      "Epoch 4240/10000, Training Loss: 0.7871895432472229, Validation Loss: 1.571078896522522\n",
      "Epoch 4241/10000, Training Loss: 0.9890526533126831, Validation Loss: 1.5390952825546265\n",
      "Epoch 4242/10000, Training Loss: 0.9708706736564636, Validation Loss: 2.2579586505889893\n",
      "Epoch 4243/10000, Training Loss: 1.1878002882003784, Validation Loss: 0.814267098903656\n",
      "Epoch 4244/10000, Training Loss: 1.2064036130905151, Validation Loss: 1.7402719259262085\n",
      "Epoch 4245/10000, Training Loss: 1.0699255466461182, Validation Loss: 0.9871378540992737\n",
      "Epoch 4246/10000, Training Loss: 1.2527146339416504, Validation Loss: 1.706679344177246\n",
      "Epoch 4247/10000, Training Loss: 1.1336116790771484, Validation Loss: 0.9028382897377014\n",
      "Epoch 4248/10000, Training Loss: 1.0797456502914429, Validation Loss: 1.4374923706054688\n",
      "Epoch 4249/10000, Training Loss: 1.0392829179763794, Validation Loss: 0.8037266135215759\n",
      "Epoch 4250/10000, Training Loss: 1.0562011003494263, Validation Loss: 1.6185778379440308\n",
      "Epoch 4251/10000, Training Loss: 1.199411153793335, Validation Loss: 1.4578185081481934\n",
      "Epoch 4252/10000, Training Loss: 1.01907217502594, Validation Loss: 1.4289684295654297\n",
      "Epoch 4253/10000, Training Loss: 0.7736945748329163, Validation Loss: 0.7817865014076233\n",
      "Epoch 4254/10000, Training Loss: 0.9301507472991943, Validation Loss: 1.3314615488052368\n",
      "Epoch 4255/10000, Training Loss: 0.8995226621627808, Validation Loss: 0.6975368857383728\n",
      "Epoch 4256/10000, Training Loss: 1.0664808750152588, Validation Loss: 0.546544075012207\n",
      "Epoch 4257/10000, Training Loss: 1.0004764795303345, Validation Loss: 0.9592376351356506\n",
      "Epoch 4258/10000, Training Loss: 0.9871193766593933, Validation Loss: 1.3648418188095093\n",
      "Epoch 4259/10000, Training Loss: 0.9906828999519348, Validation Loss: 1.045864224433899\n",
      "Epoch 4260/10000, Training Loss: 0.9905661344528198, Validation Loss: 1.0399141311645508\n",
      "Epoch 4261/10000, Training Loss: 1.1606286764144897, Validation Loss: 0.49278607964515686\n",
      "Epoch 4262/10000, Training Loss: 1.1742573976516724, Validation Loss: 0.9323630332946777\n",
      "Epoch 4263/10000, Training Loss: 1.0891542434692383, Validation Loss: 0.7879764437675476\n",
      "Epoch 4264/10000, Training Loss: 1.139567255973816, Validation Loss: 1.0929354429244995\n",
      "Epoch 4265/10000, Training Loss: 1.3106366395950317, Validation Loss: 1.2429343461990356\n",
      "Epoch 4266/10000, Training Loss: 1.1444822549819946, Validation Loss: 1.3968440294265747\n",
      "Epoch 4267/10000, Training Loss: 1.0202988386154175, Validation Loss: 1.7854318618774414\n",
      "Epoch 4268/10000, Training Loss: 0.9716395139694214, Validation Loss: 0.6939919590950012\n",
      "Epoch 4269/10000, Training Loss: 0.9184982776641846, Validation Loss: 1.6115409135818481\n",
      "Epoch 4270/10000, Training Loss: 1.1798584461212158, Validation Loss: 1.2425895929336548\n",
      "Epoch 4271/10000, Training Loss: 0.9333343505859375, Validation Loss: 0.8060102462768555\n",
      "Epoch 4272/10000, Training Loss: 0.9404053092002869, Validation Loss: 0.9334740042686462\n",
      "Epoch 4273/10000, Training Loss: 1.107237458229065, Validation Loss: 2.093086004257202\n",
      "Epoch 4274/10000, Training Loss: 1.0121384859085083, Validation Loss: 1.1272341012954712\n",
      "Epoch 4275/10000, Training Loss: 0.9603856205940247, Validation Loss: 0.8378859162330627\n",
      "Epoch 4276/10000, Training Loss: 0.8903955221176147, Validation Loss: 1.4434680938720703\n",
      "Epoch 4277/10000, Training Loss: 1.0601431131362915, Validation Loss: 1.3379408121109009\n",
      "Epoch 4278/10000, Training Loss: 0.9092792868614197, Validation Loss: 1.7293680906295776\n",
      "Epoch 4279/10000, Training Loss: 1.0940454006195068, Validation Loss: 2.0522708892822266\n",
      "Epoch 4280/10000, Training Loss: 0.8568236827850342, Validation Loss: 0.7051423192024231\n",
      "Epoch 4281/10000, Training Loss: 1.0838207006454468, Validation Loss: 1.7144569158554077\n",
      "Epoch 4282/10000, Training Loss: 1.0071423053741455, Validation Loss: 1.1108856201171875\n",
      "Epoch 4283/10000, Training Loss: 1.022271990776062, Validation Loss: 1.5643399953842163\n",
      "Epoch 4284/10000, Training Loss: 1.0305508375167847, Validation Loss: 1.800967812538147\n",
      "Epoch 4285/10000, Training Loss: 1.0881259441375732, Validation Loss: 0.885685920715332\n",
      "Epoch 4286/10000, Training Loss: 1.1444758176803589, Validation Loss: 2.6731297969818115\n",
      "Epoch 4287/10000, Training Loss: 0.9116845726966858, Validation Loss: 1.128893494606018\n",
      "Epoch 4288/10000, Training Loss: 1.1200562715530396, Validation Loss: 1.7533372640609741\n",
      "Epoch 4289/10000, Training Loss: 0.8943207859992981, Validation Loss: 1.4972325563430786\n",
      "Epoch 4290/10000, Training Loss: 0.9711622595787048, Validation Loss: 1.3857930898666382\n",
      "Epoch 4291/10000, Training Loss: 0.8274024128913879, Validation Loss: 0.881089448928833\n",
      "Epoch 4292/10000, Training Loss: 1.025334119796753, Validation Loss: 1.2583340406417847\n",
      "Epoch 4293/10000, Training Loss: 0.9510615468025208, Validation Loss: 1.000632405281067\n",
      "Epoch 4294/10000, Training Loss: 1.4179761409759521, Validation Loss: 1.7264574766159058\n",
      "Epoch 4295/10000, Training Loss: 1.1126002073287964, Validation Loss: 0.7489926218986511\n",
      "Epoch 4296/10000, Training Loss: 0.8791382908821106, Validation Loss: 1.6336101293563843\n",
      "Epoch 4297/10000, Training Loss: 0.8998213410377502, Validation Loss: 1.7795900106430054\n",
      "Epoch 4298/10000, Training Loss: 0.8025391101837158, Validation Loss: 0.762974739074707\n",
      "Epoch 4299/10000, Training Loss: 1.2815648317337036, Validation Loss: 1.5478202104568481\n",
      "Epoch 4300/10000, Training Loss: 1.2270112037658691, Validation Loss: 0.7887201309204102\n",
      "Epoch 4301/10000, Training Loss: 1.0250129699707031, Validation Loss: 0.9537165760993958\n",
      "Epoch 4302/10000, Training Loss: 1.018693208694458, Validation Loss: 2.6536977291107178\n",
      "Epoch 4303/10000, Training Loss: 0.9837085008621216, Validation Loss: 1.4350093603134155\n",
      "Epoch 4304/10000, Training Loss: 1.078514575958252, Validation Loss: 1.0396770238876343\n",
      "Epoch 4305/10000, Training Loss: 1.2313863039016724, Validation Loss: 1.312147617340088\n",
      "Epoch 4306/10000, Training Loss: 0.9737417697906494, Validation Loss: 1.1034389734268188\n",
      "Epoch 4307/10000, Training Loss: 0.9310789108276367, Validation Loss: 1.6150212287902832\n",
      "Epoch 4308/10000, Training Loss: 1.2694358825683594, Validation Loss: 1.7757782936096191\n",
      "Epoch 4309/10000, Training Loss: 1.0659903287887573, Validation Loss: 0.574948787689209\n",
      "Epoch 4310/10000, Training Loss: 0.9543420076370239, Validation Loss: 1.9465608596801758\n",
      "Epoch 4311/10000, Training Loss: 1.0093070268630981, Validation Loss: 0.996155321598053\n",
      "Epoch 4312/10000, Training Loss: 1.0140491724014282, Validation Loss: 0.7718112468719482\n",
      "Epoch 4313/10000, Training Loss: 1.1035786867141724, Validation Loss: 1.4443359375\n",
      "Epoch 4314/10000, Training Loss: 1.142153024673462, Validation Loss: 1.311261773109436\n",
      "Epoch 4315/10000, Training Loss: 1.007786512374878, Validation Loss: 1.7652589082717896\n",
      "Epoch 4316/10000, Training Loss: 1.0741323232650757, Validation Loss: 2.099740505218506\n",
      "Epoch 4317/10000, Training Loss: 1.1310811042785645, Validation Loss: 1.0514103174209595\n",
      "Epoch 4318/10000, Training Loss: 0.9741265177726746, Validation Loss: 1.4934091567993164\n",
      "Epoch 4319/10000, Training Loss: 0.9376835227012634, Validation Loss: 1.325007438659668\n",
      "Epoch 4320/10000, Training Loss: 0.9010814428329468, Validation Loss: 1.4080690145492554\n",
      "Epoch 4321/10000, Training Loss: 1.1035059690475464, Validation Loss: 0.6981525421142578\n",
      "Epoch 4322/10000, Training Loss: 1.0297255516052246, Validation Loss: 1.2521954774856567\n",
      "Epoch 4323/10000, Training Loss: 0.9964339733123779, Validation Loss: 0.8126981258392334\n",
      "Epoch 4324/10000, Training Loss: 0.8884152173995972, Validation Loss: 0.9690403938293457\n",
      "Epoch 4325/10000, Training Loss: 1.0073553323745728, Validation Loss: 1.1097263097763062\n",
      "Epoch 4326/10000, Training Loss: 1.086445689201355, Validation Loss: 0.7817322611808777\n",
      "Epoch 4327/10000, Training Loss: 1.1034486293792725, Validation Loss: 1.24390709400177\n",
      "Epoch 4328/10000, Training Loss: 0.9414818286895752, Validation Loss: 0.9051987528800964\n",
      "Epoch 4329/10000, Training Loss: 0.9749772548675537, Validation Loss: 1.1570535898208618\n",
      "Epoch 4330/10000, Training Loss: 0.8847039937973022, Validation Loss: 1.818125605583191\n",
      "Epoch 4331/10000, Training Loss: 1.2017072439193726, Validation Loss: 2.0172183513641357\n",
      "Epoch 4332/10000, Training Loss: 0.9370146989822388, Validation Loss: 1.168948769569397\n",
      "Epoch 4333/10000, Training Loss: 1.0480295419692993, Validation Loss: 0.9293933510780334\n",
      "Epoch 4334/10000, Training Loss: 1.038801670074463, Validation Loss: 0.9487121105194092\n",
      "Epoch 4335/10000, Training Loss: 0.932722806930542, Validation Loss: 0.9011380672454834\n",
      "Epoch 4336/10000, Training Loss: 1.231874704360962, Validation Loss: 1.1856966018676758\n",
      "Epoch 4337/10000, Training Loss: 1.0031962394714355, Validation Loss: 1.180429220199585\n",
      "Epoch 4338/10000, Training Loss: 1.0326039791107178, Validation Loss: 1.6017440557479858\n",
      "Epoch 4339/10000, Training Loss: 0.9900842905044556, Validation Loss: 1.035595178604126\n",
      "Epoch 4340/10000, Training Loss: 1.0127778053283691, Validation Loss: 0.999000072479248\n",
      "Epoch 4341/10000, Training Loss: 1.2208787202835083, Validation Loss: 0.587448239326477\n",
      "Epoch 4342/10000, Training Loss: 0.9630337953567505, Validation Loss: 0.844593346118927\n",
      "Epoch 4343/10000, Training Loss: 1.0966507196426392, Validation Loss: 1.913352608680725\n",
      "Epoch 4344/10000, Training Loss: 1.0123876333236694, Validation Loss: 1.2278141975402832\n",
      "Epoch 4345/10000, Training Loss: 1.0783791542053223, Validation Loss: 0.7943559288978577\n",
      "Epoch 4346/10000, Training Loss: 1.272080659866333, Validation Loss: 1.7356666326522827\n",
      "Epoch 4347/10000, Training Loss: 1.3235059976577759, Validation Loss: 1.8357354402542114\n",
      "Epoch 4348/10000, Training Loss: 1.0601404905319214, Validation Loss: 0.7468771934509277\n",
      "Epoch 4349/10000, Training Loss: 1.2071127891540527, Validation Loss: 0.8294973373413086\n",
      "Epoch 4350/10000, Training Loss: 1.057396411895752, Validation Loss: 1.5104604959487915\n",
      "Epoch 4351/10000, Training Loss: 1.29210364818573, Validation Loss: 1.6809147596359253\n",
      "Epoch 4352/10000, Training Loss: 0.9203071594238281, Validation Loss: 1.0613633394241333\n",
      "Epoch 4353/10000, Training Loss: 1.0430196523666382, Validation Loss: 1.786918044090271\n",
      "Epoch 4354/10000, Training Loss: 0.9944466948509216, Validation Loss: 0.7985789179801941\n",
      "Epoch 4355/10000, Training Loss: 0.8587992787361145, Validation Loss: 0.851810872554779\n",
      "Epoch 4356/10000, Training Loss: 1.333074688911438, Validation Loss: 1.7461438179016113\n",
      "Epoch 4357/10000, Training Loss: 1.0960102081298828, Validation Loss: 0.9006803631782532\n",
      "Epoch 4358/10000, Training Loss: 0.9491752982139587, Validation Loss: 0.7217717170715332\n",
      "Epoch 4359/10000, Training Loss: 1.009885549545288, Validation Loss: 1.2811392545700073\n",
      "Epoch 4360/10000, Training Loss: 1.2431905269622803, Validation Loss: 1.4942965507507324\n",
      "Epoch 4361/10000, Training Loss: 1.1130783557891846, Validation Loss: 1.022603988647461\n",
      "Epoch 4362/10000, Training Loss: 0.9625943303108215, Validation Loss: 2.589646577835083\n",
      "Epoch 4363/10000, Training Loss: 0.9810848236083984, Validation Loss: 1.6055339574813843\n",
      "Epoch 4364/10000, Training Loss: 1.0533446073532104, Validation Loss: 1.382879376411438\n",
      "Epoch 4365/10000, Training Loss: 1.0913543701171875, Validation Loss: 1.5342262983322144\n",
      "Epoch 4366/10000, Training Loss: 0.9195899367332458, Validation Loss: 3.2568538188934326\n",
      "Epoch 4367/10000, Training Loss: 0.8029381632804871, Validation Loss: 1.616123080253601\n",
      "Epoch 4368/10000, Training Loss: 1.0359750986099243, Validation Loss: 1.242172360420227\n",
      "Epoch 4369/10000, Training Loss: 0.9143730401992798, Validation Loss: 1.1057758331298828\n",
      "Epoch 4370/10000, Training Loss: 1.0388140678405762, Validation Loss: 0.9117233157157898\n",
      "Epoch 4371/10000, Training Loss: 0.8561407327651978, Validation Loss: 1.4171557426452637\n",
      "Epoch 4372/10000, Training Loss: 0.9258249998092651, Validation Loss: 1.6721230745315552\n",
      "Epoch 4373/10000, Training Loss: 0.8249359726905823, Validation Loss: 0.4848426282405853\n",
      "Epoch 4374/10000, Training Loss: 1.1438267230987549, Validation Loss: 1.7457084655761719\n",
      "Epoch 4375/10000, Training Loss: 0.9180735349655151, Validation Loss: 0.9786062836647034\n",
      "Epoch 4376/10000, Training Loss: 1.0444841384887695, Validation Loss: 1.0325775146484375\n",
      "Epoch 4377/10000, Training Loss: 1.0447142124176025, Validation Loss: 1.2813180685043335\n",
      "Epoch 4378/10000, Training Loss: 0.8520938158035278, Validation Loss: 1.989317774772644\n",
      "Epoch 4379/10000, Training Loss: 0.9286656975746155, Validation Loss: 1.3205283880233765\n",
      "Epoch 4380/10000, Training Loss: 1.0486310720443726, Validation Loss: 1.566163182258606\n",
      "Epoch 4381/10000, Training Loss: 0.854557991027832, Validation Loss: 0.6657965183258057\n",
      "Epoch 4382/10000, Training Loss: 0.9315239787101746, Validation Loss: 0.896668016910553\n",
      "Epoch 4383/10000, Training Loss: 0.793246865272522, Validation Loss: 1.2485971450805664\n",
      "Epoch 4384/10000, Training Loss: 1.0922126770019531, Validation Loss: 1.6212272644042969\n",
      "Epoch 4385/10000, Training Loss: 0.7108712792396545, Validation Loss: 0.5471104383468628\n",
      "Epoch 4386/10000, Training Loss: 1.1242687702178955, Validation Loss: 1.5714665651321411\n",
      "Epoch 4387/10000, Training Loss: 1.1780245304107666, Validation Loss: 1.0782638788223267\n",
      "Epoch 4388/10000, Training Loss: 0.9147378206253052, Validation Loss: 0.7298509478569031\n",
      "Epoch 4389/10000, Training Loss: 1.162867546081543, Validation Loss: 2.1393415927886963\n",
      "Epoch 4390/10000, Training Loss: 1.2669564485549927, Validation Loss: 1.3819798231124878\n",
      "Epoch 4391/10000, Training Loss: 1.0932908058166504, Validation Loss: 0.6633548140525818\n",
      "Epoch 4392/10000, Training Loss: 0.9664372801780701, Validation Loss: 0.5260586738586426\n",
      "Epoch 4393/10000, Training Loss: 1.0731455087661743, Validation Loss: 1.8600629568099976\n",
      "Epoch 4394/10000, Training Loss: 1.0512837171554565, Validation Loss: 1.146675944328308\n",
      "Epoch 4395/10000, Training Loss: 0.8390543460845947, Validation Loss: 1.2546015977859497\n",
      "Epoch 4396/10000, Training Loss: 1.1144441366195679, Validation Loss: 2.380993366241455\n",
      "Epoch 4397/10000, Training Loss: 0.8005579710006714, Validation Loss: 1.1360729932785034\n",
      "Epoch 4398/10000, Training Loss: 1.1780301332473755, Validation Loss: 1.8213014602661133\n",
      "Epoch 4399/10000, Training Loss: 1.0986744165420532, Validation Loss: 1.0847587585449219\n",
      "Epoch 4400/10000, Training Loss: 0.898248016834259, Validation Loss: 1.0060834884643555\n",
      "Epoch 4401/10000, Training Loss: 0.7969278693199158, Validation Loss: 1.122989296913147\n",
      "Epoch 4402/10000, Training Loss: 0.9990745782852173, Validation Loss: 0.4448249042034149\n",
      "Epoch 4403/10000, Training Loss: 0.916132390499115, Validation Loss: 1.835638165473938\n",
      "Epoch 4404/10000, Training Loss: 0.8719300031661987, Validation Loss: 1.2085890769958496\n",
      "Epoch 4405/10000, Training Loss: 1.2123606204986572, Validation Loss: 1.7410645484924316\n",
      "Epoch 4406/10000, Training Loss: 0.8181313276290894, Validation Loss: 0.5868871808052063\n",
      "Epoch 4407/10000, Training Loss: 0.9294406175613403, Validation Loss: 2.0831918716430664\n",
      "Epoch 4408/10000, Training Loss: 0.8316806554794312, Validation Loss: 1.1538501977920532\n",
      "Epoch 4409/10000, Training Loss: 0.9401209950447083, Validation Loss: 0.9340260028839111\n",
      "Epoch 4410/10000, Training Loss: 1.2726198434829712, Validation Loss: 1.1749053001403809\n",
      "Epoch 4411/10000, Training Loss: 1.006172776222229, Validation Loss: 1.0084253549575806\n",
      "Epoch 4412/10000, Training Loss: 0.8222252726554871, Validation Loss: 0.880070149898529\n",
      "Epoch 4413/10000, Training Loss: 1.1628565788269043, Validation Loss: 1.1386953592300415\n",
      "Epoch 4414/10000, Training Loss: 0.9697481393814087, Validation Loss: 1.7929037809371948\n",
      "Epoch 4415/10000, Training Loss: 0.8793624043464661, Validation Loss: 1.8456133604049683\n",
      "Epoch 4416/10000, Training Loss: 1.1364741325378418, Validation Loss: 1.3159465789794922\n",
      "Epoch 4417/10000, Training Loss: 0.8436142802238464, Validation Loss: 1.4009064435958862\n",
      "Epoch 4418/10000, Training Loss: 1.1358075141906738, Validation Loss: 1.3993163108825684\n",
      "Epoch 4419/10000, Training Loss: 1.0825936794281006, Validation Loss: 1.094032883644104\n",
      "Epoch 4420/10000, Training Loss: 1.0915915966033936, Validation Loss: 0.5852166414260864\n",
      "Epoch 4421/10000, Training Loss: 0.8526780009269714, Validation Loss: 0.6991860866546631\n",
      "Epoch 4422/10000, Training Loss: 0.8385806679725647, Validation Loss: 1.8909467458724976\n",
      "Epoch 4423/10000, Training Loss: 0.987052321434021, Validation Loss: 0.9926464557647705\n",
      "Epoch 4424/10000, Training Loss: 0.847903847694397, Validation Loss: 1.7023124694824219\n",
      "Epoch 4425/10000, Training Loss: 1.1444916725158691, Validation Loss: 1.5008983612060547\n",
      "Epoch 4426/10000, Training Loss: 0.8696855306625366, Validation Loss: 1.421712875366211\n",
      "Epoch 4427/10000, Training Loss: 0.9690781235694885, Validation Loss: 0.618891716003418\n",
      "Epoch 4428/10000, Training Loss: 0.9300733804702759, Validation Loss: 1.382454752922058\n",
      "Epoch 4429/10000, Training Loss: 1.1132956743240356, Validation Loss: 2.4511876106262207\n",
      "Epoch 4430/10000, Training Loss: 0.8582464456558228, Validation Loss: 0.66524338722229\n",
      "Epoch 4431/10000, Training Loss: 1.0436128377914429, Validation Loss: 0.9688384532928467\n",
      "Epoch 4432/10000, Training Loss: 0.9831138253211975, Validation Loss: 0.969879150390625\n",
      "Epoch 4433/10000, Training Loss: 0.8922985792160034, Validation Loss: 0.9379457831382751\n",
      "Epoch 4434/10000, Training Loss: 1.2671571969985962, Validation Loss: 1.662834644317627\n",
      "Epoch 4435/10000, Training Loss: 1.0243209600448608, Validation Loss: 1.5589207410812378\n",
      "Epoch 4436/10000, Training Loss: 0.9987502694129944, Validation Loss: 1.0576380491256714\n",
      "Epoch 4437/10000, Training Loss: 0.8690282106399536, Validation Loss: 1.0005162954330444\n",
      "Epoch 4438/10000, Training Loss: 0.8977734446525574, Validation Loss: 0.43984606862068176\n",
      "Epoch 4439/10000, Training Loss: 0.9179716110229492, Validation Loss: 1.6631935834884644\n",
      "Epoch 4440/10000, Training Loss: 0.8643710613250732, Validation Loss: 1.0468685626983643\n",
      "Epoch 4441/10000, Training Loss: 0.8951562643051147, Validation Loss: 0.8416704535484314\n",
      "Epoch 4442/10000, Training Loss: 1.0380423069000244, Validation Loss: 1.1751954555511475\n",
      "Epoch 4443/10000, Training Loss: 1.0848942995071411, Validation Loss: 1.8612947463989258\n",
      "Epoch 4444/10000, Training Loss: 1.4349812269210815, Validation Loss: 2.429861307144165\n",
      "Epoch 4445/10000, Training Loss: 1.0374934673309326, Validation Loss: 0.901125967502594\n",
      "Epoch 4446/10000, Training Loss: 0.8418118357658386, Validation Loss: 0.6118069291114807\n",
      "Epoch 4447/10000, Training Loss: 0.9652615785598755, Validation Loss: 0.9251212477684021\n",
      "Epoch 4448/10000, Training Loss: 0.9762933254241943, Validation Loss: 0.926291286945343\n",
      "Epoch 4449/10000, Training Loss: 1.1406517028808594, Validation Loss: 1.0303784608840942\n",
      "Epoch 4450/10000, Training Loss: 0.8574446439743042, Validation Loss: 0.9275672435760498\n",
      "Epoch 4451/10000, Training Loss: 1.0461640357971191, Validation Loss: 2.3575351238250732\n",
      "Epoch 4452/10000, Training Loss: 1.005087971687317, Validation Loss: 0.5925151705741882\n",
      "Epoch 4453/10000, Training Loss: 0.9588457345962524, Validation Loss: 1.3374992609024048\n",
      "Epoch 4454/10000, Training Loss: 0.8006528615951538, Validation Loss: 0.8117464184761047\n",
      "Epoch 4455/10000, Training Loss: 0.9270098805427551, Validation Loss: 0.8102948665618896\n",
      "Epoch 4456/10000, Training Loss: 0.9239765405654907, Validation Loss: 1.5529556274414062\n",
      "Epoch 4457/10000, Training Loss: 0.9170016050338745, Validation Loss: 1.0349088907241821\n",
      "Epoch 4458/10000, Training Loss: 1.0214070081710815, Validation Loss: 0.6024640202522278\n",
      "Epoch 4459/10000, Training Loss: 1.1961479187011719, Validation Loss: 1.2225767374038696\n",
      "Epoch 4460/10000, Training Loss: 0.8148102164268494, Validation Loss: 0.9149009585380554\n",
      "Epoch 4461/10000, Training Loss: 1.1182687282562256, Validation Loss: 1.08323073387146\n",
      "Epoch 4462/10000, Training Loss: 0.8000761270523071, Validation Loss: 0.7104172110557556\n",
      "Epoch 4463/10000, Training Loss: 0.8254528045654297, Validation Loss: 1.1397345066070557\n",
      "Epoch 4464/10000, Training Loss: 0.8261856436729431, Validation Loss: 1.1070512533187866\n",
      "Epoch 4465/10000, Training Loss: 0.9998129606246948, Validation Loss: 1.4121097326278687\n",
      "Epoch 4466/10000, Training Loss: 1.1406779289245605, Validation Loss: 0.8723575472831726\n",
      "Epoch 4467/10000, Training Loss: 0.9277926087379456, Validation Loss: 1.488938331604004\n",
      "Epoch 4468/10000, Training Loss: 1.0611212253570557, Validation Loss: 0.6846383213996887\n",
      "Epoch 4469/10000, Training Loss: 1.089773178100586, Validation Loss: 0.8597075343132019\n",
      "Epoch 4470/10000, Training Loss: 0.9241352677345276, Validation Loss: 0.9597647190093994\n",
      "Epoch 4471/10000, Training Loss: 0.9679285883903503, Validation Loss: 0.7628628611564636\n",
      "Epoch 4472/10000, Training Loss: 0.7632479071617126, Validation Loss: 1.508318543434143\n",
      "Epoch 4473/10000, Training Loss: 0.9540314078330994, Validation Loss: 0.608252763748169\n",
      "Epoch 4474/10000, Training Loss: 0.8606210947036743, Validation Loss: 0.8116479516029358\n",
      "Epoch 4475/10000, Training Loss: 0.9095873236656189, Validation Loss: 1.4083272218704224\n",
      "Epoch 4476/10000, Training Loss: 1.1738739013671875, Validation Loss: 1.065354585647583\n",
      "Epoch 4477/10000, Training Loss: 0.9930781126022339, Validation Loss: 1.6818994283676147\n",
      "Epoch 4478/10000, Training Loss: 0.9604743123054504, Validation Loss: 1.5072293281555176\n",
      "Epoch 4479/10000, Training Loss: 0.8905283212661743, Validation Loss: 0.8663569092750549\n",
      "Epoch 4480/10000, Training Loss: 0.7921218276023865, Validation Loss: 1.1906698942184448\n",
      "Epoch 4481/10000, Training Loss: 1.1160986423492432, Validation Loss: 1.10944664478302\n",
      "Epoch 4482/10000, Training Loss: 1.255095362663269, Validation Loss: 1.5897693634033203\n",
      "Epoch 4483/10000, Training Loss: 0.8663712739944458, Validation Loss: 1.331646203994751\n",
      "Epoch 4484/10000, Training Loss: 0.7981752157211304, Validation Loss: 0.5972254872322083\n",
      "Epoch 4485/10000, Training Loss: 1.0918850898742676, Validation Loss: 0.7835221290588379\n",
      "Epoch 4486/10000, Training Loss: 0.9546098709106445, Validation Loss: 0.4457970857620239\n",
      "Epoch 4487/10000, Training Loss: 1.0025731325149536, Validation Loss: 1.2024829387664795\n",
      "Epoch 4488/10000, Training Loss: 0.8497897386550903, Validation Loss: 1.1658974885940552\n",
      "Epoch 4489/10000, Training Loss: 0.8915993571281433, Validation Loss: 0.6629506945610046\n",
      "Epoch 4490/10000, Training Loss: 0.9482755661010742, Validation Loss: 1.520585060119629\n",
      "Epoch 4491/10000, Training Loss: 0.9451267719268799, Validation Loss: 0.8462064266204834\n",
      "Epoch 4492/10000, Training Loss: 1.1344023942947388, Validation Loss: 1.0510119199752808\n",
      "Epoch 4493/10000, Training Loss: 1.0246474742889404, Validation Loss: 2.1663949489593506\n",
      "Epoch 4494/10000, Training Loss: 0.9333213567733765, Validation Loss: 0.8299739360809326\n",
      "Epoch 4495/10000, Training Loss: 0.9161627292633057, Validation Loss: 1.0576918125152588\n",
      "Epoch 4496/10000, Training Loss: 0.8544594049453735, Validation Loss: 1.3694461584091187\n",
      "Epoch 4497/10000, Training Loss: 0.9085471034049988, Validation Loss: 0.9105958938598633\n",
      "Epoch 4498/10000, Training Loss: 0.7243814468383789, Validation Loss: 2.3113162517547607\n",
      "Epoch 4499/10000, Training Loss: 1.0525044202804565, Validation Loss: 1.4970197677612305\n",
      "Epoch 4500/10000, Training Loss: 0.8897479772567749, Validation Loss: 1.2850490808486938\n",
      "Epoch 4501/10000, Training Loss: 0.8776213526725769, Validation Loss: 0.3624437153339386\n",
      "Epoch 4502/10000, Training Loss: 0.937119722366333, Validation Loss: 0.636557400226593\n",
      "Epoch 4503/10000, Training Loss: 0.8188092112541199, Validation Loss: 1.33979070186615\n",
      "Epoch 4504/10000, Training Loss: 1.0172842741012573, Validation Loss: 0.9471871852874756\n",
      "Epoch 4505/10000, Training Loss: 0.9221562147140503, Validation Loss: 1.3687443733215332\n",
      "Epoch 4506/10000, Training Loss: 0.8304566740989685, Validation Loss: 0.8614709377288818\n",
      "Epoch 4507/10000, Training Loss: 0.9099377989768982, Validation Loss: 1.3234699964523315\n",
      "Epoch 4508/10000, Training Loss: 1.0722273588180542, Validation Loss: 1.0017372369766235\n",
      "Epoch 4509/10000, Training Loss: 0.8787984251976013, Validation Loss: 1.2034540176391602\n",
      "Epoch 4510/10000, Training Loss: 0.8077548146247864, Validation Loss: 1.257639765739441\n",
      "Epoch 4511/10000, Training Loss: 0.8085560202598572, Validation Loss: 1.395506739616394\n",
      "Epoch 4512/10000, Training Loss: 1.1153677701950073, Validation Loss: 1.6923693418502808\n",
      "Epoch 4513/10000, Training Loss: 1.141483187675476, Validation Loss: 1.0127102136611938\n",
      "Epoch 4514/10000, Training Loss: 0.9509090781211853, Validation Loss: 1.1905356645584106\n",
      "Epoch 4515/10000, Training Loss: 1.0899313688278198, Validation Loss: 0.686217725276947\n",
      "Epoch 4516/10000, Training Loss: 0.9916054010391235, Validation Loss: 1.3223603963851929\n",
      "Epoch 4517/10000, Training Loss: 1.0461512804031372, Validation Loss: 0.6364893913269043\n",
      "Epoch 4518/10000, Training Loss: 1.1032847166061401, Validation Loss: 0.791775643825531\n",
      "Epoch 4519/10000, Training Loss: 0.7535459995269775, Validation Loss: 0.839482843875885\n",
      "Epoch 4520/10000, Training Loss: 0.9068113565444946, Validation Loss: 0.5039644241333008\n",
      "Epoch 4521/10000, Training Loss: 0.9601411819458008, Validation Loss: 0.5349492430686951\n",
      "Epoch 4522/10000, Training Loss: 0.9356197714805603, Validation Loss: 2.4580023288726807\n",
      "Epoch 4523/10000, Training Loss: 0.7561414241790771, Validation Loss: 0.6916667819023132\n",
      "Epoch 4524/10000, Training Loss: 0.9276008605957031, Validation Loss: 0.5241943597793579\n",
      "Epoch 4525/10000, Training Loss: 0.9055852890014648, Validation Loss: 1.0130308866500854\n",
      "Epoch 4526/10000, Training Loss: 1.0086085796356201, Validation Loss: 0.9368507862091064\n",
      "Epoch 4527/10000, Training Loss: 0.8662421703338623, Validation Loss: 1.334161639213562\n",
      "Epoch 4528/10000, Training Loss: 0.8890101313591003, Validation Loss: 0.8279531002044678\n",
      "Epoch 4529/10000, Training Loss: 0.8796133399009705, Validation Loss: 1.3827992677688599\n",
      "Epoch 4530/10000, Training Loss: 0.8883448243141174, Validation Loss: 1.0445823669433594\n",
      "Epoch 4531/10000, Training Loss: 0.9217091798782349, Validation Loss: 1.449965476989746\n",
      "Epoch 4532/10000, Training Loss: 1.0428941249847412, Validation Loss: 1.3018800020217896\n",
      "Epoch 4533/10000, Training Loss: 0.9222866892814636, Validation Loss: 1.5553056001663208\n",
      "Epoch 4534/10000, Training Loss: 0.8100795745849609, Validation Loss: 1.3265126943588257\n",
      "Epoch 4535/10000, Training Loss: 1.0907692909240723, Validation Loss: 0.6831879615783691\n",
      "Epoch 4536/10000, Training Loss: 0.9299314022064209, Validation Loss: 0.7667960524559021\n",
      "Epoch 4537/10000, Training Loss: 0.8952437043190002, Validation Loss: 0.9747088551521301\n",
      "Epoch 4538/10000, Training Loss: 0.8698530197143555, Validation Loss: 0.7625608444213867\n",
      "Epoch 4539/10000, Training Loss: 1.0093398094177246, Validation Loss: 1.6488642692565918\n",
      "Epoch 4540/10000, Training Loss: 0.9030742645263672, Validation Loss: 1.2996597290039062\n",
      "Epoch 4541/10000, Training Loss: 0.9169279932975769, Validation Loss: 0.8052792549133301\n",
      "Epoch 4542/10000, Training Loss: 0.8571274280548096, Validation Loss: 0.8770878314971924\n",
      "Epoch 4543/10000, Training Loss: 0.8779324889183044, Validation Loss: 0.747610867023468\n",
      "Epoch 4544/10000, Training Loss: 0.8330899477005005, Validation Loss: 1.6208518743515015\n",
      "Epoch 4545/10000, Training Loss: 0.7583795189857483, Validation Loss: 1.0421139001846313\n",
      "Epoch 4546/10000, Training Loss: 0.7701581716537476, Validation Loss: 0.5080025792121887\n",
      "Epoch 4547/10000, Training Loss: 0.913296639919281, Validation Loss: 0.9366711974143982\n",
      "Epoch 4548/10000, Training Loss: 1.057942509651184, Validation Loss: 1.3261889219284058\n",
      "Epoch 4549/10000, Training Loss: 0.9866337180137634, Validation Loss: 0.8359057307243347\n",
      "Epoch 4550/10000, Training Loss: 0.8950541019439697, Validation Loss: 1.3926314115524292\n",
      "Epoch 4551/10000, Training Loss: 0.8879650831222534, Validation Loss: 0.8622684478759766\n",
      "Epoch 4552/10000, Training Loss: 0.8651531934738159, Validation Loss: 1.0612225532531738\n",
      "Epoch 4553/10000, Training Loss: 0.8832316994667053, Validation Loss: 1.0871487855911255\n",
      "Epoch 4554/10000, Training Loss: 0.7539374828338623, Validation Loss: 1.6408286094665527\n",
      "Epoch 4555/10000, Training Loss: 0.8277333378791809, Validation Loss: 1.7532109022140503\n",
      "Epoch 4556/10000, Training Loss: 1.0757616758346558, Validation Loss: 1.4243701696395874\n",
      "Epoch 4557/10000, Training Loss: 0.9730631709098816, Validation Loss: 1.6371442079544067\n",
      "Epoch 4558/10000, Training Loss: 1.0570473670959473, Validation Loss: 1.259060263633728\n",
      "Epoch 4559/10000, Training Loss: 0.9265406131744385, Validation Loss: 0.5826787352561951\n",
      "Epoch 4560/10000, Training Loss: 1.0638145208358765, Validation Loss: 0.7831549048423767\n",
      "Epoch 4561/10000, Training Loss: 0.9598830342292786, Validation Loss: 1.4809541702270508\n",
      "Epoch 4562/10000, Training Loss: 1.2302788496017456, Validation Loss: 0.5379626750946045\n",
      "Epoch 4563/10000, Training Loss: 0.8835694193840027, Validation Loss: 1.481100082397461\n",
      "Epoch 4564/10000, Training Loss: 0.8449200987815857, Validation Loss: 0.7210392951965332\n",
      "Epoch 4565/10000, Training Loss: 0.896744430065155, Validation Loss: 1.0074983835220337\n",
      "Epoch 4566/10000, Training Loss: 0.9656113982200623, Validation Loss: 0.8286630511283875\n",
      "Epoch 4567/10000, Training Loss: 0.9404863119125366, Validation Loss: 0.846323549747467\n",
      "Epoch 4568/10000, Training Loss: 0.8377259969711304, Validation Loss: 0.9381334185600281\n",
      "Epoch 4569/10000, Training Loss: 0.8082708120346069, Validation Loss: 1.2148362398147583\n",
      "Epoch 4570/10000, Training Loss: 0.9767037630081177, Validation Loss: 0.5610498785972595\n",
      "Epoch 4571/10000, Training Loss: 1.0126292705535889, Validation Loss: 1.0659164190292358\n",
      "Epoch 4572/10000, Training Loss: 0.7449767589569092, Validation Loss: 0.6629177927970886\n",
      "Epoch 4573/10000, Training Loss: 0.732932984828949, Validation Loss: 0.951583206653595\n",
      "Epoch 4574/10000, Training Loss: 0.8613982796669006, Validation Loss: 0.9790433049201965\n",
      "Epoch 4575/10000, Training Loss: 1.0035065412521362, Validation Loss: 0.5643697381019592\n",
      "Epoch 4576/10000, Training Loss: 0.82933509349823, Validation Loss: 0.8853101134300232\n",
      "Epoch 4577/10000, Training Loss: 0.9896622896194458, Validation Loss: 1.0860689878463745\n",
      "Epoch 4578/10000, Training Loss: 0.8283004760742188, Validation Loss: 1.1455262899398804\n",
      "Epoch 4579/10000, Training Loss: 0.8617796301841736, Validation Loss: 0.9722580909729004\n",
      "Epoch 4580/10000, Training Loss: 0.8357569575309753, Validation Loss: 2.1539130210876465\n",
      "Epoch 4581/10000, Training Loss: 0.7990808486938477, Validation Loss: 0.9561679363250732\n",
      "Epoch 4582/10000, Training Loss: 1.0307366847991943, Validation Loss: 1.7020164728164673\n",
      "Epoch 4583/10000, Training Loss: 0.9390259981155396, Validation Loss: 1.2437093257904053\n",
      "Epoch 4584/10000, Training Loss: 0.8014904260635376, Validation Loss: 1.093946933746338\n",
      "Epoch 4585/10000, Training Loss: 0.8784205913543701, Validation Loss: 1.7122713327407837\n",
      "Epoch 4586/10000, Training Loss: 0.7795226573944092, Validation Loss: 1.258541464805603\n",
      "Epoch 4587/10000, Training Loss: 1.017126202583313, Validation Loss: 1.2967355251312256\n",
      "Epoch 4588/10000, Training Loss: 0.921799898147583, Validation Loss: 0.9915944933891296\n",
      "Epoch 4589/10000, Training Loss: 0.8225613832473755, Validation Loss: 1.1335512399673462\n",
      "Epoch 4590/10000, Training Loss: 1.1503673791885376, Validation Loss: 1.363436222076416\n",
      "Epoch 4591/10000, Training Loss: 0.9869269132614136, Validation Loss: 2.011375665664673\n",
      "Epoch 4592/10000, Training Loss: 0.8946216106414795, Validation Loss: 0.9143986701965332\n",
      "Epoch 4593/10000, Training Loss: 0.765110433101654, Validation Loss: 1.1925724744796753\n",
      "Epoch 4594/10000, Training Loss: 0.8662700653076172, Validation Loss: 0.7917951941490173\n",
      "Epoch 4595/10000, Training Loss: 0.8237462639808655, Validation Loss: 1.0747753381729126\n",
      "Epoch 4596/10000, Training Loss: 0.7069373726844788, Validation Loss: 0.8637645840644836\n",
      "Epoch 4597/10000, Training Loss: 0.8313537240028381, Validation Loss: 0.5380777716636658\n",
      "Epoch 4598/10000, Training Loss: 0.8401007056236267, Validation Loss: 0.6225871443748474\n",
      "Epoch 4599/10000, Training Loss: 0.9029918313026428, Validation Loss: 0.9362435340881348\n",
      "Epoch 4600/10000, Training Loss: 0.7876901626586914, Validation Loss: 1.14932119846344\n",
      "Epoch 4601/10000, Training Loss: 1.1528187990188599, Validation Loss: 1.2453715801239014\n",
      "Epoch 4602/10000, Training Loss: 0.941475510597229, Validation Loss: 1.7306822538375854\n",
      "Epoch 4603/10000, Training Loss: 0.8198904395103455, Validation Loss: 0.6478845477104187\n",
      "Epoch 4604/10000, Training Loss: 1.0718907117843628, Validation Loss: 1.3086684942245483\n",
      "Epoch 4605/10000, Training Loss: 0.9016929864883423, Validation Loss: 0.5487868785858154\n",
      "Epoch 4606/10000, Training Loss: 0.8282752633094788, Validation Loss: 1.5761216878890991\n",
      "Epoch 4607/10000, Training Loss: 1.0695267915725708, Validation Loss: 0.9042432308197021\n",
      "Epoch 4608/10000, Training Loss: 0.8039707541465759, Validation Loss: 0.8938524723052979\n",
      "Epoch 4609/10000, Training Loss: 0.9144467711448669, Validation Loss: 1.026124358177185\n",
      "Epoch 4610/10000, Training Loss: 0.8898144960403442, Validation Loss: 1.4171243906021118\n",
      "Epoch 4611/10000, Training Loss: 0.9547261595726013, Validation Loss: 1.01468026638031\n",
      "Epoch 4612/10000, Training Loss: 1.393479824066162, Validation Loss: 2.1604998111724854\n",
      "Epoch 4613/10000, Training Loss: 1.0320988893508911, Validation Loss: 0.832874596118927\n",
      "Epoch 4614/10000, Training Loss: 0.8513031601905823, Validation Loss: 0.6376358270645142\n",
      "Epoch 4615/10000, Training Loss: 1.073622465133667, Validation Loss: 1.1239787340164185\n",
      "Epoch 4616/10000, Training Loss: 0.8865758180618286, Validation Loss: 0.7572090029716492\n",
      "Epoch 4617/10000, Training Loss: 0.957553505897522, Validation Loss: 0.881774365901947\n",
      "Epoch 4618/10000, Training Loss: 0.7935035228729248, Validation Loss: 1.5149664878845215\n",
      "Epoch 4619/10000, Training Loss: 0.7284107804298401, Validation Loss: 0.6430711150169373\n",
      "Epoch 4620/10000, Training Loss: 0.8897337317466736, Validation Loss: 0.8516477942466736\n",
      "Epoch 4621/10000, Training Loss: 0.9258639812469482, Validation Loss: 0.851829469203949\n",
      "Epoch 4622/10000, Training Loss: 0.9593602418899536, Validation Loss: 1.1400583982467651\n",
      "Epoch 4623/10000, Training Loss: 0.846544623374939, Validation Loss: 0.8505032658576965\n",
      "Epoch 4624/10000, Training Loss: 0.993256688117981, Validation Loss: 0.8693504929542542\n",
      "Epoch 4625/10000, Training Loss: 1.0041502714157104, Validation Loss: 0.8040211796760559\n",
      "Epoch 4626/10000, Training Loss: 0.8050968647003174, Validation Loss: 0.6224638819694519\n",
      "Epoch 4627/10000, Training Loss: 0.8229583501815796, Validation Loss: 1.0675110816955566\n",
      "Epoch 4628/10000, Training Loss: 0.7655479907989502, Validation Loss: 0.6282178163528442\n",
      "Epoch 4629/10000, Training Loss: 0.9218485951423645, Validation Loss: 2.0041239261627197\n",
      "Epoch 4630/10000, Training Loss: 0.7932805418968201, Validation Loss: 0.6465111970901489\n",
      "Epoch 4631/10000, Training Loss: 0.9345320463180542, Validation Loss: 1.0923007726669312\n",
      "Epoch 4632/10000, Training Loss: 0.9213768839836121, Validation Loss: 1.3862143754959106\n",
      "Epoch 4633/10000, Training Loss: 0.8028993606567383, Validation Loss: 1.3176556825637817\n",
      "Epoch 4634/10000, Training Loss: 0.8730211853981018, Validation Loss: 1.0645047426223755\n",
      "Epoch 4635/10000, Training Loss: 0.7927674651145935, Validation Loss: 0.9894391894340515\n",
      "Epoch 4636/10000, Training Loss: 0.9284571409225464, Validation Loss: 0.937744677066803\n",
      "Epoch 4637/10000, Training Loss: 0.9386050701141357, Validation Loss: 1.5209938287734985\n",
      "Epoch 4638/10000, Training Loss: 0.8365885615348816, Validation Loss: 0.8151891827583313\n",
      "Epoch 4639/10000, Training Loss: 0.9979915022850037, Validation Loss: 1.2106108665466309\n",
      "Epoch 4640/10000, Training Loss: 1.0006009340286255, Validation Loss: 1.2222163677215576\n",
      "Epoch 4641/10000, Training Loss: 0.8833065629005432, Validation Loss: 0.6748091578483582\n",
      "Epoch 4642/10000, Training Loss: 0.8522643446922302, Validation Loss: 0.7271283268928528\n",
      "Epoch 4643/10000, Training Loss: 0.7786623239517212, Validation Loss: 1.444897174835205\n",
      "Epoch 4644/10000, Training Loss: 0.8018882274627686, Validation Loss: 0.9812645316123962\n",
      "Epoch 4645/10000, Training Loss: 1.0091389417648315, Validation Loss: 1.0005390644073486\n",
      "Epoch 4646/10000, Training Loss: 0.8802675604820251, Validation Loss: 0.6602631211280823\n",
      "Epoch 4647/10000, Training Loss: 0.8756077289581299, Validation Loss: 1.1985431909561157\n",
      "Epoch 4648/10000, Training Loss: 0.8828242421150208, Validation Loss: 1.26583731174469\n",
      "Epoch 4649/10000, Training Loss: 0.7692112326622009, Validation Loss: 0.5636318325996399\n",
      "Epoch 4650/10000, Training Loss: 0.9007082581520081, Validation Loss: 0.8550782799720764\n",
      "Epoch 4651/10000, Training Loss: 1.0605978965759277, Validation Loss: 1.246692419052124\n",
      "Epoch 4652/10000, Training Loss: 1.0239008665084839, Validation Loss: 1.419456958770752\n",
      "Epoch 4653/10000, Training Loss: 0.8936825394630432, Validation Loss: 1.1562069654464722\n",
      "Epoch 4654/10000, Training Loss: 1.13665771484375, Validation Loss: 1.7600833177566528\n",
      "Epoch 4655/10000, Training Loss: 0.9325571060180664, Validation Loss: 0.8970121741294861\n",
      "Epoch 4656/10000, Training Loss: 0.894385576248169, Validation Loss: 1.1925512552261353\n",
      "Epoch 4657/10000, Training Loss: 0.936417281627655, Validation Loss: 0.522419273853302\n",
      "Epoch 4658/10000, Training Loss: 0.8545698523521423, Validation Loss: 0.5672456622123718\n",
      "Epoch 4659/10000, Training Loss: 0.8505811095237732, Validation Loss: 0.8088467121124268\n",
      "Epoch 4660/10000, Training Loss: 1.0871094465255737, Validation Loss: 1.0995227098464966\n",
      "Epoch 4661/10000, Training Loss: 0.7407459020614624, Validation Loss: 1.1870893239974976\n",
      "Epoch 4662/10000, Training Loss: 0.7564334273338318, Validation Loss: 0.8633642196655273\n",
      "Epoch 4663/10000, Training Loss: 0.735112726688385, Validation Loss: 0.9139606952667236\n",
      "Epoch 4664/10000, Training Loss: 0.941267728805542, Validation Loss: 1.4239288568496704\n",
      "Epoch 4665/10000, Training Loss: 0.8127132058143616, Validation Loss: 1.2257548570632935\n",
      "Epoch 4666/10000, Training Loss: 0.8998644351959229, Validation Loss: 1.0687768459320068\n",
      "Epoch 4667/10000, Training Loss: 0.8764061331748962, Validation Loss: 1.6647886037826538\n",
      "Epoch 4668/10000, Training Loss: 0.9316672086715698, Validation Loss: 1.183733582496643\n",
      "Epoch 4669/10000, Training Loss: 0.891918420791626, Validation Loss: 0.948535144329071\n",
      "Epoch 4670/10000, Training Loss: 1.0118849277496338, Validation Loss: 0.993626594543457\n",
      "Epoch 4671/10000, Training Loss: 0.8336874842643738, Validation Loss: 0.8660767078399658\n",
      "Epoch 4672/10000, Training Loss: 0.7577317953109741, Validation Loss: 0.683539092540741\n",
      "Epoch 4673/10000, Training Loss: 0.9974690675735474, Validation Loss: 1.3138223886489868\n",
      "Epoch 4674/10000, Training Loss: 0.9336029291152954, Validation Loss: 0.7045889496803284\n",
      "Epoch 4675/10000, Training Loss: 0.9626961350440979, Validation Loss: 0.46378448605537415\n",
      "Epoch 4676/10000, Training Loss: 0.7847185730934143, Validation Loss: 0.7731667160987854\n",
      "Epoch 4677/10000, Training Loss: 0.817835807800293, Validation Loss: 0.5185660719871521\n",
      "Epoch 4678/10000, Training Loss: 0.8574272990226746, Validation Loss: 1.0325440168380737\n",
      "Epoch 4679/10000, Training Loss: 0.8317166566848755, Validation Loss: 0.8225271701812744\n",
      "Epoch 4680/10000, Training Loss: 0.9125125408172607, Validation Loss: 1.3437939882278442\n",
      "Epoch 4681/10000, Training Loss: 0.8025391101837158, Validation Loss: 0.8469305038452148\n",
      "Epoch 4682/10000, Training Loss: 0.8480592370033264, Validation Loss: 0.6932550072669983\n",
      "Epoch 4683/10000, Training Loss: 0.8272308707237244, Validation Loss: 0.44309327006340027\n",
      "Epoch 4684/10000, Training Loss: 0.933885931968689, Validation Loss: 0.7255027890205383\n",
      "Epoch 4685/10000, Training Loss: 0.8264417052268982, Validation Loss: 0.9122545123100281\n",
      "Epoch 4686/10000, Training Loss: 0.9893752336502075, Validation Loss: 1.1029623746871948\n",
      "Epoch 4687/10000, Training Loss: 0.8642851114273071, Validation Loss: 1.2853727340698242\n",
      "Epoch 4688/10000, Training Loss: 0.8985463380813599, Validation Loss: 0.7837274074554443\n",
      "Epoch 4689/10000, Training Loss: 0.9064866304397583, Validation Loss: 1.2406798601150513\n",
      "Epoch 4690/10000, Training Loss: 0.9716648459434509, Validation Loss: 1.1361278295516968\n",
      "Epoch 4691/10000, Training Loss: 0.9705178141593933, Validation Loss: 1.0464476346969604\n",
      "Epoch 4692/10000, Training Loss: 1.1085928678512573, Validation Loss: 1.1205129623413086\n",
      "Epoch 4693/10000, Training Loss: 0.8243986964225769, Validation Loss: 1.3047388792037964\n",
      "Epoch 4694/10000, Training Loss: 0.9268943071365356, Validation Loss: 0.9359526038169861\n",
      "Epoch 4695/10000, Training Loss: 0.8431330323219299, Validation Loss: 1.0053014755249023\n",
      "Epoch 4696/10000, Training Loss: 0.8543456196784973, Validation Loss: 1.2819782495498657\n",
      "Epoch 4697/10000, Training Loss: 0.8644004464149475, Validation Loss: 0.6517412066459656\n",
      "Epoch 4698/10000, Training Loss: 0.830356240272522, Validation Loss: 0.9655367732048035\n",
      "Epoch 4699/10000, Training Loss: 0.8859770894050598, Validation Loss: 0.8140175938606262\n",
      "Epoch 4700/10000, Training Loss: 0.7585973143577576, Validation Loss: 0.7651465535163879\n",
      "Epoch 4701/10000, Training Loss: 0.9180240035057068, Validation Loss: 1.1911066770553589\n",
      "Epoch 4702/10000, Training Loss: 0.8846870064735413, Validation Loss: 1.2112258672714233\n",
      "Epoch 4703/10000, Training Loss: 0.7474276423454285, Validation Loss: 0.5777248740196228\n",
      "Epoch 4704/10000, Training Loss: 0.8935562968254089, Validation Loss: 1.0246796607971191\n",
      "Epoch 4705/10000, Training Loss: 0.9239416718482971, Validation Loss: 1.1937247514724731\n",
      "Epoch 4706/10000, Training Loss: 0.8563517332077026, Validation Loss: 1.2547115087509155\n",
      "Epoch 4707/10000, Training Loss: 0.8946053385734558, Validation Loss: 0.7083591818809509\n",
      "Epoch 4708/10000, Training Loss: 0.8643080592155457, Validation Loss: 0.9707977175712585\n",
      "Epoch 4709/10000, Training Loss: 0.7538428902626038, Validation Loss: 0.606854259967804\n",
      "Epoch 4710/10000, Training Loss: 0.9839816689491272, Validation Loss: 1.1906192302703857\n",
      "Epoch 4711/10000, Training Loss: 0.92315673828125, Validation Loss: 0.7351109385490417\n",
      "Epoch 4712/10000, Training Loss: 0.8115624785423279, Validation Loss: 1.4004367589950562\n",
      "Epoch 4713/10000, Training Loss: 0.8913465738296509, Validation Loss: 1.1724642515182495\n",
      "Epoch 4714/10000, Training Loss: 0.7701489925384521, Validation Loss: 0.6979910731315613\n",
      "Epoch 4715/10000, Training Loss: 1.0023900270462036, Validation Loss: 0.6992918848991394\n",
      "Epoch 4716/10000, Training Loss: 0.7753369212150574, Validation Loss: 2.0328657627105713\n",
      "Epoch 4717/10000, Training Loss: 0.7688015699386597, Validation Loss: 0.8621988296508789\n",
      "Epoch 4718/10000, Training Loss: 0.9785471558570862, Validation Loss: 0.6695205569267273\n",
      "Epoch 4719/10000, Training Loss: 0.7918316125869751, Validation Loss: 1.25928795337677\n",
      "Epoch 4720/10000, Training Loss: 0.7981857657432556, Validation Loss: 0.8499945998191833\n",
      "Epoch 4721/10000, Training Loss: 0.7600025534629822, Validation Loss: 1.051579475402832\n",
      "Epoch 4722/10000, Training Loss: 0.7755458354949951, Validation Loss: 0.5751336812973022\n",
      "Epoch 4723/10000, Training Loss: 0.9558693170547485, Validation Loss: 1.103263258934021\n",
      "Epoch 4724/10000, Training Loss: 0.8386389017105103, Validation Loss: 0.692361056804657\n",
      "Epoch 4725/10000, Training Loss: 0.8912684917449951, Validation Loss: 1.067305088043213\n",
      "Epoch 4726/10000, Training Loss: 0.7683175802230835, Validation Loss: 0.7969682216644287\n",
      "Epoch 4727/10000, Training Loss: 0.8402354717254639, Validation Loss: 0.9084967970848083\n",
      "Epoch 4728/10000, Training Loss: 1.0780775547027588, Validation Loss: 0.848743200302124\n",
      "Epoch 4729/10000, Training Loss: 0.8475058078765869, Validation Loss: 1.3502702713012695\n",
      "Epoch 4730/10000, Training Loss: 0.7863922715187073, Validation Loss: 0.6323220133781433\n",
      "Epoch 4731/10000, Training Loss: 0.9426921010017395, Validation Loss: 0.8892362713813782\n",
      "Epoch 4732/10000, Training Loss: 0.8113349080085754, Validation Loss: 1.4094444513320923\n",
      "Epoch 4733/10000, Training Loss: 0.8434897661209106, Validation Loss: 0.962116539478302\n",
      "Epoch 4734/10000, Training Loss: 0.7797421813011169, Validation Loss: 1.1934608221054077\n",
      "Epoch 4735/10000, Training Loss: 0.786687433719635, Validation Loss: 0.8617144227027893\n",
      "Epoch 4736/10000, Training Loss: 0.7689840793609619, Validation Loss: 1.086215853691101\n",
      "Epoch 4737/10000, Training Loss: 0.7763067483901978, Validation Loss: 1.341868281364441\n",
      "Epoch 4738/10000, Training Loss: 0.8978714346885681, Validation Loss: 1.3702784776687622\n",
      "Epoch 4739/10000, Training Loss: 0.7478030323982239, Validation Loss: 1.0956346988677979\n",
      "Epoch 4740/10000, Training Loss: 0.7411569356918335, Validation Loss: 0.5116929411888123\n",
      "Epoch 4741/10000, Training Loss: 0.8840201497077942, Validation Loss: 1.3643617630004883\n",
      "Epoch 4742/10000, Training Loss: 0.7615667581558228, Validation Loss: 1.2292431592941284\n",
      "Epoch 4743/10000, Training Loss: 0.775911271572113, Validation Loss: 0.9753043055534363\n",
      "Epoch 4744/10000, Training Loss: 0.9970171451568604, Validation Loss: 1.0196701288223267\n",
      "Epoch 4745/10000, Training Loss: 0.9532323479652405, Validation Loss: 0.7901251316070557\n",
      "Epoch 4746/10000, Training Loss: 1.0063399076461792, Validation Loss: 1.1814963817596436\n",
      "Epoch 4747/10000, Training Loss: 0.8896999955177307, Validation Loss: 0.9642811417579651\n",
      "Epoch 4748/10000, Training Loss: 0.8023106455802917, Validation Loss: 0.8745585083961487\n",
      "Epoch 4749/10000, Training Loss: 0.7332305312156677, Validation Loss: 0.9905338287353516\n",
      "Epoch 4750/10000, Training Loss: 0.9261159300804138, Validation Loss: 1.61484956741333\n",
      "Epoch 4751/10000, Training Loss: 0.9303541779518127, Validation Loss: 0.6910416483879089\n",
      "Epoch 4752/10000, Training Loss: 0.7794339060783386, Validation Loss: 0.7137441635131836\n",
      "Epoch 4753/10000, Training Loss: 0.9378055930137634, Validation Loss: 0.8464958667755127\n",
      "Epoch 4754/10000, Training Loss: 0.9164936542510986, Validation Loss: 1.243343710899353\n",
      "Epoch 4755/10000, Training Loss: 0.7850686311721802, Validation Loss: 0.961200475692749\n",
      "Epoch 4756/10000, Training Loss: 0.8281096816062927, Validation Loss: 0.9055006504058838\n",
      "Epoch 4757/10000, Training Loss: 0.7621954679489136, Validation Loss: 1.063122034072876\n",
      "Epoch 4758/10000, Training Loss: 0.7138668298721313, Validation Loss: 1.0985796451568604\n",
      "Epoch 4759/10000, Training Loss: 0.8992289304733276, Validation Loss: 0.7197232246398926\n",
      "Epoch 4760/10000, Training Loss: 0.9538094997406006, Validation Loss: 1.2634730339050293\n",
      "Epoch 4761/10000, Training Loss: 0.7720295190811157, Validation Loss: 1.2387019395828247\n",
      "Epoch 4762/10000, Training Loss: 0.7662836909294128, Validation Loss: 1.099031686782837\n",
      "Epoch 4763/10000, Training Loss: 0.7935930490493774, Validation Loss: 1.0029126405715942\n",
      "Epoch 4764/10000, Training Loss: 0.8346580266952515, Validation Loss: 0.7674883008003235\n",
      "Epoch 4765/10000, Training Loss: 0.7500871419906616, Validation Loss: 0.7638298869132996\n",
      "Epoch 4766/10000, Training Loss: 0.7673135995864868, Validation Loss: 0.7753181457519531\n",
      "Epoch 4767/10000, Training Loss: 0.7714363932609558, Validation Loss: 0.7857480049133301\n",
      "Epoch 4768/10000, Training Loss: 0.8082135915756226, Validation Loss: 0.6080885529518127\n",
      "Epoch 4769/10000, Training Loss: 0.8734369874000549, Validation Loss: 0.6499786972999573\n",
      "Epoch 4770/10000, Training Loss: 0.7719825506210327, Validation Loss: 1.593502402305603\n",
      "Epoch 4771/10000, Training Loss: 0.8096174597740173, Validation Loss: 1.04676353931427\n",
      "Epoch 4772/10000, Training Loss: 0.9867360591888428, Validation Loss: 1.5274134874343872\n",
      "Epoch 4773/10000, Training Loss: 0.9466673135757446, Validation Loss: 1.1498785018920898\n",
      "Epoch 4774/10000, Training Loss: 0.7790494561195374, Validation Loss: 0.566691517829895\n",
      "Epoch 4775/10000, Training Loss: 0.8375424742698669, Validation Loss: 1.3780770301818848\n",
      "Epoch 4776/10000, Training Loss: 0.8740485906600952, Validation Loss: 0.8026542067527771\n",
      "Epoch 4777/10000, Training Loss: 0.8630878925323486, Validation Loss: 0.8682876229286194\n",
      "Epoch 4778/10000, Training Loss: 0.8933740258216858, Validation Loss: 1.2782155275344849\n",
      "Epoch 4779/10000, Training Loss: 0.7961142659187317, Validation Loss: 0.825977623462677\n",
      "Epoch 4780/10000, Training Loss: 0.8285249471664429, Validation Loss: 0.7483041882514954\n",
      "Epoch 4781/10000, Training Loss: 0.7637690901756287, Validation Loss: 1.290509819984436\n",
      "Epoch 4782/10000, Training Loss: 0.8259919285774231, Validation Loss: 0.9708980917930603\n",
      "Epoch 4783/10000, Training Loss: 0.7773675322532654, Validation Loss: 1.393224835395813\n",
      "Epoch 4784/10000, Training Loss: 1.0697404146194458, Validation Loss: 0.5493793487548828\n",
      "Epoch 4785/10000, Training Loss: 0.7188783288002014, Validation Loss: 0.9895015358924866\n",
      "Epoch 4786/10000, Training Loss: 0.8119456171989441, Validation Loss: 0.7380350232124329\n",
      "Epoch 4787/10000, Training Loss: 0.7903958559036255, Validation Loss: 0.4986080825328827\n",
      "Epoch 4788/10000, Training Loss: 0.7678704857826233, Validation Loss: 1.1633895635604858\n",
      "Epoch 4789/10000, Training Loss: 0.7793341279029846, Validation Loss: 0.7644889950752258\n",
      "Epoch 4790/10000, Training Loss: 0.8242821097373962, Validation Loss: 1.1067651510238647\n",
      "Epoch 4791/10000, Training Loss: 0.8603286743164062, Validation Loss: 1.7712808847427368\n",
      "Epoch 4792/10000, Training Loss: 0.7910439372062683, Validation Loss: 1.3261146545410156\n",
      "Epoch 4793/10000, Training Loss: 0.8433268070220947, Validation Loss: 0.708720862865448\n",
      "Epoch 4794/10000, Training Loss: 0.9657814502716064, Validation Loss: 1.2062842845916748\n",
      "Epoch 4795/10000, Training Loss: 0.9160417914390564, Validation Loss: 0.6162207722663879\n",
      "Epoch 4796/10000, Training Loss: 0.9004238843917847, Validation Loss: 0.9582042694091797\n",
      "Epoch 4797/10000, Training Loss: 0.7876631021499634, Validation Loss: 0.6524193286895752\n",
      "Epoch 4798/10000, Training Loss: 0.8151286244392395, Validation Loss: 1.04323410987854\n",
      "Epoch 4799/10000, Training Loss: 0.8147016167640686, Validation Loss: 1.0659372806549072\n",
      "Epoch 4800/10000, Training Loss: 0.7954729795455933, Validation Loss: 0.9856442809104919\n",
      "Epoch 4801/10000, Training Loss: 0.8334879279136658, Validation Loss: 1.1689883470535278\n",
      "Epoch 4802/10000, Training Loss: 0.7453362345695496, Validation Loss: 0.7218627333641052\n",
      "Epoch 4803/10000, Training Loss: 0.8043768405914307, Validation Loss: 0.611487865447998\n",
      "Epoch 4804/10000, Training Loss: 0.8945690393447876, Validation Loss: 0.87563157081604\n",
      "Epoch 4805/10000, Training Loss: 0.8264780640602112, Validation Loss: 0.8217975497245789\n",
      "Epoch 4806/10000, Training Loss: 0.7087078094482422, Validation Loss: 0.6983919143676758\n",
      "Epoch 4807/10000, Training Loss: 0.8093092441558838, Validation Loss: 0.7882082462310791\n",
      "Epoch 4808/10000, Training Loss: 0.8978625535964966, Validation Loss: 1.1197172403335571\n",
      "Epoch 4809/10000, Training Loss: 0.7612588405609131, Validation Loss: 0.5611310601234436\n",
      "Epoch 4810/10000, Training Loss: 0.9310916662216187, Validation Loss: 0.9372849464416504\n",
      "Epoch 4811/10000, Training Loss: 0.7505025267601013, Validation Loss: 1.1177259683609009\n",
      "Epoch 4812/10000, Training Loss: 0.9153335094451904, Validation Loss: 2.9493343830108643\n",
      "Epoch 4813/10000, Training Loss: 0.7254754304885864, Validation Loss: 0.690741777420044\n",
      "Epoch 4814/10000, Training Loss: 0.8864806890487671, Validation Loss: 1.0670497417449951\n",
      "Epoch 4815/10000, Training Loss: 0.8909957408905029, Validation Loss: 0.726207435131073\n",
      "Epoch 4816/10000, Training Loss: 1.0018845796585083, Validation Loss: 0.4727579355239868\n",
      "Epoch 4817/10000, Training Loss: 0.7980820536613464, Validation Loss: 0.9894992709159851\n",
      "Epoch 4818/10000, Training Loss: 0.7864301204681396, Validation Loss: 1.093064785003662\n",
      "Epoch 4819/10000, Training Loss: 0.9233790040016174, Validation Loss: 0.9883160591125488\n",
      "Epoch 4820/10000, Training Loss: 0.806011438369751, Validation Loss: 0.8429145812988281\n",
      "Epoch 4821/10000, Training Loss: 0.8371106386184692, Validation Loss: 0.6492490172386169\n",
      "Epoch 4822/10000, Training Loss: 0.7436602711677551, Validation Loss: 0.7675483822822571\n",
      "Epoch 4823/10000, Training Loss: 0.7431805729866028, Validation Loss: 0.6292846202850342\n",
      "Epoch 4824/10000, Training Loss: 0.8558728694915771, Validation Loss: 0.7233030796051025\n",
      "Epoch 4825/10000, Training Loss: 0.7695046067237854, Validation Loss: 0.8688916563987732\n",
      "Epoch 4826/10000, Training Loss: 0.8823532462120056, Validation Loss: 0.7819008827209473\n",
      "Epoch 4827/10000, Training Loss: 0.8294492959976196, Validation Loss: 0.9229777455329895\n",
      "Epoch 4828/10000, Training Loss: 0.7554340362548828, Validation Loss: 0.7167198657989502\n",
      "Epoch 4829/10000, Training Loss: 0.9737933874130249, Validation Loss: 1.3491376638412476\n",
      "Epoch 4830/10000, Training Loss: 0.9314662218093872, Validation Loss: 0.8241891860961914\n",
      "Epoch 4831/10000, Training Loss: 0.8875647187232971, Validation Loss: 0.5006914734840393\n",
      "Epoch 4832/10000, Training Loss: 0.9375983476638794, Validation Loss: 1.1862082481384277\n",
      "Epoch 4833/10000, Training Loss: 0.8156156539916992, Validation Loss: 1.0195567607879639\n",
      "Epoch 4834/10000, Training Loss: 0.797936201095581, Validation Loss: 1.0718072652816772\n",
      "Epoch 4835/10000, Training Loss: 0.859244704246521, Validation Loss: 1.047511339187622\n",
      "Epoch 4836/10000, Training Loss: 0.8979077935218811, Validation Loss: 0.6384006142616272\n",
      "Epoch 4837/10000, Training Loss: 0.8470587134361267, Validation Loss: 1.3206032514572144\n",
      "Epoch 4838/10000, Training Loss: 0.9332246780395508, Validation Loss: 0.7735467553138733\n",
      "Epoch 4839/10000, Training Loss: 0.8325780034065247, Validation Loss: 0.7850967049598694\n",
      "Epoch 4840/10000, Training Loss: 0.8344324827194214, Validation Loss: 1.035494089126587\n",
      "Epoch 4841/10000, Training Loss: 0.7531903982162476, Validation Loss: 0.7280607223510742\n",
      "Epoch 4842/10000, Training Loss: 0.8096646666526794, Validation Loss: 0.7679064869880676\n",
      "Epoch 4843/10000, Training Loss: 0.8736671209335327, Validation Loss: 1.208348035812378\n",
      "Epoch 4844/10000, Training Loss: 0.8646496534347534, Validation Loss: 0.6983432173728943\n",
      "Epoch 4845/10000, Training Loss: 0.8809429407119751, Validation Loss: 0.9860453605651855\n",
      "Epoch 4846/10000, Training Loss: 0.7880117893218994, Validation Loss: 0.5772839188575745\n",
      "Epoch 4847/10000, Training Loss: 1.0336850881576538, Validation Loss: 0.5982186198234558\n",
      "Epoch 4848/10000, Training Loss: 0.8942182064056396, Validation Loss: 0.8389468193054199\n",
      "Epoch 4849/10000, Training Loss: 0.8219571709632874, Validation Loss: 0.8858931064605713\n",
      "Epoch 4850/10000, Training Loss: 0.8054894208908081, Validation Loss: 0.989011824131012\n",
      "Epoch 4851/10000, Training Loss: 0.7007404565811157, Validation Loss: 0.798182487487793\n",
      "Epoch 4852/10000, Training Loss: 0.885208249092102, Validation Loss: 1.0808354616165161\n",
      "Epoch 4853/10000, Training Loss: 0.8221705555915833, Validation Loss: 0.6918311715126038\n",
      "Epoch 4854/10000, Training Loss: 0.8171377182006836, Validation Loss: 0.8057420253753662\n",
      "Epoch 4855/10000, Training Loss: 0.791731595993042, Validation Loss: 1.594220519065857\n",
      "Epoch 4856/10000, Training Loss: 0.7915987372398376, Validation Loss: 0.39572396874427795\n",
      "Epoch 4857/10000, Training Loss: 0.8209965825080872, Validation Loss: 1.619880199432373\n",
      "Epoch 4858/10000, Training Loss: 0.8815667033195496, Validation Loss: 2.004589319229126\n",
      "Epoch 4859/10000, Training Loss: 0.9785502552986145, Validation Loss: 0.5592188239097595\n",
      "Epoch 4860/10000, Training Loss: 0.8969403505325317, Validation Loss: 0.9461157917976379\n",
      "Epoch 4861/10000, Training Loss: 0.878036379814148, Validation Loss: 0.6481850743293762\n",
      "Epoch 4862/10000, Training Loss: 0.8741825819015503, Validation Loss: 0.7779531478881836\n",
      "Epoch 4863/10000, Training Loss: 0.7920358180999756, Validation Loss: 0.6256486773490906\n",
      "Epoch 4864/10000, Training Loss: 0.800285279750824, Validation Loss: 1.0751250982284546\n",
      "Epoch 4865/10000, Training Loss: 0.8439623117446899, Validation Loss: 1.0798126459121704\n",
      "Epoch 4866/10000, Training Loss: 0.8170257210731506, Validation Loss: 0.5072126984596252\n",
      "Epoch 4867/10000, Training Loss: 0.7675238251686096, Validation Loss: 1.2971590757369995\n",
      "Epoch 4868/10000, Training Loss: 0.7869133353233337, Validation Loss: 0.6584461331367493\n",
      "Epoch 4869/10000, Training Loss: 0.7556177377700806, Validation Loss: 0.5246445536613464\n",
      "Epoch 4870/10000, Training Loss: 0.7198349833488464, Validation Loss: 0.4819171726703644\n",
      "Epoch 4871/10000, Training Loss: 0.7189366221427917, Validation Loss: 0.6532849669456482\n",
      "Epoch 4872/10000, Training Loss: 0.8665379881858826, Validation Loss: 0.91773921251297\n",
      "Epoch 4873/10000, Training Loss: 0.7068740129470825, Validation Loss: 0.7439740300178528\n",
      "Epoch 4874/10000, Training Loss: 0.8587188720703125, Validation Loss: 1.4678038358688354\n",
      "Epoch 4875/10000, Training Loss: 0.7581595182418823, Validation Loss: 0.7798507213592529\n",
      "Epoch 4876/10000, Training Loss: 1.1097157001495361, Validation Loss: 0.6185676455497742\n",
      "Epoch 4877/10000, Training Loss: 0.8678871989250183, Validation Loss: 0.7017934918403625\n",
      "Epoch 4878/10000, Training Loss: 0.7442314624786377, Validation Loss: 0.7468147873878479\n",
      "Epoch 4879/10000, Training Loss: 0.756142258644104, Validation Loss: 0.6720393300056458\n",
      "Epoch 4880/10000, Training Loss: 0.8139824271202087, Validation Loss: 0.5205349326133728\n",
      "Epoch 4881/10000, Training Loss: 0.7848863005638123, Validation Loss: 1.0692585706710815\n",
      "Epoch 4882/10000, Training Loss: 0.9297471642494202, Validation Loss: 0.7118293642997742\n",
      "Epoch 4883/10000, Training Loss: 0.8549293875694275, Validation Loss: 1.2736893892288208\n",
      "Epoch 4884/10000, Training Loss: 0.8968709111213684, Validation Loss: 0.9402594566345215\n",
      "Epoch 4885/10000, Training Loss: 0.9413985013961792, Validation Loss: 1.0217519998550415\n",
      "Epoch 4886/10000, Training Loss: 0.7302525639533997, Validation Loss: 0.5993549227714539\n",
      "Epoch 4887/10000, Training Loss: 0.7438570261001587, Validation Loss: 0.8830183148384094\n",
      "Epoch 4888/10000, Training Loss: 0.803621232509613, Validation Loss: 1.0439680814743042\n",
      "Epoch 4889/10000, Training Loss: 0.7953752279281616, Validation Loss: 1.1051424741744995\n",
      "Epoch 4890/10000, Training Loss: 0.7895374298095703, Validation Loss: 1.006750226020813\n",
      "Epoch 4891/10000, Training Loss: 0.7881100177764893, Validation Loss: 1.140624761581421\n",
      "Epoch 4892/10000, Training Loss: 0.7864548563957214, Validation Loss: 0.8752098083496094\n",
      "Epoch 4893/10000, Training Loss: 0.80583256483078, Validation Loss: 0.5777044892311096\n",
      "Epoch 4894/10000, Training Loss: 0.8914979696273804, Validation Loss: 0.8210681080818176\n",
      "Epoch 4895/10000, Training Loss: 0.808199942111969, Validation Loss: 1.1692912578582764\n",
      "Epoch 4896/10000, Training Loss: 0.7683389186859131, Validation Loss: 0.5597582459449768\n",
      "Epoch 4897/10000, Training Loss: 0.7930746674537659, Validation Loss: 0.9371901154518127\n",
      "Epoch 4898/10000, Training Loss: 0.9696857929229736, Validation Loss: 1.247456669807434\n",
      "Epoch 4899/10000, Training Loss: 0.7433494925498962, Validation Loss: 0.7173569798469543\n",
      "Epoch 4900/10000, Training Loss: 0.9425782561302185, Validation Loss: 0.4585125744342804\n",
      "Epoch 4901/10000, Training Loss: 0.810693085193634, Validation Loss: 0.8124787211418152\n",
      "Epoch 4902/10000, Training Loss: 0.7516463398933411, Validation Loss: 1.0053316354751587\n",
      "Epoch 4903/10000, Training Loss: 0.7832708954811096, Validation Loss: 0.7129087448120117\n",
      "Epoch 4904/10000, Training Loss: 0.8898677825927734, Validation Loss: 1.0937707424163818\n",
      "Epoch 4905/10000, Training Loss: 0.8361038565635681, Validation Loss: 0.690972626209259\n",
      "Epoch 4906/10000, Training Loss: 0.9261376261711121, Validation Loss: 0.7874138951301575\n",
      "Epoch 4907/10000, Training Loss: 0.6383216381072998, Validation Loss: 0.8064842820167542\n",
      "Epoch 4908/10000, Training Loss: 0.7926931977272034, Validation Loss: 0.9610159397125244\n",
      "Epoch 4909/10000, Training Loss: 0.838351309299469, Validation Loss: 0.7435321807861328\n",
      "Epoch 4910/10000, Training Loss: 0.6605473756790161, Validation Loss: 0.6715776920318604\n",
      "Epoch 4911/10000, Training Loss: 0.8129631876945496, Validation Loss: 0.684654712677002\n",
      "Epoch 4912/10000, Training Loss: 0.7972074747085571, Validation Loss: 0.8099510669708252\n",
      "Epoch 4913/10000, Training Loss: 1.024613618850708, Validation Loss: 1.2529772520065308\n",
      "Epoch 4914/10000, Training Loss: 0.811315655708313, Validation Loss: 0.7844571471214294\n",
      "Epoch 4915/10000, Training Loss: 0.7425658702850342, Validation Loss: 0.9136731624603271\n",
      "Epoch 4916/10000, Training Loss: 0.7393168210983276, Validation Loss: 0.7178258299827576\n",
      "Epoch 4917/10000, Training Loss: 0.7466541528701782, Validation Loss: 1.1918615102767944\n",
      "Epoch 4918/10000, Training Loss: 0.8536686301231384, Validation Loss: 0.8013458847999573\n",
      "Epoch 4919/10000, Training Loss: 0.8060615062713623, Validation Loss: 0.6005885004997253\n",
      "Epoch 4920/10000, Training Loss: 0.8736813068389893, Validation Loss: 1.0366616249084473\n",
      "Epoch 4921/10000, Training Loss: 0.7882616519927979, Validation Loss: 0.8054275512695312\n",
      "Epoch 4922/10000, Training Loss: 0.8262386322021484, Validation Loss: 0.6374214887619019\n",
      "Epoch 4923/10000, Training Loss: 0.7763543725013733, Validation Loss: 0.7892200350761414\n",
      "Epoch 4924/10000, Training Loss: 0.7978333830833435, Validation Loss: 0.3695228397846222\n",
      "Epoch 4925/10000, Training Loss: 0.7318716049194336, Validation Loss: 0.6879648566246033\n",
      "Epoch 4926/10000, Training Loss: 0.7712563276290894, Validation Loss: 0.9846509099006653\n",
      "Epoch 4927/10000, Training Loss: 0.7922037839889526, Validation Loss: 0.6983659267425537\n",
      "Epoch 4928/10000, Training Loss: 0.8443751931190491, Validation Loss: 0.5477768182754517\n",
      "Epoch 4929/10000, Training Loss: 0.8736228942871094, Validation Loss: 0.52801513671875\n",
      "Epoch 4930/10000, Training Loss: 0.8241596221923828, Validation Loss: 0.7978384494781494\n",
      "Epoch 4931/10000, Training Loss: 0.8530706167221069, Validation Loss: 1.1204700469970703\n",
      "Epoch 4932/10000, Training Loss: 0.6785356998443604, Validation Loss: 1.0525734424591064\n",
      "Epoch 4933/10000, Training Loss: 0.9672024250030518, Validation Loss: 0.8305413126945496\n",
      "Epoch 4934/10000, Training Loss: 0.7807549834251404, Validation Loss: 1.05325186252594\n",
      "Epoch 4935/10000, Training Loss: 0.8563105463981628, Validation Loss: 0.8687016367912292\n",
      "Epoch 4936/10000, Training Loss: 0.6856461763381958, Validation Loss: 0.5197592377662659\n",
      "Epoch 4937/10000, Training Loss: 1.067594051361084, Validation Loss: 1.4006134271621704\n",
      "Epoch 4938/10000, Training Loss: 0.9809558391571045, Validation Loss: 0.9018427729606628\n",
      "Epoch 4939/10000, Training Loss: 0.9058638215065002, Validation Loss: 2.1959726810455322\n",
      "Epoch 4940/10000, Training Loss: 0.8031410574913025, Validation Loss: 1.03204345703125\n",
      "Epoch 4941/10000, Training Loss: 0.790230393409729, Validation Loss: 0.7018463015556335\n",
      "Epoch 4942/10000, Training Loss: 0.8426307439804077, Validation Loss: 0.929331362247467\n",
      "Epoch 4943/10000, Training Loss: 0.818358838558197, Validation Loss: 1.35151207447052\n",
      "Epoch 4944/10000, Training Loss: 0.8323278427124023, Validation Loss: 0.8332967758178711\n",
      "Epoch 4945/10000, Training Loss: 0.8490667939186096, Validation Loss: 0.879586935043335\n",
      "Epoch 4946/10000, Training Loss: 0.9205477833747864, Validation Loss: 1.0796712636947632\n",
      "Epoch 4947/10000, Training Loss: 0.7214136123657227, Validation Loss: 1.2014440298080444\n",
      "Epoch 4948/10000, Training Loss: 0.8877764940261841, Validation Loss: 0.9293558597564697\n",
      "Epoch 4949/10000, Training Loss: 0.8996508717536926, Validation Loss: 1.073731541633606\n",
      "Epoch 4950/10000, Training Loss: 0.7148265242576599, Validation Loss: 0.5014888644218445\n",
      "Epoch 4951/10000, Training Loss: 0.7581348419189453, Validation Loss: 0.814474880695343\n",
      "Epoch 4952/10000, Training Loss: 0.6906493902206421, Validation Loss: 0.5340506434440613\n",
      "Epoch 4953/10000, Training Loss: 0.7357214689254761, Validation Loss: 1.4201655387878418\n",
      "Epoch 4954/10000, Training Loss: 0.8737878799438477, Validation Loss: 0.7494969964027405\n",
      "Epoch 4955/10000, Training Loss: 0.8210360407829285, Validation Loss: 1.0103569030761719\n",
      "Epoch 4956/10000, Training Loss: 0.7581692934036255, Validation Loss: 0.8164095282554626\n",
      "Epoch 4957/10000, Training Loss: 0.8515851497650146, Validation Loss: 1.3850841522216797\n",
      "Epoch 4958/10000, Training Loss: 0.7041394710540771, Validation Loss: 0.5604367852210999\n",
      "Epoch 4959/10000, Training Loss: 0.7550737857818604, Validation Loss: 0.8005609512329102\n",
      "Epoch 4960/10000, Training Loss: 0.7992795705795288, Validation Loss: 0.8211166858673096\n",
      "Epoch 4961/10000, Training Loss: 0.7680790424346924, Validation Loss: 0.6350421905517578\n",
      "Epoch 4962/10000, Training Loss: 0.8978509306907654, Validation Loss: 0.9645830988883972\n",
      "Epoch 4963/10000, Training Loss: 0.775151252746582, Validation Loss: 1.20237135887146\n",
      "Epoch 4964/10000, Training Loss: 0.8841580748558044, Validation Loss: 1.0293596982955933\n",
      "Epoch 4965/10000, Training Loss: 0.8482657074928284, Validation Loss: 0.7096186280250549\n",
      "Epoch 4966/10000, Training Loss: 0.7691565752029419, Validation Loss: 0.996532678604126\n",
      "Epoch 4967/10000, Training Loss: 0.8477575778961182, Validation Loss: 1.0489429235458374\n",
      "Epoch 4968/10000, Training Loss: 0.998429000377655, Validation Loss: 1.0059410333633423\n",
      "Epoch 4969/10000, Training Loss: 0.7371232509613037, Validation Loss: 0.9525603652000427\n",
      "Epoch 4970/10000, Training Loss: 0.8837729692459106, Validation Loss: 1.0035098791122437\n",
      "Epoch 4971/10000, Training Loss: 0.7521569728851318, Validation Loss: 1.2731022834777832\n",
      "Epoch 4972/10000, Training Loss: 0.8976306319236755, Validation Loss: 0.8508509993553162\n",
      "Epoch 4973/10000, Training Loss: 0.753881573677063, Validation Loss: 0.9072832465171814\n",
      "Epoch 4974/10000, Training Loss: 0.7898858785629272, Validation Loss: 1.139795184135437\n",
      "Epoch 4975/10000, Training Loss: 0.7726830840110779, Validation Loss: 1.1113218069076538\n",
      "Epoch 4976/10000, Training Loss: 0.8208885192871094, Validation Loss: 0.6114862561225891\n",
      "Epoch 4977/10000, Training Loss: 0.7905473709106445, Validation Loss: 1.1586880683898926\n",
      "Epoch 4978/10000, Training Loss: 0.7262607216835022, Validation Loss: 0.9035859704017639\n",
      "Epoch 4979/10000, Training Loss: 0.8300862908363342, Validation Loss: 0.5498520135879517\n",
      "Epoch 4980/10000, Training Loss: 0.8993089199066162, Validation Loss: 1.109931230545044\n",
      "Epoch 4981/10000, Training Loss: 0.6996867656707764, Validation Loss: 0.7503959536552429\n",
      "Epoch 4982/10000, Training Loss: 0.8388671278953552, Validation Loss: 0.8216341137886047\n",
      "Epoch 4983/10000, Training Loss: 0.8136734366416931, Validation Loss: 0.5803424715995789\n",
      "Epoch 4984/10000, Training Loss: 0.7275301218032837, Validation Loss: 0.6640921235084534\n",
      "Epoch 4985/10000, Training Loss: 0.8331482410430908, Validation Loss: 0.8792594075202942\n",
      "Epoch 4986/10000, Training Loss: 0.7565997838973999, Validation Loss: 0.879446268081665\n",
      "Epoch 4987/10000, Training Loss: 0.7209339737892151, Validation Loss: 0.9059353470802307\n",
      "Epoch 4988/10000, Training Loss: 0.7486518621444702, Validation Loss: 0.9646251797676086\n",
      "Epoch 4989/10000, Training Loss: 0.8043131232261658, Validation Loss: 0.8050135970115662\n",
      "Epoch 4990/10000, Training Loss: 0.8176866769790649, Validation Loss: 1.0236150026321411\n",
      "Epoch 4991/10000, Training Loss: 0.7835988998413086, Validation Loss: 0.9052433967590332\n",
      "Epoch 4992/10000, Training Loss: 0.8693943619728088, Validation Loss: 1.066633701324463\n",
      "Epoch 4993/10000, Training Loss: 0.7527784109115601, Validation Loss: 0.6902024745941162\n",
      "Epoch 4994/10000, Training Loss: 0.789356529712677, Validation Loss: 1.5596460103988647\n",
      "Epoch 4995/10000, Training Loss: 0.7353162169456482, Validation Loss: 1.184531331062317\n",
      "Epoch 4996/10000, Training Loss: 0.8548631072044373, Validation Loss: 1.2292484045028687\n",
      "Epoch 4997/10000, Training Loss: 0.8611549735069275, Validation Loss: 0.9762467741966248\n",
      "Epoch 4998/10000, Training Loss: 0.7353400588035583, Validation Loss: 0.7366060614585876\n",
      "Epoch 4999/10000, Training Loss: 0.8692451119422913, Validation Loss: 0.8180812001228333\n",
      "Epoch 5000/10000, Training Loss: 0.9045553207397461, Validation Loss: 1.10767662525177\n",
      "Epoch 5001/10000, Training Loss: 0.7301458716392517, Validation Loss: 0.8054931163787842\n",
      "Epoch 5002/10000, Training Loss: 0.8405110836029053, Validation Loss: 0.7477075457572937\n",
      "Epoch 5003/10000, Training Loss: 0.8995645046234131, Validation Loss: 1.2601391077041626\n",
      "Epoch 5004/10000, Training Loss: 0.7772579789161682, Validation Loss: 0.7961084842681885\n",
      "Epoch 5005/10000, Training Loss: 0.7254889011383057, Validation Loss: 0.8601207137107849\n",
      "Epoch 5006/10000, Training Loss: 0.7853711843490601, Validation Loss: 1.0040184259414673\n",
      "Epoch 5007/10000, Training Loss: 0.9328142404556274, Validation Loss: 0.6511178612709045\n",
      "Epoch 5008/10000, Training Loss: 0.7431158423423767, Validation Loss: 0.6557372808456421\n",
      "Epoch 5009/10000, Training Loss: 0.7751789093017578, Validation Loss: 0.7774350643157959\n",
      "Epoch 5010/10000, Training Loss: 0.7005476355552673, Validation Loss: 0.8975098729133606\n",
      "Epoch 5011/10000, Training Loss: 0.8291771411895752, Validation Loss: 0.5013253092765808\n",
      "Epoch 5012/10000, Training Loss: 0.7849546074867249, Validation Loss: 0.8958585858345032\n",
      "Epoch 5013/10000, Training Loss: 0.8248641490936279, Validation Loss: 0.5886344909667969\n",
      "Epoch 5014/10000, Training Loss: 0.7346653342247009, Validation Loss: 0.5340074896812439\n",
      "Epoch 5015/10000, Training Loss: 0.8662108778953552, Validation Loss: 0.9884669184684753\n",
      "Epoch 5016/10000, Training Loss: 0.6726362705230713, Validation Loss: 0.66914302110672\n",
      "Epoch 5017/10000, Training Loss: 1.0106544494628906, Validation Loss: 1.1918469667434692\n",
      "Epoch 5018/10000, Training Loss: 0.8310269713401794, Validation Loss: 0.46781763434410095\n",
      "Epoch 5019/10000, Training Loss: 0.8120240569114685, Validation Loss: 0.730208694934845\n",
      "Epoch 5020/10000, Training Loss: 0.7051678895950317, Validation Loss: 0.6452250480651855\n",
      "Epoch 5021/10000, Training Loss: 0.7563304901123047, Validation Loss: 1.3432873487472534\n",
      "Epoch 5022/10000, Training Loss: 0.8334341049194336, Validation Loss: 0.882483184337616\n",
      "Epoch 5023/10000, Training Loss: 0.7498137950897217, Validation Loss: 0.8302893042564392\n",
      "Epoch 5024/10000, Training Loss: 0.7165015935897827, Validation Loss: 0.9004237055778503\n",
      "Epoch 5025/10000, Training Loss: 0.8090189695358276, Validation Loss: 1.0194796323776245\n",
      "Epoch 5026/10000, Training Loss: 0.7383060455322266, Validation Loss: 1.1039122343063354\n",
      "Epoch 5027/10000, Training Loss: 0.8697319030761719, Validation Loss: 1.2941961288452148\n",
      "Epoch 5028/10000, Training Loss: 0.8239670991897583, Validation Loss: 1.0970607995986938\n",
      "Epoch 5029/10000, Training Loss: 0.7937244176864624, Validation Loss: 0.7453393340110779\n",
      "Epoch 5030/10000, Training Loss: 0.7508056163787842, Validation Loss: 0.7186298370361328\n",
      "Epoch 5031/10000, Training Loss: 0.7425190210342407, Validation Loss: 0.7012308239936829\n",
      "Epoch 5032/10000, Training Loss: 0.8165131211280823, Validation Loss: 0.8250188231468201\n",
      "Epoch 5033/10000, Training Loss: 0.7907184362411499, Validation Loss: 1.1892708539962769\n",
      "Epoch 5034/10000, Training Loss: 0.7229357361793518, Validation Loss: 0.8207128047943115\n",
      "Epoch 5035/10000, Training Loss: 0.8240130543708801, Validation Loss: 1.2557032108306885\n",
      "Epoch 5036/10000, Training Loss: 0.7455801367759705, Validation Loss: 0.919427216053009\n",
      "Epoch 5037/10000, Training Loss: 0.9123870134353638, Validation Loss: 0.8474752902984619\n",
      "Epoch 5038/10000, Training Loss: 0.7582777738571167, Validation Loss: 1.1748040914535522\n",
      "Epoch 5039/10000, Training Loss: 0.742011308670044, Validation Loss: 0.8345692157745361\n",
      "Epoch 5040/10000, Training Loss: 0.6688668727874756, Validation Loss: 1.094231128692627\n",
      "Epoch 5041/10000, Training Loss: 0.9455620050430298, Validation Loss: 1.9393596649169922\n",
      "Epoch 5042/10000, Training Loss: 0.7705735564231873, Validation Loss: 0.8770273327827454\n",
      "Epoch 5043/10000, Training Loss: 0.7682403326034546, Validation Loss: 1.4343786239624023\n",
      "Epoch 5044/10000, Training Loss: 0.8593751192092896, Validation Loss: 0.6852967143058777\n",
      "Epoch 5045/10000, Training Loss: 0.6809290647506714, Validation Loss: 0.9452721476554871\n",
      "Epoch 5046/10000, Training Loss: 0.8391357660293579, Validation Loss: 0.9437983632087708\n",
      "Epoch 5047/10000, Training Loss: 0.8005103468894958, Validation Loss: 1.1149996519088745\n",
      "Epoch 5048/10000, Training Loss: 0.9099727272987366, Validation Loss: 1.1404904127120972\n",
      "Epoch 5049/10000, Training Loss: 0.880468487739563, Validation Loss: 0.6187682151794434\n",
      "Epoch 5050/10000, Training Loss: 0.8534863591194153, Validation Loss: 0.5151041150093079\n",
      "Epoch 5051/10000, Training Loss: 0.7093510031700134, Validation Loss: 0.8430576920509338\n",
      "Epoch 5052/10000, Training Loss: 0.7742717266082764, Validation Loss: 0.8391769528388977\n",
      "Epoch 5053/10000, Training Loss: 0.8123478293418884, Validation Loss: 0.5352982878684998\n",
      "Epoch 5054/10000, Training Loss: 0.859214723110199, Validation Loss: 0.8659922480583191\n",
      "Epoch 5055/10000, Training Loss: 0.8498767018318176, Validation Loss: 1.0584832429885864\n",
      "Epoch 5056/10000, Training Loss: 0.769943118095398, Validation Loss: 0.7112007737159729\n",
      "Epoch 5057/10000, Training Loss: 0.7107734084129333, Validation Loss: 0.6704208254814148\n",
      "Epoch 5058/10000, Training Loss: 0.7756681442260742, Validation Loss: 0.6146212816238403\n",
      "Epoch 5059/10000, Training Loss: 0.719929575920105, Validation Loss: 0.5154648423194885\n",
      "Epoch 5060/10000, Training Loss: 0.7956230640411377, Validation Loss: 0.7742931842803955\n",
      "Epoch 5061/10000, Training Loss: 0.7290098667144775, Validation Loss: 0.6793769001960754\n",
      "Epoch 5062/10000, Training Loss: 0.8564449548721313, Validation Loss: 0.9033476710319519\n",
      "Epoch 5063/10000, Training Loss: 0.8894652724266052, Validation Loss: 1.0050102472305298\n",
      "Epoch 5064/10000, Training Loss: 0.8032321333885193, Validation Loss: 1.037144422531128\n",
      "Epoch 5065/10000, Training Loss: 0.82197505235672, Validation Loss: 0.9457966685295105\n",
      "Epoch 5066/10000, Training Loss: 0.7736351490020752, Validation Loss: 0.9598727822303772\n",
      "Epoch 5067/10000, Training Loss: 0.7762576341629028, Validation Loss: 0.4930157959461212\n",
      "Epoch 5068/10000, Training Loss: 0.7503753304481506, Validation Loss: 1.0177243947982788\n",
      "Epoch 5069/10000, Training Loss: 0.7250560522079468, Validation Loss: 0.6840450167655945\n",
      "Epoch 5070/10000, Training Loss: 0.7633143067359924, Validation Loss: 0.5706587433815002\n",
      "Epoch 5071/10000, Training Loss: 0.7668739557266235, Validation Loss: 0.6527268886566162\n",
      "Epoch 5072/10000, Training Loss: 0.7180776000022888, Validation Loss: 0.4900892972946167\n",
      "Epoch 5073/10000, Training Loss: 0.7877867817878723, Validation Loss: 1.0751228332519531\n",
      "Epoch 5074/10000, Training Loss: 0.7031587362289429, Validation Loss: 1.0652610063552856\n",
      "Epoch 5075/10000, Training Loss: 0.8341283798217773, Validation Loss: 0.7169174551963806\n",
      "Epoch 5076/10000, Training Loss: 0.7989107370376587, Validation Loss: 1.2217141389846802\n",
      "Epoch 5077/10000, Training Loss: 0.7153645753860474, Validation Loss: 0.7825209498405457\n",
      "Epoch 5078/10000, Training Loss: 0.748319685459137, Validation Loss: 0.806812584400177\n",
      "Epoch 5079/10000, Training Loss: 0.8212158679962158, Validation Loss: 0.8698382377624512\n",
      "Epoch 5080/10000, Training Loss: 0.8190049529075623, Validation Loss: 1.0841261148452759\n",
      "Epoch 5081/10000, Training Loss: 0.677954375743866, Validation Loss: 0.8958547115325928\n",
      "Epoch 5082/10000, Training Loss: 0.785982072353363, Validation Loss: 0.8616837859153748\n",
      "Epoch 5083/10000, Training Loss: 0.7078166604042053, Validation Loss: 0.9232325553894043\n",
      "Epoch 5084/10000, Training Loss: 0.7559134364128113, Validation Loss: 0.9967758655548096\n",
      "Epoch 5085/10000, Training Loss: 0.7730463743209839, Validation Loss: 1.0703169107437134\n",
      "Epoch 5086/10000, Training Loss: 0.7224330306053162, Validation Loss: 1.176396131515503\n",
      "Epoch 5087/10000, Training Loss: 0.7786706686019897, Validation Loss: 0.8403144478797913\n",
      "Epoch 5088/10000, Training Loss: 0.8453003168106079, Validation Loss: 1.0713940858840942\n",
      "Epoch 5089/10000, Training Loss: 0.8165642619132996, Validation Loss: 0.7830731272697449\n",
      "Epoch 5090/10000, Training Loss: 0.9491263031959534, Validation Loss: 1.3017669916152954\n",
      "Epoch 5091/10000, Training Loss: 0.728252649307251, Validation Loss: 0.6859574913978577\n",
      "Epoch 5092/10000, Training Loss: 0.7205162644386292, Validation Loss: 0.7711606621742249\n",
      "Epoch 5093/10000, Training Loss: 0.7122441530227661, Validation Loss: 0.541205644607544\n",
      "Epoch 5094/10000, Training Loss: 0.8499498963356018, Validation Loss: 0.7212438583374023\n",
      "Epoch 5095/10000, Training Loss: 0.8053691387176514, Validation Loss: 0.6808032989501953\n",
      "Epoch 5096/10000, Training Loss: 0.826027512550354, Validation Loss: 1.0027823448181152\n",
      "Epoch 5097/10000, Training Loss: 0.7333256006240845, Validation Loss: 0.6273868083953857\n",
      "Epoch 5098/10000, Training Loss: 0.7938912510871887, Validation Loss: 0.9719083309173584\n",
      "Epoch 5099/10000, Training Loss: 0.7482872009277344, Validation Loss: 0.9524766802787781\n",
      "Epoch 5100/10000, Training Loss: 0.64133220911026, Validation Loss: 0.8720118403434753\n",
      "Epoch 5101/10000, Training Loss: 0.763205885887146, Validation Loss: 0.9321961402893066\n",
      "Epoch 5102/10000, Training Loss: 0.7673546671867371, Validation Loss: 1.07066810131073\n",
      "Epoch 5103/10000, Training Loss: 0.6974289417266846, Validation Loss: 0.8739595413208008\n",
      "Epoch 5104/10000, Training Loss: 0.7836058735847473, Validation Loss: 1.0765191316604614\n",
      "Epoch 5105/10000, Training Loss: 0.7031164765357971, Validation Loss: 1.2838821411132812\n",
      "Epoch 5106/10000, Training Loss: 0.7434860467910767, Validation Loss: 0.9540172219276428\n",
      "Epoch 5107/10000, Training Loss: 0.8955270051956177, Validation Loss: 0.666499674320221\n",
      "Epoch 5108/10000, Training Loss: 0.832099199295044, Validation Loss: 1.1367565393447876\n",
      "Epoch 5109/10000, Training Loss: 0.7158399224281311, Validation Loss: 0.836405336856842\n",
      "Epoch 5110/10000, Training Loss: 0.7416566610336304, Validation Loss: 0.742042064666748\n",
      "Epoch 5111/10000, Training Loss: 0.7293561697006226, Validation Loss: 0.6113825440406799\n",
      "Epoch 5112/10000, Training Loss: 0.8937109708786011, Validation Loss: 0.6879025101661682\n",
      "Epoch 5113/10000, Training Loss: 0.7967580556869507, Validation Loss: 1.0342735052108765\n",
      "Epoch 5114/10000, Training Loss: 0.7956202030181885, Validation Loss: 0.7181915640830994\n",
      "Epoch 5115/10000, Training Loss: 0.8435227870941162, Validation Loss: 1.0147249698638916\n",
      "Epoch 5116/10000, Training Loss: 0.7058519721031189, Validation Loss: 1.0588377714157104\n",
      "Epoch 5117/10000, Training Loss: 0.7474758625030518, Validation Loss: 1.28715980052948\n",
      "Epoch 5118/10000, Training Loss: 0.7584031224250793, Validation Loss: 1.198106288909912\n",
      "Epoch 5119/10000, Training Loss: 0.8106809854507446, Validation Loss: 0.6754236817359924\n",
      "Epoch 5120/10000, Training Loss: 0.7655355930328369, Validation Loss: 1.015961766242981\n",
      "Epoch 5121/10000, Training Loss: 0.7899843454360962, Validation Loss: 0.7121418118476868\n",
      "Epoch 5122/10000, Training Loss: 0.754045844078064, Validation Loss: 1.5440958738327026\n",
      "Epoch 5123/10000, Training Loss: 0.7723273038864136, Validation Loss: 0.8742566108703613\n",
      "Epoch 5124/10000, Training Loss: 0.7372971773147583, Validation Loss: 0.7704021334648132\n",
      "Epoch 5125/10000, Training Loss: 0.7134827971458435, Validation Loss: 0.6369783282279968\n",
      "Epoch 5126/10000, Training Loss: 0.6818224191665649, Validation Loss: 0.7389967441558838\n",
      "Epoch 5127/10000, Training Loss: 0.7896143198013306, Validation Loss: 0.663119912147522\n",
      "Epoch 5128/10000, Training Loss: 0.7351728677749634, Validation Loss: 0.6518837809562683\n",
      "Epoch 5129/10000, Training Loss: 0.8232914209365845, Validation Loss: 1.117936134338379\n",
      "Epoch 5130/10000, Training Loss: 0.8280155658721924, Validation Loss: 0.9090860486030579\n",
      "Epoch 5131/10000, Training Loss: 0.7075232267379761, Validation Loss: 1.1980446577072144\n",
      "Epoch 5132/10000, Training Loss: 0.869752049446106, Validation Loss: 1.1946200132369995\n",
      "Epoch 5133/10000, Training Loss: 0.763299822807312, Validation Loss: 0.8411113619804382\n",
      "Epoch 5134/10000, Training Loss: 0.7203046083450317, Validation Loss: 0.9157018065452576\n",
      "Epoch 5135/10000, Training Loss: 0.79279625415802, Validation Loss: 0.6609497666358948\n",
      "Epoch 5136/10000, Training Loss: 0.7778133153915405, Validation Loss: 1.2210474014282227\n",
      "Epoch 5137/10000, Training Loss: 0.7203734517097473, Validation Loss: 1.0390039682388306\n",
      "Epoch 5138/10000, Training Loss: 0.7100169062614441, Validation Loss: 0.7118659615516663\n",
      "Epoch 5139/10000, Training Loss: 0.7735138535499573, Validation Loss: 0.6579316854476929\n",
      "Epoch 5140/10000, Training Loss: 0.7849019169807434, Validation Loss: 0.8617462515830994\n",
      "Epoch 5141/10000, Training Loss: 0.7336907982826233, Validation Loss: 0.6287439465522766\n",
      "Epoch 5142/10000, Training Loss: 0.7254238128662109, Validation Loss: 0.8875334858894348\n",
      "Epoch 5143/10000, Training Loss: 0.8311060667037964, Validation Loss: 0.7507345080375671\n",
      "Epoch 5144/10000, Training Loss: 0.7769718170166016, Validation Loss: 0.5554654002189636\n",
      "Epoch 5145/10000, Training Loss: 0.716390073299408, Validation Loss: 0.783696711063385\n",
      "Epoch 5146/10000, Training Loss: 0.7689610123634338, Validation Loss: 0.6168222427368164\n",
      "Epoch 5147/10000, Training Loss: 0.7453807592391968, Validation Loss: 0.8888063430786133\n",
      "Epoch 5148/10000, Training Loss: 0.7539272308349609, Validation Loss: 1.1121827363967896\n",
      "Epoch 5149/10000, Training Loss: 0.7866640686988831, Validation Loss: 1.1097410917282104\n",
      "Epoch 5150/10000, Training Loss: 0.7360565662384033, Validation Loss: 1.1822196245193481\n",
      "Epoch 5151/10000, Training Loss: 0.8858883380889893, Validation Loss: 0.6956527829170227\n",
      "Epoch 5152/10000, Training Loss: 0.7263159155845642, Validation Loss: 0.6173115372657776\n",
      "Epoch 5153/10000, Training Loss: 0.7165136933326721, Validation Loss: 0.8169825673103333\n",
      "Epoch 5154/10000, Training Loss: 0.8847361207008362, Validation Loss: 1.1578428745269775\n",
      "Epoch 5155/10000, Training Loss: 0.7113765478134155, Validation Loss: 1.0196599960327148\n",
      "Epoch 5156/10000, Training Loss: 0.7937619686126709, Validation Loss: 1.0140008926391602\n",
      "Epoch 5157/10000, Training Loss: 0.7515326142311096, Validation Loss: 0.5315539240837097\n",
      "Epoch 5158/10000, Training Loss: 0.85068279504776, Validation Loss: 0.9042870402336121\n",
      "Epoch 5159/10000, Training Loss: 0.7045613527297974, Validation Loss: 0.5444625020027161\n",
      "Epoch 5160/10000, Training Loss: 0.7188886404037476, Validation Loss: 0.6073736548423767\n",
      "Epoch 5161/10000, Training Loss: 0.8171141147613525, Validation Loss: 0.8424043655395508\n",
      "Epoch 5162/10000, Training Loss: 0.6935471892356873, Validation Loss: 1.1143882274627686\n",
      "Epoch 5163/10000, Training Loss: 0.7134634852409363, Validation Loss: 1.2013635635375977\n",
      "Epoch 5164/10000, Training Loss: 0.7548908591270447, Validation Loss: 0.8521609902381897\n",
      "Epoch 5165/10000, Training Loss: 0.6843106150627136, Validation Loss: 0.9092414379119873\n",
      "Epoch 5166/10000, Training Loss: 0.7773867249488831, Validation Loss: 0.8776872754096985\n",
      "Epoch 5167/10000, Training Loss: 0.7146618366241455, Validation Loss: 0.6801429390907288\n",
      "Epoch 5168/10000, Training Loss: 0.7308514714241028, Validation Loss: 0.9107694029808044\n",
      "Epoch 5169/10000, Training Loss: 0.7783670425415039, Validation Loss: 1.1841598749160767\n",
      "Epoch 5170/10000, Training Loss: 0.6760963201522827, Validation Loss: 0.620543360710144\n",
      "Epoch 5171/10000, Training Loss: 0.7908462882041931, Validation Loss: 1.2396284341812134\n",
      "Epoch 5172/10000, Training Loss: 0.760367751121521, Validation Loss: 1.158742070198059\n",
      "Epoch 5173/10000, Training Loss: 0.7878417372703552, Validation Loss: 0.8472111821174622\n",
      "Epoch 5174/10000, Training Loss: 0.7893173694610596, Validation Loss: 0.7702052593231201\n",
      "Epoch 5175/10000, Training Loss: 0.6185532212257385, Validation Loss: 0.8058357834815979\n",
      "Epoch 5176/10000, Training Loss: 0.7359127402305603, Validation Loss: 1.1140419244766235\n",
      "Epoch 5177/10000, Training Loss: 0.89264976978302, Validation Loss: 0.7707894444465637\n",
      "Epoch 5178/10000, Training Loss: 0.7825829982757568, Validation Loss: 0.8520843386650085\n",
      "Epoch 5179/10000, Training Loss: 0.7085388898849487, Validation Loss: 0.6993399262428284\n",
      "Epoch 5180/10000, Training Loss: 0.7055009007453918, Validation Loss: 1.010278582572937\n",
      "Epoch 5181/10000, Training Loss: 0.8431882858276367, Validation Loss: 1.097525954246521\n",
      "Epoch 5182/10000, Training Loss: 0.7483832836151123, Validation Loss: 0.6883017420768738\n",
      "Epoch 5183/10000, Training Loss: 0.7244390249252319, Validation Loss: 0.8450465202331543\n",
      "Epoch 5184/10000, Training Loss: 0.7165305018424988, Validation Loss: 1.1321285963058472\n",
      "Epoch 5185/10000, Training Loss: 0.7963672280311584, Validation Loss: 0.6742865443229675\n",
      "Epoch 5186/10000, Training Loss: 0.7094733715057373, Validation Loss: 0.8411651253700256\n",
      "Epoch 5187/10000, Training Loss: 0.6761747598648071, Validation Loss: 1.1221014261245728\n",
      "Epoch 5188/10000, Training Loss: 0.8019112944602966, Validation Loss: 0.5843544006347656\n",
      "Epoch 5189/10000, Training Loss: 0.7347959280014038, Validation Loss: 0.727240800857544\n",
      "Epoch 5190/10000, Training Loss: 0.8175023794174194, Validation Loss: 1.014675498008728\n",
      "Epoch 5191/10000, Training Loss: 0.6899245977401733, Validation Loss: 0.6814368367195129\n",
      "Epoch 5192/10000, Training Loss: 0.7416858077049255, Validation Loss: 0.7234176993370056\n",
      "Epoch 5193/10000, Training Loss: 0.7045779824256897, Validation Loss: 0.999018132686615\n",
      "Epoch 5194/10000, Training Loss: 0.7036766409873962, Validation Loss: 0.7824752926826477\n",
      "Epoch 5195/10000, Training Loss: 0.742633581161499, Validation Loss: 0.8352129459381104\n",
      "Epoch 5196/10000, Training Loss: 0.7273508310317993, Validation Loss: 0.7793956398963928\n",
      "Epoch 5197/10000, Training Loss: 0.8601644039154053, Validation Loss: 0.7314419150352478\n",
      "Epoch 5198/10000, Training Loss: 0.738675057888031, Validation Loss: 0.8537247180938721\n",
      "Epoch 5199/10000, Training Loss: 0.9044564962387085, Validation Loss: 0.727651834487915\n",
      "Epoch 5200/10000, Training Loss: 0.852455198764801, Validation Loss: 0.5428780317306519\n",
      "Epoch 5201/10000, Training Loss: 0.663415789604187, Validation Loss: 0.7157342433929443\n",
      "Epoch 5202/10000, Training Loss: 0.8196754455566406, Validation Loss: 1.4221731424331665\n",
      "Epoch 5203/10000, Training Loss: 0.7391964197158813, Validation Loss: 0.6881888508796692\n",
      "Epoch 5204/10000, Training Loss: 0.7689791321754456, Validation Loss: 0.7238553166389465\n",
      "Epoch 5205/10000, Training Loss: 0.7406014800071716, Validation Loss: 0.7224054932594299\n",
      "Epoch 5206/10000, Training Loss: 0.8443349599838257, Validation Loss: 1.241738200187683\n",
      "Epoch 5207/10000, Training Loss: 0.7534779906272888, Validation Loss: 0.7130480408668518\n",
      "Epoch 5208/10000, Training Loss: 0.7712593078613281, Validation Loss: 1.2951282262802124\n",
      "Epoch 5209/10000, Training Loss: 0.7477697134017944, Validation Loss: 0.778348982334137\n",
      "Epoch 5210/10000, Training Loss: 0.77586430311203, Validation Loss: 0.9510858654975891\n",
      "Epoch 5211/10000, Training Loss: 0.838280439376831, Validation Loss: 1.331749439239502\n",
      "Epoch 5212/10000, Training Loss: 0.7066614627838135, Validation Loss: 0.9868202209472656\n",
      "Epoch 5213/10000, Training Loss: 0.748816728591919, Validation Loss: 1.969279170036316\n",
      "Epoch 5214/10000, Training Loss: 0.6902236342430115, Validation Loss: 0.5548437237739563\n",
      "Epoch 5215/10000, Training Loss: 0.7909849882125854, Validation Loss: 0.6475609540939331\n",
      "Epoch 5216/10000, Training Loss: 0.8463544845581055, Validation Loss: 1.0627864599227905\n",
      "Epoch 5217/10000, Training Loss: 0.7434247732162476, Validation Loss: 0.6442201733589172\n",
      "Epoch 5218/10000, Training Loss: 0.8174382448196411, Validation Loss: 0.67411869764328\n",
      "Epoch 5219/10000, Training Loss: 0.7119547724723816, Validation Loss: 0.8620254993438721\n",
      "Epoch 5220/10000, Training Loss: 0.7380490899085999, Validation Loss: 1.2169429063796997\n",
      "Epoch 5221/10000, Training Loss: 0.7618879675865173, Validation Loss: 0.9539766907691956\n",
      "Epoch 5222/10000, Training Loss: 0.8028841018676758, Validation Loss: 1.1871296167373657\n",
      "Epoch 5223/10000, Training Loss: 0.7978440523147583, Validation Loss: 0.4973005950450897\n",
      "Epoch 5224/10000, Training Loss: 0.7807415723800659, Validation Loss: 0.6659148931503296\n",
      "Epoch 5225/10000, Training Loss: 0.9968904256820679, Validation Loss: 2.0351829528808594\n",
      "Epoch 5226/10000, Training Loss: 0.7289644479751587, Validation Loss: 0.8219346404075623\n",
      "Epoch 5227/10000, Training Loss: 0.8391773700714111, Validation Loss: 0.9699289202690125\n",
      "Epoch 5228/10000, Training Loss: 0.7372530698776245, Validation Loss: 0.46790704131126404\n",
      "Epoch 5229/10000, Training Loss: 0.7863738536834717, Validation Loss: 0.5926045775413513\n",
      "Epoch 5230/10000, Training Loss: 0.8100157976150513, Validation Loss: 0.7867327332496643\n",
      "Epoch 5231/10000, Training Loss: 0.8947011232376099, Validation Loss: 0.8383212089538574\n",
      "Epoch 5232/10000, Training Loss: 0.8287702202796936, Validation Loss: 1.1255911588668823\n",
      "Epoch 5233/10000, Training Loss: 0.7460812926292419, Validation Loss: 0.7904362678527832\n",
      "Epoch 5234/10000, Training Loss: 0.7674548029899597, Validation Loss: 0.6961250901222229\n",
      "Epoch 5235/10000, Training Loss: 0.8421639204025269, Validation Loss: 0.866030752658844\n",
      "Epoch 5236/10000, Training Loss: 0.738740086555481, Validation Loss: 1.0296940803527832\n",
      "Epoch 5237/10000, Training Loss: 0.7347449064254761, Validation Loss: 0.8518152832984924\n",
      "Epoch 5238/10000, Training Loss: 0.7509915232658386, Validation Loss: 0.7200058102607727\n",
      "Epoch 5239/10000, Training Loss: 0.7509390115737915, Validation Loss: 1.0759021043777466\n",
      "Epoch 5240/10000, Training Loss: 0.7330248951911926, Validation Loss: 1.2881317138671875\n",
      "Epoch 5241/10000, Training Loss: 0.6757226586341858, Validation Loss: 0.6746387481689453\n",
      "Epoch 5242/10000, Training Loss: 0.7505730986595154, Validation Loss: 0.9608214497566223\n",
      "Epoch 5243/10000, Training Loss: 0.8587796092033386, Validation Loss: 0.9744659066200256\n",
      "Epoch 5244/10000, Training Loss: 0.6796834468841553, Validation Loss: 0.9800012111663818\n",
      "Epoch 5245/10000, Training Loss: 0.7277403473854065, Validation Loss: 0.825380265712738\n",
      "Epoch 5246/10000, Training Loss: 0.9008749723434448, Validation Loss: 0.5826647877693176\n",
      "Epoch 5247/10000, Training Loss: 0.7797537446022034, Validation Loss: 0.7794986367225647\n",
      "Epoch 5248/10000, Training Loss: 0.7425318956375122, Validation Loss: 0.7791114449501038\n",
      "Epoch 5249/10000, Training Loss: 0.7252116799354553, Validation Loss: 0.6511826515197754\n",
      "Epoch 5250/10000, Training Loss: 0.6637591123580933, Validation Loss: 0.8844811320304871\n",
      "Epoch 5251/10000, Training Loss: 0.7444092631340027, Validation Loss: 1.9439295530319214\n",
      "Epoch 5252/10000, Training Loss: 0.6927909255027771, Validation Loss: 0.8209217190742493\n",
      "Epoch 5253/10000, Training Loss: 0.7292615175247192, Validation Loss: 0.789275586605072\n",
      "Epoch 5254/10000, Training Loss: 0.7094932794570923, Validation Loss: 1.0853211879730225\n",
      "Epoch 5255/10000, Training Loss: 0.9006569981575012, Validation Loss: 1.2429143190383911\n",
      "Epoch 5256/10000, Training Loss: 0.7422050833702087, Validation Loss: 1.343571662902832\n",
      "Epoch 5257/10000, Training Loss: 0.692650556564331, Validation Loss: 0.7451000213623047\n",
      "Epoch 5258/10000, Training Loss: 0.7400588393211365, Validation Loss: 0.7377623915672302\n",
      "Epoch 5259/10000, Training Loss: 0.8176626563072205, Validation Loss: 1.1036189794540405\n",
      "Epoch 5260/10000, Training Loss: 0.7348109483718872, Validation Loss: 0.6402867436408997\n",
      "Epoch 5261/10000, Training Loss: 0.7393128275871277, Validation Loss: 0.920968770980835\n",
      "Epoch 5262/10000, Training Loss: 0.7035027146339417, Validation Loss: 0.8967157006263733\n",
      "Epoch 5263/10000, Training Loss: 0.7231124043464661, Validation Loss: 0.8184814453125\n",
      "Epoch 5264/10000, Training Loss: 0.7183669805526733, Validation Loss: 0.694648027420044\n",
      "Epoch 5265/10000, Training Loss: 0.7208268046379089, Validation Loss: 0.8103046417236328\n",
      "Epoch 5266/10000, Training Loss: 0.7675084471702576, Validation Loss: 0.8373667597770691\n",
      "Epoch 5267/10000, Training Loss: 0.7709342241287231, Validation Loss: 0.5187369585037231\n",
      "Epoch 5268/10000, Training Loss: 0.6827813386917114, Validation Loss: 0.8626234531402588\n",
      "Epoch 5269/10000, Training Loss: 0.8174136877059937, Validation Loss: 0.6504858136177063\n",
      "Epoch 5270/10000, Training Loss: 0.7562519907951355, Validation Loss: 0.7810013294219971\n",
      "Epoch 5271/10000, Training Loss: 0.7200517058372498, Validation Loss: 0.7799983620643616\n",
      "Epoch 5272/10000, Training Loss: 0.7372876405715942, Validation Loss: 0.6326763033866882\n",
      "Epoch 5273/10000, Training Loss: 0.7816376090049744, Validation Loss: 0.7251481413841248\n",
      "Epoch 5274/10000, Training Loss: 0.7720272541046143, Validation Loss: 0.683236300945282\n",
      "Epoch 5275/10000, Training Loss: 0.6976422667503357, Validation Loss: 0.4864148199558258\n",
      "Epoch 5276/10000, Training Loss: 0.85177081823349, Validation Loss: 0.9033837914466858\n",
      "Epoch 5277/10000, Training Loss: 0.7972675561904907, Validation Loss: 0.769993782043457\n",
      "Epoch 5278/10000, Training Loss: 0.7767571210861206, Validation Loss: 0.6840593218803406\n",
      "Epoch 5279/10000, Training Loss: 0.745508074760437, Validation Loss: 0.5489607453346252\n",
      "Epoch 5280/10000, Training Loss: 0.7368746995925903, Validation Loss: 0.6459067463874817\n",
      "Epoch 5281/10000, Training Loss: 0.6066044569015503, Validation Loss: 0.6735460162162781\n",
      "Epoch 5282/10000, Training Loss: 0.8130692839622498, Validation Loss: 0.7463741302490234\n",
      "Epoch 5283/10000, Training Loss: 0.7440162301063538, Validation Loss: 0.8462367057800293\n",
      "Epoch 5284/10000, Training Loss: 0.7271025776863098, Validation Loss: 1.0365612506866455\n",
      "Epoch 5285/10000, Training Loss: 0.7375984787940979, Validation Loss: 0.5749124884605408\n",
      "Epoch 5286/10000, Training Loss: 0.7272393107414246, Validation Loss: 0.7704293131828308\n",
      "Epoch 5287/10000, Training Loss: 0.7180961966514587, Validation Loss: 1.0181866884231567\n",
      "Epoch 5288/10000, Training Loss: 0.7390010356903076, Validation Loss: 0.8214936256408691\n",
      "Epoch 5289/10000, Training Loss: 0.7241165637969971, Validation Loss: 0.5701844096183777\n",
      "Epoch 5290/10000, Training Loss: 0.7456598281860352, Validation Loss: 0.4801192581653595\n",
      "Epoch 5291/10000, Training Loss: 0.7110996246337891, Validation Loss: 1.1790567636489868\n",
      "Epoch 5292/10000, Training Loss: 0.722217857837677, Validation Loss: 1.0094621181488037\n",
      "Epoch 5293/10000, Training Loss: 0.7587836980819702, Validation Loss: 0.6884241700172424\n",
      "Epoch 5294/10000, Training Loss: 0.7250194549560547, Validation Loss: 0.5746736526489258\n",
      "Epoch 5295/10000, Training Loss: 0.7825482487678528, Validation Loss: 0.8758004307746887\n",
      "Epoch 5296/10000, Training Loss: 0.7281337380409241, Validation Loss: 0.5198966860771179\n",
      "Epoch 5297/10000, Training Loss: 0.7952626347541809, Validation Loss: 0.8171096444129944\n",
      "Epoch 5298/10000, Training Loss: 0.7801221609115601, Validation Loss: 0.8418933749198914\n",
      "Epoch 5299/10000, Training Loss: 0.6556790471076965, Validation Loss: 0.9074892401695251\n",
      "Epoch 5300/10000, Training Loss: 0.6770585179328918, Validation Loss: 0.8613521456718445\n",
      "Epoch 5301/10000, Training Loss: 0.7663869261741638, Validation Loss: 0.8035227656364441\n",
      "Epoch 5302/10000, Training Loss: 0.680030107498169, Validation Loss: 0.5350457429885864\n",
      "Epoch 5303/10000, Training Loss: 0.7594954967498779, Validation Loss: 0.7080966830253601\n",
      "Epoch 5304/10000, Training Loss: 0.7343230843544006, Validation Loss: 0.7150189280509949\n",
      "Epoch 5305/10000, Training Loss: 0.7672199606895447, Validation Loss: 1.4073551893234253\n",
      "Epoch 5306/10000, Training Loss: 0.7430387139320374, Validation Loss: 0.613396942615509\n",
      "Epoch 5307/10000, Training Loss: 0.6991695165634155, Validation Loss: 0.709572970867157\n",
      "Epoch 5308/10000, Training Loss: 0.7161940336227417, Validation Loss: 0.7391050457954407\n",
      "Epoch 5309/10000, Training Loss: 0.7671963572502136, Validation Loss: 0.7168192863464355\n",
      "Epoch 5310/10000, Training Loss: 0.8301515579223633, Validation Loss: 1.1542155742645264\n",
      "Epoch 5311/10000, Training Loss: 0.7271691560745239, Validation Loss: 0.748968780040741\n",
      "Epoch 5312/10000, Training Loss: 0.7425255179405212, Validation Loss: 0.5495845675468445\n",
      "Epoch 5313/10000, Training Loss: 0.7994428873062134, Validation Loss: 0.8000113368034363\n",
      "Epoch 5314/10000, Training Loss: 0.720643937587738, Validation Loss: 0.6661668419837952\n",
      "Epoch 5315/10000, Training Loss: 0.739995539188385, Validation Loss: 0.742133378982544\n",
      "Epoch 5316/10000, Training Loss: 0.6869808435440063, Validation Loss: 0.8709140419960022\n",
      "Epoch 5317/10000, Training Loss: 0.7890296578407288, Validation Loss: 0.6577410697937012\n",
      "Epoch 5318/10000, Training Loss: 0.7255312204360962, Validation Loss: 0.9805131554603577\n",
      "Epoch 5319/10000, Training Loss: 0.7783692479133606, Validation Loss: 0.694340169429779\n",
      "Epoch 5320/10000, Training Loss: 0.7366820573806763, Validation Loss: 0.5848613381385803\n",
      "Epoch 5321/10000, Training Loss: 0.7798416614532471, Validation Loss: 0.8466944098472595\n",
      "Epoch 5322/10000, Training Loss: 0.7706833481788635, Validation Loss: 1.0529694557189941\n",
      "Epoch 5323/10000, Training Loss: 0.7154486775398254, Validation Loss: 0.7884540557861328\n",
      "Epoch 5324/10000, Training Loss: 0.7367738485336304, Validation Loss: 1.0086970329284668\n",
      "Epoch 5325/10000, Training Loss: 0.7518044710159302, Validation Loss: 1.100429654121399\n",
      "Epoch 5326/10000, Training Loss: 0.8004648685455322, Validation Loss: 0.7455329895019531\n",
      "Epoch 5327/10000, Training Loss: 0.7087022066116333, Validation Loss: 0.7465165257453918\n",
      "Epoch 5328/10000, Training Loss: 0.737119734287262, Validation Loss: 0.9481727480888367\n",
      "Epoch 5329/10000, Training Loss: 0.8518088459968567, Validation Loss: 0.8314506411552429\n",
      "Epoch 5330/10000, Training Loss: 0.71980220079422, Validation Loss: 0.5631692409515381\n",
      "Epoch 5331/10000, Training Loss: 0.706479012966156, Validation Loss: 0.6413187384605408\n",
      "Epoch 5332/10000, Training Loss: 0.7473767995834351, Validation Loss: 0.6138599514961243\n",
      "Epoch 5333/10000, Training Loss: 0.735931396484375, Validation Loss: 0.6095830798149109\n",
      "Epoch 5334/10000, Training Loss: 0.7062379121780396, Validation Loss: 0.7084644436836243\n",
      "Epoch 5335/10000, Training Loss: 0.819896936416626, Validation Loss: 0.9939876198768616\n",
      "Epoch 5336/10000, Training Loss: 0.7552565336227417, Validation Loss: 0.7260915637016296\n",
      "Epoch 5337/10000, Training Loss: 0.7878177165985107, Validation Loss: 1.019288420677185\n",
      "Epoch 5338/10000, Training Loss: 0.7896985411643982, Validation Loss: 0.9252737164497375\n",
      "Epoch 5339/10000, Training Loss: 0.749257504940033, Validation Loss: 0.9685996174812317\n",
      "Epoch 5340/10000, Training Loss: 0.704024612903595, Validation Loss: 0.5943994522094727\n",
      "Epoch 5341/10000, Training Loss: 0.753356397151947, Validation Loss: 0.9812970757484436\n",
      "Epoch 5342/10000, Training Loss: 0.7279843091964722, Validation Loss: 0.9528278708457947\n",
      "Epoch 5343/10000, Training Loss: 0.8077230453491211, Validation Loss: 1.1683498620986938\n",
      "Epoch 5344/10000, Training Loss: 0.8430477976799011, Validation Loss: 0.8305005431175232\n",
      "Epoch 5345/10000, Training Loss: 0.7465518116950989, Validation Loss: 0.6249677538871765\n",
      "Epoch 5346/10000, Training Loss: 0.6880261301994324, Validation Loss: 0.8149688243865967\n",
      "Epoch 5347/10000, Training Loss: 0.742721438407898, Validation Loss: 1.2203617095947266\n",
      "Epoch 5348/10000, Training Loss: 0.7489815354347229, Validation Loss: 0.7125167846679688\n",
      "Epoch 5349/10000, Training Loss: 0.7638566493988037, Validation Loss: 0.9516384601593018\n",
      "Epoch 5350/10000, Training Loss: 0.7672357559204102, Validation Loss: 1.2298277616500854\n",
      "Epoch 5351/10000, Training Loss: 0.6726853847503662, Validation Loss: 0.8386263251304626\n",
      "Epoch 5352/10000, Training Loss: 0.764380156993866, Validation Loss: 0.8195860981941223\n",
      "Epoch 5353/10000, Training Loss: 0.8082672357559204, Validation Loss: 0.5761158466339111\n",
      "Epoch 5354/10000, Training Loss: 0.7091550827026367, Validation Loss: 0.850177526473999\n",
      "Epoch 5355/10000, Training Loss: 0.7988201379776001, Validation Loss: 0.8651180863380432\n",
      "Epoch 5356/10000, Training Loss: 0.7322072982788086, Validation Loss: 0.766084611415863\n",
      "Epoch 5357/10000, Training Loss: 0.7380602955818176, Validation Loss: 0.7408096194267273\n",
      "Epoch 5358/10000, Training Loss: 0.6996631622314453, Validation Loss: 0.9668928980827332\n",
      "Epoch 5359/10000, Training Loss: 0.7046772241592407, Validation Loss: 0.939566433429718\n",
      "Epoch 5360/10000, Training Loss: 0.7864647507667542, Validation Loss: 0.8241250514984131\n",
      "Epoch 5361/10000, Training Loss: 0.7307560443878174, Validation Loss: 0.7471932768821716\n",
      "Epoch 5362/10000, Training Loss: 0.7827783823013306, Validation Loss: 1.0842005014419556\n",
      "Epoch 5363/10000, Training Loss: 0.7360280156135559, Validation Loss: 0.9212039113044739\n",
      "Epoch 5364/10000, Training Loss: 0.6896472573280334, Validation Loss: 0.9202497601509094\n",
      "Epoch 5365/10000, Training Loss: 0.6472145915031433, Validation Loss: 0.7750787138938904\n",
      "Epoch 5366/10000, Training Loss: 0.8027964234352112, Validation Loss: 1.1703577041625977\n",
      "Epoch 5367/10000, Training Loss: 0.7297834157943726, Validation Loss: 0.5696166157722473\n",
      "Epoch 5368/10000, Training Loss: 0.7764679193496704, Validation Loss: 1.847529411315918\n",
      "Epoch 5369/10000, Training Loss: 0.748612642288208, Validation Loss: 1.1713824272155762\n",
      "Epoch 5370/10000, Training Loss: 0.74677973985672, Validation Loss: 0.7222194075584412\n",
      "Epoch 5371/10000, Training Loss: 0.6790606379508972, Validation Loss: 0.8097628951072693\n",
      "Epoch 5372/10000, Training Loss: 0.6854175329208374, Validation Loss: 1.060998558998108\n",
      "Epoch 5373/10000, Training Loss: 0.7204020023345947, Validation Loss: 0.6626723408699036\n",
      "Epoch 5374/10000, Training Loss: 0.7424611449241638, Validation Loss: 0.8287737965583801\n",
      "Epoch 5375/10000, Training Loss: 0.6701542735099792, Validation Loss: 0.891015350818634\n",
      "Epoch 5376/10000, Training Loss: 0.733674943447113, Validation Loss: 0.7382755875587463\n",
      "Epoch 5377/10000, Training Loss: 0.8387253880500793, Validation Loss: 1.1675280332565308\n",
      "Epoch 5378/10000, Training Loss: 0.7742215394973755, Validation Loss: 0.6873874664306641\n",
      "Epoch 5379/10000, Training Loss: 0.9030201435089111, Validation Loss: 1.1680277585983276\n",
      "Epoch 5380/10000, Training Loss: 0.7020155787467957, Validation Loss: 1.0595369338989258\n",
      "Epoch 5381/10000, Training Loss: 0.7718918919563293, Validation Loss: 1.0277854204177856\n",
      "Epoch 5382/10000, Training Loss: 0.7582709193229675, Validation Loss: 0.7384505271911621\n",
      "Epoch 5383/10000, Training Loss: 0.6996476054191589, Validation Loss: 0.6483656764030457\n",
      "Epoch 5384/10000, Training Loss: 0.7123115062713623, Validation Loss: 0.6893656849861145\n",
      "Epoch 5385/10000, Training Loss: 0.671173095703125, Validation Loss: 1.1021016836166382\n",
      "Epoch 5386/10000, Training Loss: 0.7123493552207947, Validation Loss: 1.0827676057815552\n",
      "Epoch 5387/10000, Training Loss: 0.6894336342811584, Validation Loss: 0.6527376174926758\n",
      "Epoch 5388/10000, Training Loss: 0.6920413374900818, Validation Loss: 0.8189327120780945\n",
      "Epoch 5389/10000, Training Loss: 0.6960243582725525, Validation Loss: 0.721998393535614\n",
      "Epoch 5390/10000, Training Loss: 0.7441152930259705, Validation Loss: 0.7829220294952393\n",
      "Epoch 5391/10000, Training Loss: 0.7497693300247192, Validation Loss: 0.6720244884490967\n",
      "Epoch 5392/10000, Training Loss: 0.7215225696563721, Validation Loss: 1.1719437837600708\n",
      "Epoch 5393/10000, Training Loss: 0.9129245281219482, Validation Loss: 0.6996539235115051\n",
      "Epoch 5394/10000, Training Loss: 0.761617124080658, Validation Loss: 1.078161597251892\n",
      "Epoch 5395/10000, Training Loss: 0.7492717504501343, Validation Loss: 0.6961244940757751\n",
      "Epoch 5396/10000, Training Loss: 0.7097927927970886, Validation Loss: 0.6266202926635742\n",
      "Epoch 5397/10000, Training Loss: 0.7065422534942627, Validation Loss: 0.7945156097412109\n",
      "Epoch 5398/10000, Training Loss: 0.7161452174186707, Validation Loss: 0.871221125125885\n",
      "Epoch 5399/10000, Training Loss: 0.6702080965042114, Validation Loss: 1.1749054193496704\n",
      "Epoch 5400/10000, Training Loss: 0.6904510259628296, Validation Loss: 0.6513529419898987\n",
      "Epoch 5401/10000, Training Loss: 0.8786674737930298, Validation Loss: 1.561470627784729\n",
      "Epoch 5402/10000, Training Loss: 0.5963953733444214, Validation Loss: 0.8487856388092041\n",
      "Epoch 5403/10000, Training Loss: 0.702109694480896, Validation Loss: 1.1031454801559448\n",
      "Epoch 5404/10000, Training Loss: 0.7538861036300659, Validation Loss: 0.8640648722648621\n",
      "Epoch 5405/10000, Training Loss: 0.7106732726097107, Validation Loss: 0.6796805262565613\n",
      "Epoch 5406/10000, Training Loss: 0.7338069081306458, Validation Loss: 0.8750143647193909\n",
      "Epoch 5407/10000, Training Loss: 0.6911043524742126, Validation Loss: 0.6259394288063049\n",
      "Epoch 5408/10000, Training Loss: 0.7182105183601379, Validation Loss: 0.8996482491493225\n",
      "Epoch 5409/10000, Training Loss: 0.7508159279823303, Validation Loss: 0.9229283928871155\n",
      "Epoch 5410/10000, Training Loss: 0.7226706743240356, Validation Loss: 0.8603294491767883\n",
      "Epoch 5411/10000, Training Loss: 0.6595973968505859, Validation Loss: 0.7544434070587158\n",
      "Epoch 5412/10000, Training Loss: 0.7973859310150146, Validation Loss: 0.4508708417415619\n",
      "Epoch 5413/10000, Training Loss: 0.7705188989639282, Validation Loss: 0.7366343140602112\n",
      "Epoch 5414/10000, Training Loss: 0.7425413727760315, Validation Loss: 0.7046873569488525\n",
      "Epoch 5415/10000, Training Loss: 0.7801125645637512, Validation Loss: 1.0518651008605957\n",
      "Epoch 5416/10000, Training Loss: 0.7628446817398071, Validation Loss: 0.7136558890342712\n",
      "Epoch 5417/10000, Training Loss: 0.7447654008865356, Validation Loss: 0.835314929485321\n",
      "Epoch 5418/10000, Training Loss: 0.7007880806922913, Validation Loss: 0.882422685623169\n",
      "Epoch 5419/10000, Training Loss: 0.7158864736557007, Validation Loss: 0.7925803065299988\n",
      "Epoch 5420/10000, Training Loss: 0.65802001953125, Validation Loss: 0.715071439743042\n",
      "Epoch 5421/10000, Training Loss: 0.7188607454299927, Validation Loss: 0.6870844960212708\n",
      "Epoch 5422/10000, Training Loss: 0.6600661873817444, Validation Loss: 0.6917491555213928\n",
      "Epoch 5423/10000, Training Loss: 0.7026380300521851, Validation Loss: 1.0583909749984741\n",
      "Epoch 5424/10000, Training Loss: 0.7321087718009949, Validation Loss: 1.1545296907424927\n",
      "Epoch 5425/10000, Training Loss: 0.7031622529029846, Validation Loss: 0.9422547817230225\n",
      "Epoch 5426/10000, Training Loss: 0.7143201231956482, Validation Loss: 0.785713255405426\n",
      "Epoch 5427/10000, Training Loss: 0.6957696676254272, Validation Loss: 0.6284626126289368\n",
      "Epoch 5428/10000, Training Loss: 0.7201477885246277, Validation Loss: 1.1387131214141846\n",
      "Epoch 5429/10000, Training Loss: 0.7379719018936157, Validation Loss: 0.8688996434211731\n",
      "Epoch 5430/10000, Training Loss: 0.7065485715866089, Validation Loss: 0.711571455001831\n",
      "Epoch 5431/10000, Training Loss: 0.765121579170227, Validation Loss: 1.0465986728668213\n",
      "Epoch 5432/10000, Training Loss: 0.7310567498207092, Validation Loss: 0.8953037261962891\n",
      "Epoch 5433/10000, Training Loss: 0.7800308465957642, Validation Loss: 0.9125831127166748\n",
      "Epoch 5434/10000, Training Loss: 0.6676111221313477, Validation Loss: 0.6988006234169006\n",
      "Epoch 5435/10000, Training Loss: 0.8671330809593201, Validation Loss: 1.3739924430847168\n",
      "Epoch 5436/10000, Training Loss: 0.7376798987388611, Validation Loss: 0.6580707430839539\n",
      "Epoch 5437/10000, Training Loss: 0.7521219253540039, Validation Loss: 0.9123393893241882\n",
      "Epoch 5438/10000, Training Loss: 0.7551937103271484, Validation Loss: 0.7821249961853027\n",
      "Epoch 5439/10000, Training Loss: 0.7149859070777893, Validation Loss: 0.7436204552650452\n",
      "Epoch 5440/10000, Training Loss: 0.7571690678596497, Validation Loss: 0.8148295283317566\n",
      "Epoch 5441/10000, Training Loss: 0.6625868678092957, Validation Loss: 0.6852056980133057\n",
      "Epoch 5442/10000, Training Loss: 0.8538796305656433, Validation Loss: 0.7215204834938049\n",
      "Epoch 5443/10000, Training Loss: 0.7528238892555237, Validation Loss: 0.8383750915527344\n",
      "Epoch 5444/10000, Training Loss: 0.8063216805458069, Validation Loss: 0.8786508440971375\n",
      "Epoch 5445/10000, Training Loss: 0.7479610443115234, Validation Loss: 0.6401039958000183\n",
      "Epoch 5446/10000, Training Loss: 0.6824964284896851, Validation Loss: 0.8307015299797058\n",
      "Epoch 5447/10000, Training Loss: 0.7275121212005615, Validation Loss: 0.7883455753326416\n",
      "Epoch 5448/10000, Training Loss: 0.6941937208175659, Validation Loss: 1.0697236061096191\n",
      "Epoch 5449/10000, Training Loss: 0.7108027338981628, Validation Loss: 0.8962254524230957\n",
      "Epoch 5450/10000, Training Loss: 0.6772676110267639, Validation Loss: 1.3085061311721802\n",
      "Epoch 5451/10000, Training Loss: 0.8086562752723694, Validation Loss: 0.8060755729675293\n",
      "Epoch 5452/10000, Training Loss: 0.7185021042823792, Validation Loss: 0.7314304709434509\n",
      "Epoch 5453/10000, Training Loss: 0.7480976581573486, Validation Loss: 0.9929625391960144\n",
      "Epoch 5454/10000, Training Loss: 0.7417498826980591, Validation Loss: 0.7922704219818115\n",
      "Epoch 5455/10000, Training Loss: 0.6964982151985168, Validation Loss: 0.6989225745201111\n",
      "Epoch 5456/10000, Training Loss: 0.8293449878692627, Validation Loss: 0.6568827033042908\n",
      "Epoch 5457/10000, Training Loss: 0.7227084040641785, Validation Loss: 1.016239047050476\n",
      "Epoch 5458/10000, Training Loss: 0.7308817505836487, Validation Loss: 0.6781989932060242\n",
      "Epoch 5459/10000, Training Loss: 0.7173118591308594, Validation Loss: 0.7413614392280579\n",
      "Epoch 5460/10000, Training Loss: 0.7107723951339722, Validation Loss: 1.1379873752593994\n",
      "Epoch 5461/10000, Training Loss: 0.7823498249053955, Validation Loss: 0.6916834712028503\n",
      "Epoch 5462/10000, Training Loss: 0.6963726878166199, Validation Loss: 0.8244309425354004\n",
      "Epoch 5463/10000, Training Loss: 0.7212351560592651, Validation Loss: 0.7200892567634583\n",
      "Epoch 5464/10000, Training Loss: 0.7098129987716675, Validation Loss: 0.8829073905944824\n",
      "Epoch 5465/10000, Training Loss: 0.6827173829078674, Validation Loss: 0.5747368931770325\n",
      "Epoch 5466/10000, Training Loss: 0.67584228515625, Validation Loss: 1.1137434244155884\n",
      "Epoch 5467/10000, Training Loss: 0.7461257576942444, Validation Loss: 1.1621932983398438\n",
      "Epoch 5468/10000, Training Loss: 0.7369105219841003, Validation Loss: 0.6925251483917236\n",
      "Epoch 5469/10000, Training Loss: 0.7099330425262451, Validation Loss: 0.9344844818115234\n",
      "Epoch 5470/10000, Training Loss: 0.6943738460540771, Validation Loss: 0.735035240650177\n",
      "Epoch 5471/10000, Training Loss: 0.6744652986526489, Validation Loss: 0.926367461681366\n",
      "Epoch 5472/10000, Training Loss: 0.6961218118667603, Validation Loss: 0.8251814842224121\n",
      "Epoch 5473/10000, Training Loss: 0.7448586225509644, Validation Loss: 0.868708074092865\n",
      "Epoch 5474/10000, Training Loss: 0.6959860324859619, Validation Loss: 1.012494444847107\n",
      "Epoch 5475/10000, Training Loss: 0.7120048999786377, Validation Loss: 0.9298120141029358\n",
      "Epoch 5476/10000, Training Loss: 0.8093254566192627, Validation Loss: 0.6777588725090027\n",
      "Epoch 5477/10000, Training Loss: 0.7393880486488342, Validation Loss: 0.6036213636398315\n",
      "Epoch 5478/10000, Training Loss: 0.7646318674087524, Validation Loss: 0.7696247696876526\n",
      "Epoch 5479/10000, Training Loss: 0.6831549406051636, Validation Loss: 0.7306825518608093\n",
      "Epoch 5480/10000, Training Loss: 0.6611391305923462, Validation Loss: 0.7830761075019836\n",
      "Epoch 5481/10000, Training Loss: 0.7101145386695862, Validation Loss: 0.8436322808265686\n",
      "Epoch 5482/10000, Training Loss: 0.7288776636123657, Validation Loss: 0.5968953371047974\n",
      "Epoch 5483/10000, Training Loss: 0.7632421851158142, Validation Loss: 1.0226300954818726\n",
      "Epoch 5484/10000, Training Loss: 0.7748787999153137, Validation Loss: 1.034166693687439\n",
      "Epoch 5485/10000, Training Loss: 0.7158588767051697, Validation Loss: 1.0766808986663818\n",
      "Epoch 5486/10000, Training Loss: 0.795399010181427, Validation Loss: 0.8836703896522522\n",
      "Epoch 5487/10000, Training Loss: 0.7505220770835876, Validation Loss: 0.9553741812705994\n",
      "Epoch 5488/10000, Training Loss: 0.7997039556503296, Validation Loss: 1.1663804054260254\n",
      "Epoch 5489/10000, Training Loss: 0.721955418586731, Validation Loss: 0.7855048775672913\n",
      "Epoch 5490/10000, Training Loss: 0.8306752443313599, Validation Loss: 0.6507578492164612\n",
      "Epoch 5491/10000, Training Loss: 0.7330204248428345, Validation Loss: 0.9812533855438232\n",
      "Epoch 5492/10000, Training Loss: 0.7174393534660339, Validation Loss: 0.7354767322540283\n",
      "Epoch 5493/10000, Training Loss: 0.7090275287628174, Validation Loss: 0.9987030029296875\n",
      "Epoch 5494/10000, Training Loss: 0.7012293338775635, Validation Loss: 0.8373705744743347\n",
      "Epoch 5495/10000, Training Loss: 0.756244421005249, Validation Loss: 0.9015393257141113\n",
      "Epoch 5496/10000, Training Loss: 0.6921919584274292, Validation Loss: 0.9466424584388733\n",
      "Epoch 5497/10000, Training Loss: 0.6358983516693115, Validation Loss: 0.8873102068901062\n",
      "Epoch 5498/10000, Training Loss: 0.8093960881233215, Validation Loss: 1.10504949092865\n",
      "Epoch 5499/10000, Training Loss: 0.709818422794342, Validation Loss: 0.6107072234153748\n",
      "Epoch 5500/10000, Training Loss: 0.6487842202186584, Validation Loss: 0.6810052394866943\n",
      "Epoch 5501/10000, Training Loss: 0.7265313863754272, Validation Loss: 0.728348433971405\n",
      "Epoch 5502/10000, Training Loss: 0.8085821866989136, Validation Loss: 1.0411356687545776\n",
      "Epoch 5503/10000, Training Loss: 0.6913578510284424, Validation Loss: 0.893953800201416\n",
      "Epoch 5504/10000, Training Loss: 0.64533931016922, Validation Loss: 1.0287576913833618\n",
      "Epoch 5505/10000, Training Loss: 0.6938183903694153, Validation Loss: 0.8980560302734375\n",
      "Epoch 5506/10000, Training Loss: 0.69759601354599, Validation Loss: 0.5479177832603455\n",
      "Epoch 5507/10000, Training Loss: 0.7465488910675049, Validation Loss: 0.6828966736793518\n",
      "Epoch 5508/10000, Training Loss: 0.7500523328781128, Validation Loss: 1.1698778867721558\n",
      "Epoch 5509/10000, Training Loss: 0.720320999622345, Validation Loss: 0.9560462832450867\n",
      "Epoch 5510/10000, Training Loss: 0.6481730341911316, Validation Loss: 0.7052239775657654\n",
      "Epoch 5511/10000, Training Loss: 0.7602413892745972, Validation Loss: 0.6129302978515625\n",
      "Epoch 5512/10000, Training Loss: 0.7622981667518616, Validation Loss: 0.7801198363304138\n",
      "Epoch 5513/10000, Training Loss: 0.712875247001648, Validation Loss: 0.7079074382781982\n",
      "Epoch 5514/10000, Training Loss: 0.7287495136260986, Validation Loss: 0.7067143321037292\n",
      "Epoch 5515/10000, Training Loss: 0.7441406846046448, Validation Loss: 0.4489917457103729\n",
      "Epoch 5516/10000, Training Loss: 0.7003557682037354, Validation Loss: 0.6887452006340027\n",
      "Epoch 5517/10000, Training Loss: 0.8054850697517395, Validation Loss: 0.7183629870414734\n",
      "Epoch 5518/10000, Training Loss: 0.7361662983894348, Validation Loss: 0.8659282326698303\n",
      "Epoch 5519/10000, Training Loss: 0.7847992777824402, Validation Loss: 0.9067042469978333\n",
      "Epoch 5520/10000, Training Loss: 0.6919825673103333, Validation Loss: 0.7476540207862854\n",
      "Epoch 5521/10000, Training Loss: 0.7826539278030396, Validation Loss: 0.6903162002563477\n",
      "Epoch 5522/10000, Training Loss: 0.6991172432899475, Validation Loss: 0.6530079245567322\n",
      "Epoch 5523/10000, Training Loss: 0.7394670248031616, Validation Loss: 1.1277693510055542\n",
      "Epoch 5524/10000, Training Loss: 0.6865717172622681, Validation Loss: 0.8429980278015137\n",
      "Epoch 5525/10000, Training Loss: 0.6803516745567322, Validation Loss: 0.5717678666114807\n",
      "Epoch 5526/10000, Training Loss: 0.7215295433998108, Validation Loss: 0.6963334083557129\n",
      "Epoch 5527/10000, Training Loss: 0.68614262342453, Validation Loss: 0.7076159119606018\n",
      "Epoch 5528/10000, Training Loss: 0.7604246735572815, Validation Loss: 0.7215194702148438\n",
      "Epoch 5529/10000, Training Loss: 0.6894633769989014, Validation Loss: 0.7794886231422424\n",
      "Epoch 5530/10000, Training Loss: 0.7082633972167969, Validation Loss: 0.6980094909667969\n",
      "Epoch 5531/10000, Training Loss: 0.7448078393936157, Validation Loss: 0.9784004092216492\n",
      "Epoch 5532/10000, Training Loss: 0.7327877879142761, Validation Loss: 0.8643627762794495\n",
      "Epoch 5533/10000, Training Loss: 0.6988160610198975, Validation Loss: 0.6787781119346619\n",
      "Epoch 5534/10000, Training Loss: 0.7593346238136292, Validation Loss: 0.8304588794708252\n",
      "Epoch 5535/10000, Training Loss: 0.654704749584198, Validation Loss: 1.158246397972107\n",
      "Epoch 5536/10000, Training Loss: 0.6995058655738831, Validation Loss: 0.7132002711296082\n",
      "Epoch 5537/10000, Training Loss: 0.6653798222541809, Validation Loss: 0.8517990708351135\n",
      "Epoch 5538/10000, Training Loss: 0.7351658940315247, Validation Loss: 0.9163329005241394\n",
      "Epoch 5539/10000, Training Loss: 0.7223252058029175, Validation Loss: 1.0585728883743286\n",
      "Epoch 5540/10000, Training Loss: 0.7091954350471497, Validation Loss: 0.9394826889038086\n",
      "Epoch 5541/10000, Training Loss: 0.7012450695037842, Validation Loss: 0.7614660859107971\n",
      "Epoch 5542/10000, Training Loss: 0.7300525903701782, Validation Loss: 0.7309272289276123\n",
      "Epoch 5543/10000, Training Loss: 0.7193917036056519, Validation Loss: 0.762885570526123\n",
      "Epoch 5544/10000, Training Loss: 0.7072113752365112, Validation Loss: 0.6986845135688782\n",
      "Epoch 5545/10000, Training Loss: 0.7163666486740112, Validation Loss: 0.8048286437988281\n",
      "Epoch 5546/10000, Training Loss: 0.6797768473625183, Validation Loss: 0.6642952561378479\n",
      "Epoch 5547/10000, Training Loss: 0.7195979356765747, Validation Loss: 0.9438511729240417\n",
      "Epoch 5548/10000, Training Loss: 0.7511695027351379, Validation Loss: 0.8080752491950989\n",
      "Epoch 5549/10000, Training Loss: 0.6920446157455444, Validation Loss: 1.064670443534851\n",
      "Epoch 5550/10000, Training Loss: 0.7102123498916626, Validation Loss: 0.659058153629303\n",
      "Epoch 5551/10000, Training Loss: 0.7306802272796631, Validation Loss: 0.7575514316558838\n",
      "Epoch 5552/10000, Training Loss: 0.7016101479530334, Validation Loss: 0.9616270661354065\n",
      "Epoch 5553/10000, Training Loss: 0.7660679221153259, Validation Loss: 0.7852640748023987\n",
      "Epoch 5554/10000, Training Loss: 0.6994644999504089, Validation Loss: 0.5515682101249695\n",
      "Epoch 5555/10000, Training Loss: 0.7001043558120728, Validation Loss: 0.4094262421131134\n",
      "Epoch 5556/10000, Training Loss: 0.7678438425064087, Validation Loss: 1.0452080965042114\n",
      "Epoch 5557/10000, Training Loss: 0.8420262932777405, Validation Loss: 1.0848820209503174\n",
      "Epoch 5558/10000, Training Loss: 0.6708007454872131, Validation Loss: 0.962797224521637\n",
      "Epoch 5559/10000, Training Loss: 0.6592333912849426, Validation Loss: 0.8570824265480042\n",
      "Epoch 5560/10000, Training Loss: 0.6363325715065002, Validation Loss: 1.295384168624878\n",
      "Epoch 5561/10000, Training Loss: 0.8019639253616333, Validation Loss: 1.1005339622497559\n",
      "Epoch 5562/10000, Training Loss: 0.7134277820587158, Validation Loss: 1.0369154214859009\n",
      "Epoch 5563/10000, Training Loss: 0.7904585599899292, Validation Loss: 1.01844322681427\n",
      "Epoch 5564/10000, Training Loss: 0.7361417412757874, Validation Loss: 0.9866154789924622\n",
      "Epoch 5565/10000, Training Loss: 0.7697193622589111, Validation Loss: 0.6864333748817444\n",
      "Epoch 5566/10000, Training Loss: 0.6918786764144897, Validation Loss: 0.9248718619346619\n",
      "Epoch 5567/10000, Training Loss: 0.750498354434967, Validation Loss: 0.8496190905570984\n",
      "Epoch 5568/10000, Training Loss: 0.7299837470054626, Validation Loss: 1.0687839984893799\n",
      "Epoch 5569/10000, Training Loss: 0.6647524237632751, Validation Loss: 0.5969860553741455\n",
      "Epoch 5570/10000, Training Loss: 0.6846469044685364, Validation Loss: 0.6671357750892639\n",
      "Epoch 5571/10000, Training Loss: 0.7066182494163513, Validation Loss: 1.0075141191482544\n",
      "Epoch 5572/10000, Training Loss: 0.7035292983055115, Validation Loss: 1.0332610607147217\n",
      "Epoch 5573/10000, Training Loss: 0.7177302837371826, Validation Loss: 0.5679786205291748\n",
      "Epoch 5574/10000, Training Loss: 0.743789553642273, Validation Loss: 0.800539493560791\n",
      "Epoch 5575/10000, Training Loss: 0.6996088624000549, Validation Loss: 0.6405785083770752\n",
      "Epoch 5576/10000, Training Loss: 0.7139556407928467, Validation Loss: 0.8753666877746582\n",
      "Epoch 5577/10000, Training Loss: 0.7282989025115967, Validation Loss: 0.6574376225471497\n",
      "Epoch 5578/10000, Training Loss: 0.7092971801757812, Validation Loss: 0.7563764452934265\n",
      "Epoch 5579/10000, Training Loss: 0.866411566734314, Validation Loss: 0.7786446213722229\n",
      "Epoch 5580/10000, Training Loss: 0.6973387002944946, Validation Loss: 0.680243194103241\n",
      "Epoch 5581/10000, Training Loss: 0.7853769063949585, Validation Loss: 0.7411968111991882\n",
      "Epoch 5582/10000, Training Loss: 0.7416664958000183, Validation Loss: 0.5189762115478516\n",
      "Epoch 5583/10000, Training Loss: 0.6923103928565979, Validation Loss: 0.8173127770423889\n",
      "Epoch 5584/10000, Training Loss: 0.7470059990882874, Validation Loss: 0.6209378838539124\n",
      "Epoch 5585/10000, Training Loss: 0.6465410590171814, Validation Loss: 0.6526978611946106\n",
      "Epoch 5586/10000, Training Loss: 0.7589810490608215, Validation Loss: 0.6165879368782043\n",
      "Epoch 5587/10000, Training Loss: 0.6688303351402283, Validation Loss: 0.722485363483429\n",
      "Epoch 5588/10000, Training Loss: 0.6649318337440491, Validation Loss: 1.0383483171463013\n",
      "Epoch 5589/10000, Training Loss: 0.6894768476486206, Validation Loss: 1.1233878135681152\n",
      "Epoch 5590/10000, Training Loss: 0.7167910933494568, Validation Loss: 0.6648784875869751\n",
      "Epoch 5591/10000, Training Loss: 0.6999017000198364, Validation Loss: 0.6037285923957825\n",
      "Epoch 5592/10000, Training Loss: 0.7205418348312378, Validation Loss: 0.5104625821113586\n",
      "Epoch 5593/10000, Training Loss: 0.6682500243186951, Validation Loss: 0.6988096237182617\n",
      "Epoch 5594/10000, Training Loss: 0.7229407429695129, Validation Loss: 0.6782352924346924\n",
      "Epoch 5595/10000, Training Loss: 0.7236698865890503, Validation Loss: 0.6627559065818787\n",
      "Epoch 5596/10000, Training Loss: 0.7562664747238159, Validation Loss: 0.5081279873847961\n",
      "Epoch 5597/10000, Training Loss: 0.7451043725013733, Validation Loss: 1.0478556156158447\n",
      "Epoch 5598/10000, Training Loss: 0.7476006150245667, Validation Loss: 0.6865906715393066\n",
      "Epoch 5599/10000, Training Loss: 0.66243976354599, Validation Loss: 1.0178042650222778\n",
      "Epoch 5600/10000, Training Loss: 0.7181026935577393, Validation Loss: 0.849114179611206\n",
      "Epoch 5601/10000, Training Loss: 0.6828688383102417, Validation Loss: 0.8946564197540283\n",
      "Epoch 5602/10000, Training Loss: 0.7813497185707092, Validation Loss: 0.790897786617279\n",
      "Epoch 5603/10000, Training Loss: 0.7512231469154358, Validation Loss: 0.7606823444366455\n",
      "Epoch 5604/10000, Training Loss: 0.6977322697639465, Validation Loss: 0.9494335651397705\n",
      "Epoch 5605/10000, Training Loss: 0.7733615040779114, Validation Loss: 0.7829411625862122\n",
      "Epoch 5606/10000, Training Loss: 0.6990517377853394, Validation Loss: 0.8350558876991272\n",
      "Epoch 5607/10000, Training Loss: 0.6871764063835144, Validation Loss: 0.7103424668312073\n",
      "Epoch 5608/10000, Training Loss: 0.7552763223648071, Validation Loss: 0.9499689936637878\n",
      "Epoch 5609/10000, Training Loss: 0.6885312795639038, Validation Loss: 0.8652090430259705\n",
      "Epoch 5610/10000, Training Loss: 0.6921297907829285, Validation Loss: 0.8315193057060242\n",
      "Epoch 5611/10000, Training Loss: 0.6855453252792358, Validation Loss: 0.7682533264160156\n",
      "Epoch 5612/10000, Training Loss: 0.6962969899177551, Validation Loss: 0.829643726348877\n",
      "Epoch 5613/10000, Training Loss: 0.6246412992477417, Validation Loss: 0.9286375641822815\n",
      "Epoch 5614/10000, Training Loss: 0.6615968942642212, Validation Loss: 0.7621565461158752\n",
      "Epoch 5615/10000, Training Loss: 0.6597583293914795, Validation Loss: 0.8555484414100647\n",
      "Epoch 5616/10000, Training Loss: 0.749182939529419, Validation Loss: 1.0804517269134521\n",
      "Epoch 5617/10000, Training Loss: 0.7439619302749634, Validation Loss: 0.5948002934455872\n",
      "Epoch 5618/10000, Training Loss: 0.6781181693077087, Validation Loss: 0.6831629276275635\n",
      "Epoch 5619/10000, Training Loss: 0.7537088990211487, Validation Loss: 0.7528760433197021\n",
      "Epoch 5620/10000, Training Loss: 0.7442948222160339, Validation Loss: 0.9003543853759766\n",
      "Epoch 5621/10000, Training Loss: 0.7250795960426331, Validation Loss: 0.5878642201423645\n",
      "Epoch 5622/10000, Training Loss: 0.6783180236816406, Validation Loss: 0.7407353520393372\n",
      "Epoch 5623/10000, Training Loss: 0.7801904082298279, Validation Loss: 0.7278392314910889\n",
      "Epoch 5624/10000, Training Loss: 0.7121191620826721, Validation Loss: 0.8946471214294434\n",
      "Epoch 5625/10000, Training Loss: 0.7420845031738281, Validation Loss: 0.6125929951667786\n",
      "Epoch 5626/10000, Training Loss: 0.7272163033485413, Validation Loss: 0.866583526134491\n",
      "Epoch 5627/10000, Training Loss: 0.68050217628479, Validation Loss: 0.7819902300834656\n",
      "Epoch 5628/10000, Training Loss: 0.6938121318817139, Validation Loss: 0.6323146820068359\n",
      "Epoch 5629/10000, Training Loss: 0.7322911024093628, Validation Loss: 0.5989799499511719\n",
      "Epoch 5630/10000, Training Loss: 0.7658558487892151, Validation Loss: 0.9111312031745911\n",
      "Epoch 5631/10000, Training Loss: 0.7565809488296509, Validation Loss: 0.6677472591400146\n",
      "Epoch 5632/10000, Training Loss: 0.6993939280509949, Validation Loss: 0.6936542391777039\n",
      "Epoch 5633/10000, Training Loss: 0.7136875987052917, Validation Loss: 1.1803948879241943\n",
      "Epoch 5634/10000, Training Loss: 0.7562220096588135, Validation Loss: 0.8962104320526123\n",
      "Epoch 5635/10000, Training Loss: 0.7450185418128967, Validation Loss: 0.8426886200904846\n",
      "Epoch 5636/10000, Training Loss: 0.7449996471405029, Validation Loss: 0.8087278008460999\n",
      "Epoch 5637/10000, Training Loss: 0.6473416686058044, Validation Loss: 0.8451743721961975\n",
      "Epoch 5638/10000, Training Loss: 0.7008777260780334, Validation Loss: 0.7119914889335632\n",
      "Epoch 5639/10000, Training Loss: 0.6892684698104858, Validation Loss: 0.9630767703056335\n",
      "Epoch 5640/10000, Training Loss: 0.6930065751075745, Validation Loss: 0.8505498766899109\n",
      "Epoch 5641/10000, Training Loss: 0.6851305365562439, Validation Loss: 0.6418467164039612\n",
      "Epoch 5642/10000, Training Loss: 0.7340479493141174, Validation Loss: 0.8341848254203796\n",
      "Epoch 5643/10000, Training Loss: 0.7267909646034241, Validation Loss: 0.5569484829902649\n",
      "Epoch 5644/10000, Training Loss: 0.6475571990013123, Validation Loss: 0.8856611251831055\n",
      "Epoch 5645/10000, Training Loss: 0.7135107517242432, Validation Loss: 0.8531439900398254\n",
      "Epoch 5646/10000, Training Loss: 0.686025083065033, Validation Loss: 0.8180716037750244\n",
      "Epoch 5647/10000, Training Loss: 0.7078860402107239, Validation Loss: 1.3186475038528442\n",
      "Epoch 5648/10000, Training Loss: 0.7023547887802124, Validation Loss: 0.7683749794960022\n",
      "Epoch 5649/10000, Training Loss: 0.7158483862876892, Validation Loss: 0.9828646183013916\n",
      "Epoch 5650/10000, Training Loss: 0.727112889289856, Validation Loss: 0.8622608184814453\n",
      "Epoch 5651/10000, Training Loss: 0.7459297180175781, Validation Loss: 0.7022678852081299\n",
      "Epoch 5652/10000, Training Loss: 0.6831997632980347, Validation Loss: 0.8467724919319153\n",
      "Epoch 5653/10000, Training Loss: 0.6984366178512573, Validation Loss: 1.4249178171157837\n",
      "Epoch 5654/10000, Training Loss: 0.6888280510902405, Validation Loss: 0.714952290058136\n",
      "Epoch 5655/10000, Training Loss: 0.7228792905807495, Validation Loss: 0.7460951805114746\n",
      "Epoch 5656/10000, Training Loss: 0.7482569813728333, Validation Loss: 0.8759869933128357\n",
      "Epoch 5657/10000, Training Loss: 0.7531424164772034, Validation Loss: 0.8620564937591553\n",
      "Epoch 5658/10000, Training Loss: 0.7123813629150391, Validation Loss: 0.7990145087242126\n",
      "Epoch 5659/10000, Training Loss: 0.6959267854690552, Validation Loss: 0.6785023212432861\n",
      "Epoch 5660/10000, Training Loss: 0.7262402772903442, Validation Loss: 0.6760354042053223\n",
      "Epoch 5661/10000, Training Loss: 0.7362810969352722, Validation Loss: 0.796917736530304\n",
      "Epoch 5662/10000, Training Loss: 0.7663339972496033, Validation Loss: 0.9810299873352051\n",
      "Epoch 5663/10000, Training Loss: 0.7241398692131042, Validation Loss: 0.9076380729675293\n",
      "Epoch 5664/10000, Training Loss: 0.6856135725975037, Validation Loss: 0.7925812602043152\n",
      "Epoch 5665/10000, Training Loss: 0.6720702052116394, Validation Loss: 0.992725670337677\n",
      "Epoch 5666/10000, Training Loss: 0.7190748453140259, Validation Loss: 1.1766191720962524\n",
      "Epoch 5667/10000, Training Loss: 0.65260910987854, Validation Loss: 0.6573388576507568\n",
      "Epoch 5668/10000, Training Loss: 0.6758021116256714, Validation Loss: 0.8677619099617004\n",
      "Epoch 5669/10000, Training Loss: 0.679901123046875, Validation Loss: 0.6679105758666992\n",
      "Epoch 5670/10000, Training Loss: 0.7998151183128357, Validation Loss: 0.8796033263206482\n",
      "Epoch 5671/10000, Training Loss: 0.6875352263450623, Validation Loss: 0.645679771900177\n",
      "Epoch 5672/10000, Training Loss: 0.7036406993865967, Validation Loss: 0.6758763194084167\n",
      "Epoch 5673/10000, Training Loss: 0.7168086171150208, Validation Loss: 0.9685302376747131\n",
      "Epoch 5674/10000, Training Loss: 0.7277253866195679, Validation Loss: 0.7041854858398438\n",
      "Epoch 5675/10000, Training Loss: 0.773371160030365, Validation Loss: 0.7468839287757874\n",
      "Epoch 5676/10000, Training Loss: 0.7009257674217224, Validation Loss: 0.5730308294296265\n",
      "Epoch 5677/10000, Training Loss: 0.6909926533699036, Validation Loss: 0.7727136611938477\n",
      "Epoch 5678/10000, Training Loss: 0.6878623366355896, Validation Loss: 0.6334486603736877\n",
      "Epoch 5679/10000, Training Loss: 0.7026255130767822, Validation Loss: 0.7643399834632874\n",
      "Epoch 5680/10000, Training Loss: 0.7232657074928284, Validation Loss: 0.5388344526290894\n",
      "Epoch 5681/10000, Training Loss: 0.6523213982582092, Validation Loss: 1.0211784839630127\n",
      "Epoch 5682/10000, Training Loss: 0.7244113087654114, Validation Loss: 0.514239490032196\n",
      "Epoch 5683/10000, Training Loss: 0.7056761384010315, Validation Loss: 0.7422453761100769\n",
      "Epoch 5684/10000, Training Loss: 0.6316061019897461, Validation Loss: 0.8191989064216614\n",
      "Epoch 5685/10000, Training Loss: 0.7196595072746277, Validation Loss: 1.073649287223816\n",
      "Epoch 5686/10000, Training Loss: 0.743046224117279, Validation Loss: 0.6172873377799988\n",
      "Epoch 5687/10000, Training Loss: 0.6854712963104248, Validation Loss: 0.7193427085876465\n",
      "Epoch 5688/10000, Training Loss: 0.6547046899795532, Validation Loss: 0.7231503129005432\n",
      "Epoch 5689/10000, Training Loss: 0.7081724405288696, Validation Loss: 0.7270681858062744\n",
      "Epoch 5690/10000, Training Loss: 0.6751439571380615, Validation Loss: 0.6375334858894348\n",
      "Epoch 5691/10000, Training Loss: 0.677740216255188, Validation Loss: 1.0939944982528687\n",
      "Epoch 5692/10000, Training Loss: 0.6815829873085022, Validation Loss: 0.8841677308082581\n",
      "Epoch 5693/10000, Training Loss: 0.7120381593704224, Validation Loss: 0.6807599663734436\n",
      "Epoch 5694/10000, Training Loss: 0.6857742071151733, Validation Loss: 0.7728598117828369\n",
      "Epoch 5695/10000, Training Loss: 0.6918509006500244, Validation Loss: 1.0603563785552979\n",
      "Epoch 5696/10000, Training Loss: 0.7263031005859375, Validation Loss: 0.9065635800361633\n",
      "Epoch 5697/10000, Training Loss: 0.6901940107345581, Validation Loss: 0.8340403437614441\n",
      "Epoch 5698/10000, Training Loss: 0.6882004141807556, Validation Loss: 0.8825939297676086\n",
      "Epoch 5699/10000, Training Loss: 0.735296905040741, Validation Loss: 0.6383543610572815\n",
      "Epoch 5700/10000, Training Loss: 0.8193050026893616, Validation Loss: 0.9747300744056702\n",
      "Epoch 5701/10000, Training Loss: 0.6852317452430725, Validation Loss: 1.015397071838379\n",
      "Epoch 5702/10000, Training Loss: 0.6854400634765625, Validation Loss: 0.671074628829956\n",
      "Epoch 5703/10000, Training Loss: 0.6483116149902344, Validation Loss: 0.6934278011322021\n",
      "Epoch 5704/10000, Training Loss: 0.6650075912475586, Validation Loss: 0.8684412837028503\n",
      "Epoch 5705/10000, Training Loss: 0.8167797327041626, Validation Loss: 0.7452914118766785\n",
      "Epoch 5706/10000, Training Loss: 0.7463147640228271, Validation Loss: 0.6943280100822449\n",
      "Epoch 5707/10000, Training Loss: 0.778641402721405, Validation Loss: 0.6521444916725159\n",
      "Epoch 5708/10000, Training Loss: 0.6843151450157166, Validation Loss: 0.7916171550750732\n",
      "Epoch 5709/10000, Training Loss: 0.6572358012199402, Validation Loss: 0.538443922996521\n",
      "Epoch 5710/10000, Training Loss: 0.6970630288124084, Validation Loss: 0.6706373691558838\n",
      "Epoch 5711/10000, Training Loss: 0.701599657535553, Validation Loss: 1.1017078161239624\n",
      "Epoch 5712/10000, Training Loss: 0.701282799243927, Validation Loss: 1.011031985282898\n",
      "Epoch 5713/10000, Training Loss: 0.6942167282104492, Validation Loss: 0.7520873546600342\n",
      "Epoch 5714/10000, Training Loss: 0.6576032638549805, Validation Loss: 0.7218785285949707\n",
      "Epoch 5715/10000, Training Loss: 0.7450416088104248, Validation Loss: 0.9177188873291016\n",
      "Epoch 5716/10000, Training Loss: 0.6816362142562866, Validation Loss: 0.7778719067573547\n",
      "Epoch 5717/10000, Training Loss: 0.7109984755516052, Validation Loss: 0.9019653797149658\n",
      "Epoch 5718/10000, Training Loss: 0.6978495121002197, Validation Loss: 0.6526935696601868\n",
      "Epoch 5719/10000, Training Loss: 0.719779908657074, Validation Loss: 0.5878910422325134\n",
      "Epoch 5720/10000, Training Loss: 0.6535377502441406, Validation Loss: 0.5554516315460205\n",
      "Epoch 5721/10000, Training Loss: 0.7425070405006409, Validation Loss: 0.830976665019989\n",
      "Epoch 5722/10000, Training Loss: 0.7405866384506226, Validation Loss: 0.8031833171844482\n",
      "Epoch 5723/10000, Training Loss: 0.7058757543563843, Validation Loss: 0.6625614762306213\n",
      "Epoch 5724/10000, Training Loss: 0.6687660813331604, Validation Loss: 0.7814761996269226\n",
      "Epoch 5725/10000, Training Loss: 0.6363547444343567, Validation Loss: 0.9711508750915527\n",
      "Epoch 5726/10000, Training Loss: 0.6812331676483154, Validation Loss: 1.1193498373031616\n",
      "Epoch 5727/10000, Training Loss: 0.6747169494628906, Validation Loss: 0.7265616059303284\n",
      "Epoch 5728/10000, Training Loss: 0.7272065877914429, Validation Loss: 0.7014557719230652\n",
      "Epoch 5729/10000, Training Loss: 0.7279008030891418, Validation Loss: 0.8645117282867432\n",
      "Epoch 5730/10000, Training Loss: 0.7032860517501831, Validation Loss: 0.7681736350059509\n",
      "Epoch 5731/10000, Training Loss: 0.738922655582428, Validation Loss: 0.7619673609733582\n",
      "Epoch 5732/10000, Training Loss: 0.712993323802948, Validation Loss: 0.7226008772850037\n",
      "Epoch 5733/10000, Training Loss: 0.6925486326217651, Validation Loss: 0.6090357899665833\n",
      "Epoch 5734/10000, Training Loss: 0.6310416460037231, Validation Loss: 0.5520073771476746\n",
      "Epoch 5735/10000, Training Loss: 0.6721062064170837, Validation Loss: 0.8286364078521729\n",
      "Epoch 5736/10000, Training Loss: 0.6275956034660339, Validation Loss: 1.0467342138290405\n",
      "Epoch 5737/10000, Training Loss: 0.6828027367591858, Validation Loss: 0.7319478988647461\n",
      "Epoch 5738/10000, Training Loss: 0.6554259061813354, Validation Loss: 0.8007577061653137\n",
      "Epoch 5739/10000, Training Loss: 0.6902297139167786, Validation Loss: 0.7434428334236145\n",
      "Epoch 5740/10000, Training Loss: 0.6913525462150574, Validation Loss: 0.64500492811203\n",
      "Epoch 5741/10000, Training Loss: 0.685688853263855, Validation Loss: 0.8690414428710938\n",
      "Epoch 5742/10000, Training Loss: 0.7285823225975037, Validation Loss: 0.7956156134605408\n",
      "Epoch 5743/10000, Training Loss: 0.7004784345626831, Validation Loss: 0.7452430725097656\n",
      "Epoch 5744/10000, Training Loss: 0.6507447361946106, Validation Loss: 0.5687614679336548\n",
      "Epoch 5745/10000, Training Loss: 0.7226330041885376, Validation Loss: 1.022312045097351\n",
      "Epoch 5746/10000, Training Loss: 0.7346718907356262, Validation Loss: 0.8447969555854797\n",
      "Epoch 5747/10000, Training Loss: 0.7217485904693604, Validation Loss: 0.8616213202476501\n",
      "Epoch 5748/10000, Training Loss: 0.779845118522644, Validation Loss: 0.8423406481742859\n",
      "Epoch 5749/10000, Training Loss: 0.6452246904373169, Validation Loss: 0.749267578125\n",
      "Epoch 5750/10000, Training Loss: 0.7382224202156067, Validation Loss: 0.6488131880760193\n",
      "Epoch 5751/10000, Training Loss: 0.6980571150779724, Validation Loss: 0.7486072182655334\n",
      "Epoch 5752/10000, Training Loss: 0.6920257210731506, Validation Loss: 0.6390414237976074\n",
      "Epoch 5753/10000, Training Loss: 0.7006726861000061, Validation Loss: 0.5577647089958191\n",
      "Epoch 5754/10000, Training Loss: 0.6743115782737732, Validation Loss: 0.919782817363739\n",
      "Epoch 5755/10000, Training Loss: 0.6723762154579163, Validation Loss: 0.7453729510307312\n",
      "Epoch 5756/10000, Training Loss: 0.7085428833961487, Validation Loss: 0.8056526184082031\n",
      "Epoch 5757/10000, Training Loss: 0.6875123381614685, Validation Loss: 0.957188069820404\n",
      "Epoch 5758/10000, Training Loss: 0.7231836318969727, Validation Loss: 0.9115809798240662\n",
      "Epoch 5759/10000, Training Loss: 0.6851319670677185, Validation Loss: 1.0805948972702026\n",
      "Epoch 5760/10000, Training Loss: 0.7164264917373657, Validation Loss: 0.7134141325950623\n",
      "Epoch 5761/10000, Training Loss: 0.6910082101821899, Validation Loss: 1.4536046981811523\n",
      "Epoch 5762/10000, Training Loss: 0.7269226908683777, Validation Loss: 0.7663202285766602\n",
      "Epoch 5763/10000, Training Loss: 0.6669140458106995, Validation Loss: 0.7844323515892029\n",
      "Epoch 5764/10000, Training Loss: 0.6438060998916626, Validation Loss: 0.7284849286079407\n",
      "Epoch 5765/10000, Training Loss: 0.7742387056350708, Validation Loss: 0.7477136254310608\n",
      "Epoch 5766/10000, Training Loss: 0.7381766438484192, Validation Loss: 0.6511817574501038\n",
      "Epoch 5767/10000, Training Loss: 0.6816973090171814, Validation Loss: 0.716866672039032\n",
      "Epoch 5768/10000, Training Loss: 0.7116615772247314, Validation Loss: 0.681203305721283\n",
      "Epoch 5769/10000, Training Loss: 0.7821789979934692, Validation Loss: 0.8696754574775696\n",
      "Epoch 5770/10000, Training Loss: 0.6982837915420532, Validation Loss: 0.6343216300010681\n",
      "Epoch 5771/10000, Training Loss: 0.6890378594398499, Validation Loss: 0.8195355534553528\n",
      "Epoch 5772/10000, Training Loss: 0.7046083807945251, Validation Loss: 0.9456911087036133\n",
      "Epoch 5773/10000, Training Loss: 0.6732693314552307, Validation Loss: 0.7083280086517334\n",
      "Epoch 5774/10000, Training Loss: 0.6553524732589722, Validation Loss: 0.7876395583152771\n",
      "Epoch 5775/10000, Training Loss: 0.6963829398155212, Validation Loss: 0.4977162182331085\n",
      "Epoch 5776/10000, Training Loss: 0.6894826292991638, Validation Loss: 1.0880721807479858\n",
      "Epoch 5777/10000, Training Loss: 0.7224679589271545, Validation Loss: 0.6040419340133667\n",
      "Epoch 5778/10000, Training Loss: 0.7604313492774963, Validation Loss: 0.9216850399971008\n",
      "Epoch 5779/10000, Training Loss: 0.6942363381385803, Validation Loss: 0.8471463322639465\n",
      "Epoch 5780/10000, Training Loss: 0.7280206680297852, Validation Loss: 0.7015337944030762\n",
      "Epoch 5781/10000, Training Loss: 0.6542645692825317, Validation Loss: 0.8343794345855713\n",
      "Epoch 5782/10000, Training Loss: 0.72854083776474, Validation Loss: 0.5946516990661621\n",
      "Epoch 5783/10000, Training Loss: 0.7695156335830688, Validation Loss: 0.9345545768737793\n",
      "Epoch 5784/10000, Training Loss: 0.7290259599685669, Validation Loss: 0.7790114283561707\n",
      "Epoch 5785/10000, Training Loss: 0.7643600106239319, Validation Loss: 0.6709863543510437\n",
      "Epoch 5786/10000, Training Loss: 0.7887650728225708, Validation Loss: 0.7849288582801819\n",
      "Epoch 5787/10000, Training Loss: 0.7254305481910706, Validation Loss: 0.4408282935619354\n",
      "Epoch 5788/10000, Training Loss: 0.6711870431900024, Validation Loss: 0.9416406750679016\n",
      "Epoch 5789/10000, Training Loss: 0.7375416159629822, Validation Loss: 0.7248215079307556\n",
      "Epoch 5790/10000, Training Loss: 0.7161543369293213, Validation Loss: 0.772391140460968\n",
      "Epoch 5791/10000, Training Loss: 0.6671741008758545, Validation Loss: 0.8166518211364746\n",
      "Epoch 5792/10000, Training Loss: 0.7099381685256958, Validation Loss: 0.8383746147155762\n",
      "Epoch 5793/10000, Training Loss: 0.7116819620132446, Validation Loss: 1.1156134605407715\n",
      "Epoch 5794/10000, Training Loss: 0.7131955027580261, Validation Loss: 1.4187699556350708\n",
      "Epoch 5795/10000, Training Loss: 0.6715651154518127, Validation Loss: 0.8339831233024597\n",
      "Epoch 5796/10000, Training Loss: 0.6880267262458801, Validation Loss: 1.0642048120498657\n",
      "Epoch 5797/10000, Training Loss: 0.7175919413566589, Validation Loss: 1.056395411491394\n",
      "Epoch 5798/10000, Training Loss: 0.7369674444198608, Validation Loss: 0.594506561756134\n",
      "Epoch 5799/10000, Training Loss: 0.6861594319343567, Validation Loss: 0.6140360832214355\n",
      "Epoch 5800/10000, Training Loss: 0.6857435703277588, Validation Loss: 0.56031733751297\n",
      "Epoch 5801/10000, Training Loss: 0.674457848072052, Validation Loss: 0.4881286323070526\n",
      "Epoch 5802/10000, Training Loss: 0.6861907243728638, Validation Loss: 0.6225346326828003\n",
      "Epoch 5803/10000, Training Loss: 0.7289656400680542, Validation Loss: 0.9169661402702332\n",
      "Epoch 5804/10000, Training Loss: 0.6897867918014526, Validation Loss: 0.7908957004547119\n",
      "Epoch 5805/10000, Training Loss: 0.6906459331512451, Validation Loss: 0.8204075694084167\n",
      "Epoch 5806/10000, Training Loss: 0.7172316908836365, Validation Loss: 0.6643826961517334\n",
      "Epoch 5807/10000, Training Loss: 0.7033605575561523, Validation Loss: 0.870633602142334\n",
      "Epoch 5808/10000, Training Loss: 0.6798230409622192, Validation Loss: 0.8095038533210754\n",
      "Epoch 5809/10000, Training Loss: 0.679137110710144, Validation Loss: 0.8038284778594971\n",
      "Epoch 5810/10000, Training Loss: 0.6960725784301758, Validation Loss: 0.801436185836792\n",
      "Epoch 5811/10000, Training Loss: 0.6781622767448425, Validation Loss: 0.8048650622367859\n",
      "Epoch 5812/10000, Training Loss: 0.7475847601890564, Validation Loss: 0.7749257683753967\n",
      "Epoch 5813/10000, Training Loss: 0.6600074768066406, Validation Loss: 0.4445584714412689\n",
      "Epoch 5814/10000, Training Loss: 0.7142866849899292, Validation Loss: 0.8576583862304688\n",
      "Epoch 5815/10000, Training Loss: 0.6802952289581299, Validation Loss: 0.8777895569801331\n",
      "Epoch 5816/10000, Training Loss: 0.6103540062904358, Validation Loss: 1.0634509325027466\n",
      "Epoch 5817/10000, Training Loss: 0.7188590168952942, Validation Loss: 0.47384560108184814\n",
      "Epoch 5818/10000, Training Loss: 0.7186587452888489, Validation Loss: 0.9321506023406982\n",
      "Epoch 5819/10000, Training Loss: 0.7353410124778748, Validation Loss: 1.0521399974822998\n",
      "Epoch 5820/10000, Training Loss: 0.7132536172866821, Validation Loss: 0.651817262172699\n",
      "Epoch 5821/10000, Training Loss: 0.648767352104187, Validation Loss: 0.5962035059928894\n",
      "Epoch 5822/10000, Training Loss: 0.6601122617721558, Validation Loss: 0.681678056716919\n",
      "Epoch 5823/10000, Training Loss: 0.6571314930915833, Validation Loss: 0.6519057750701904\n",
      "Epoch 5824/10000, Training Loss: 0.7671060562133789, Validation Loss: 0.6462253928184509\n",
      "Epoch 5825/10000, Training Loss: 0.7016058564186096, Validation Loss: 0.9990963935852051\n",
      "Epoch 5826/10000, Training Loss: 0.6847632527351379, Validation Loss: 0.7512372136116028\n",
      "Epoch 5827/10000, Training Loss: 0.7177473306655884, Validation Loss: 0.7496110796928406\n",
      "Epoch 5828/10000, Training Loss: 0.6602343320846558, Validation Loss: 0.7416672706604004\n",
      "Epoch 5829/10000, Training Loss: 0.6636975407600403, Validation Loss: 0.7108209729194641\n",
      "Epoch 5830/10000, Training Loss: 0.6708230972290039, Validation Loss: 0.6199848055839539\n",
      "Epoch 5831/10000, Training Loss: 0.6539852619171143, Validation Loss: 0.8431637287139893\n",
      "Epoch 5832/10000, Training Loss: 0.6808236241340637, Validation Loss: 0.7441526055335999\n",
      "Epoch 5833/10000, Training Loss: 0.7436557412147522, Validation Loss: 0.7393509745597839\n",
      "Epoch 5834/10000, Training Loss: 0.6910709142684937, Validation Loss: 0.6915743350982666\n",
      "Epoch 5835/10000, Training Loss: 0.6970969438552856, Validation Loss: 0.7688028216362\n",
      "Epoch 5836/10000, Training Loss: 0.7083960175514221, Validation Loss: 0.7237123847007751\n",
      "Epoch 5837/10000, Training Loss: 0.6755586862564087, Validation Loss: 0.817431628704071\n",
      "Epoch 5838/10000, Training Loss: 0.6705307364463806, Validation Loss: 0.9152615070343018\n",
      "Epoch 5839/10000, Training Loss: 0.7056949138641357, Validation Loss: 0.8480942249298096\n",
      "Epoch 5840/10000, Training Loss: 0.7271489500999451, Validation Loss: 1.0219497680664062\n",
      "Epoch 5841/10000, Training Loss: 0.6711845993995667, Validation Loss: 0.7768095135688782\n",
      "Epoch 5842/10000, Training Loss: 0.6455062031745911, Validation Loss: 0.8593406677246094\n",
      "Epoch 5843/10000, Training Loss: 0.7168260216712952, Validation Loss: 0.7188892960548401\n",
      "Epoch 5844/10000, Training Loss: 0.6692977547645569, Validation Loss: 1.1438649892807007\n",
      "Epoch 5845/10000, Training Loss: 0.6463250517845154, Validation Loss: 0.5381091237068176\n",
      "Epoch 5846/10000, Training Loss: 0.6605218648910522, Validation Loss: 0.8824909329414368\n",
      "Epoch 5847/10000, Training Loss: 0.7032318115234375, Validation Loss: 0.8714177012443542\n",
      "Epoch 5848/10000, Training Loss: 0.670677900314331, Validation Loss: 0.4962862730026245\n",
      "Epoch 5849/10000, Training Loss: 0.6549230813980103, Validation Loss: 0.6988816261291504\n",
      "Epoch 5850/10000, Training Loss: 0.690630316734314, Validation Loss: 0.9232409596443176\n",
      "Epoch 5851/10000, Training Loss: 0.7199201583862305, Validation Loss: 0.40703487396240234\n",
      "Epoch 5852/10000, Training Loss: 0.6463230848312378, Validation Loss: 0.8434579372406006\n",
      "Epoch 5853/10000, Training Loss: 0.6597837209701538, Validation Loss: 0.9324575066566467\n",
      "Epoch 5854/10000, Training Loss: 0.6668110489845276, Validation Loss: 0.9193446040153503\n",
      "Epoch 5855/10000, Training Loss: 0.6560357809066772, Validation Loss: 0.7023296356201172\n",
      "Epoch 5856/10000, Training Loss: 0.8876024484634399, Validation Loss: 0.7166077494621277\n",
      "Epoch 5857/10000, Training Loss: 0.7325462102890015, Validation Loss: 0.5060702562332153\n",
      "Epoch 5858/10000, Training Loss: 0.6469557285308838, Validation Loss: 0.6718103289604187\n",
      "Epoch 5859/10000, Training Loss: 0.6766241788864136, Validation Loss: 0.7071518301963806\n",
      "Epoch 5860/10000, Training Loss: 0.6511169672012329, Validation Loss: 0.7927919030189514\n",
      "Epoch 5861/10000, Training Loss: 0.6976083517074585, Validation Loss: 0.5906611680984497\n",
      "Epoch 5862/10000, Training Loss: 0.7002620100975037, Validation Loss: 0.7717989087104797\n",
      "Epoch 5863/10000, Training Loss: 0.7597905397415161, Validation Loss: 0.5306664705276489\n",
      "Epoch 5864/10000, Training Loss: 0.6982566118240356, Validation Loss: 0.7021446228027344\n",
      "Epoch 5865/10000, Training Loss: 0.6908093690872192, Validation Loss: 0.8000195622444153\n",
      "Epoch 5866/10000, Training Loss: 0.7533422708511353, Validation Loss: 0.8359261155128479\n",
      "Epoch 5867/10000, Training Loss: 0.6812962889671326, Validation Loss: 0.5332251191139221\n",
      "Epoch 5868/10000, Training Loss: 0.666061520576477, Validation Loss: 0.6198212504386902\n",
      "Epoch 5869/10000, Training Loss: 0.6938218474388123, Validation Loss: 0.7170537114143372\n",
      "Epoch 5870/10000, Training Loss: 0.7320224046707153, Validation Loss: 1.0123857259750366\n",
      "Epoch 5871/10000, Training Loss: 0.6858336925506592, Validation Loss: 0.6404359936714172\n",
      "Epoch 5872/10000, Training Loss: 0.6886640787124634, Validation Loss: 0.7106609344482422\n",
      "Epoch 5873/10000, Training Loss: 0.7057101726531982, Validation Loss: 0.6164631843566895\n",
      "Epoch 5874/10000, Training Loss: 0.6782616972923279, Validation Loss: 0.6206696629524231\n",
      "Epoch 5875/10000, Training Loss: 0.6604611873626709, Validation Loss: 0.5895960927009583\n",
      "Epoch 5876/10000, Training Loss: 0.6738600134849548, Validation Loss: 0.7466064095497131\n",
      "Epoch 5877/10000, Training Loss: 0.6760920286178589, Validation Loss: 0.6455660462379456\n",
      "Epoch 5878/10000, Training Loss: 0.7058541178703308, Validation Loss: 0.7420527338981628\n",
      "Epoch 5879/10000, Training Loss: 0.675967812538147, Validation Loss: 0.7222855687141418\n",
      "Epoch 5880/10000, Training Loss: 0.7299765348434448, Validation Loss: 0.9105611443519592\n",
      "Epoch 5881/10000, Training Loss: 0.6757635474205017, Validation Loss: 0.7278401255607605\n",
      "Epoch 5882/10000, Training Loss: 0.6736965775489807, Validation Loss: 0.8323531150817871\n",
      "Epoch 5883/10000, Training Loss: 0.6919708251953125, Validation Loss: 0.913465678691864\n",
      "Epoch 5884/10000, Training Loss: 0.6834906935691833, Validation Loss: 0.9836856722831726\n",
      "Epoch 5885/10000, Training Loss: 0.6966590285301208, Validation Loss: 0.6305128335952759\n",
      "Epoch 5886/10000, Training Loss: 0.6523171663284302, Validation Loss: 0.7670995593070984\n",
      "Epoch 5887/10000, Training Loss: 0.6809081435203552, Validation Loss: 0.6131067276000977\n",
      "Epoch 5888/10000, Training Loss: 0.7343749403953552, Validation Loss: 0.6574986577033997\n",
      "Epoch 5889/10000, Training Loss: 0.6892595887184143, Validation Loss: 0.8558986783027649\n",
      "Epoch 5890/10000, Training Loss: 0.693115770816803, Validation Loss: 0.7470097541809082\n",
      "Epoch 5891/10000, Training Loss: 0.7177912592887878, Validation Loss: 0.8551726937294006\n",
      "Epoch 5892/10000, Training Loss: 0.6908902525901794, Validation Loss: 0.6660230755805969\n",
      "Epoch 5893/10000, Training Loss: 0.6981471180915833, Validation Loss: 0.8707429766654968\n",
      "Epoch 5894/10000, Training Loss: 0.6685094833374023, Validation Loss: 0.760591983795166\n",
      "Epoch 5895/10000, Training Loss: 0.6933459639549255, Validation Loss: 0.6975722312927246\n",
      "Epoch 5896/10000, Training Loss: 0.7178488373756409, Validation Loss: 0.8456785678863525\n",
      "Epoch 5897/10000, Training Loss: 0.6982804536819458, Validation Loss: 0.6731497645378113\n",
      "Epoch 5898/10000, Training Loss: 0.669700026512146, Validation Loss: 0.6915910840034485\n",
      "Epoch 5899/10000, Training Loss: 0.6794208288192749, Validation Loss: 0.7687602639198303\n",
      "Epoch 5900/10000, Training Loss: 0.7287359237670898, Validation Loss: 0.6188485026359558\n",
      "Epoch 5901/10000, Training Loss: 0.6978875398635864, Validation Loss: 0.7974104881286621\n",
      "Epoch 5902/10000, Training Loss: 0.6680567860603333, Validation Loss: 0.5179646015167236\n",
      "Epoch 5903/10000, Training Loss: 0.7503976821899414, Validation Loss: 0.9406664371490479\n",
      "Epoch 5904/10000, Training Loss: 0.6624935865402222, Validation Loss: 0.5914029479026794\n",
      "Epoch 5905/10000, Training Loss: 0.6650241017341614, Validation Loss: 0.6355605721473694\n",
      "Epoch 5906/10000, Training Loss: 0.6983225345611572, Validation Loss: 0.9040632247924805\n",
      "Epoch 5907/10000, Training Loss: 0.6970859169960022, Validation Loss: 0.894700825214386\n",
      "Epoch 5908/10000, Training Loss: 0.6593344807624817, Validation Loss: 0.6995882391929626\n",
      "Epoch 5909/10000, Training Loss: 0.6586506366729736, Validation Loss: 0.5650937557220459\n",
      "Epoch 5910/10000, Training Loss: 0.6854696869850159, Validation Loss: 0.5302960872650146\n",
      "Epoch 5911/10000, Training Loss: 0.6785170435905457, Validation Loss: 0.7714142203330994\n",
      "Epoch 5912/10000, Training Loss: 0.726487398147583, Validation Loss: 0.8620992302894592\n",
      "Epoch 5913/10000, Training Loss: 0.6737762093544006, Validation Loss: 0.6501161456108093\n",
      "Epoch 5914/10000, Training Loss: 0.6778280735015869, Validation Loss: 1.0924195051193237\n",
      "Epoch 5915/10000, Training Loss: 0.6810118556022644, Validation Loss: 0.6795719265937805\n",
      "Epoch 5916/10000, Training Loss: 0.6688177585601807, Validation Loss: 0.6679010391235352\n",
      "Epoch 5917/10000, Training Loss: 0.6631934642791748, Validation Loss: 0.6354125142097473\n",
      "Epoch 5918/10000, Training Loss: 0.7380626201629639, Validation Loss: 0.759120762348175\n",
      "Epoch 5919/10000, Training Loss: 0.6550467610359192, Validation Loss: 0.8211102485656738\n",
      "Epoch 5920/10000, Training Loss: 0.6840683817863464, Validation Loss: 0.7300508618354797\n",
      "Epoch 5921/10000, Training Loss: 0.6777054071426392, Validation Loss: 0.6463701725006104\n",
      "Epoch 5922/10000, Training Loss: 0.6556727886199951, Validation Loss: 0.7569286823272705\n",
      "Epoch 5923/10000, Training Loss: 0.6554396152496338, Validation Loss: 0.6524378657341003\n",
      "Epoch 5924/10000, Training Loss: 0.66219162940979, Validation Loss: 0.6035812497138977\n",
      "Epoch 5925/10000, Training Loss: 0.7551276683807373, Validation Loss: 0.891162633895874\n",
      "Epoch 5926/10000, Training Loss: 0.6977882981300354, Validation Loss: 1.0647263526916504\n",
      "Epoch 5927/10000, Training Loss: 0.6663802266120911, Validation Loss: 0.8584952354431152\n",
      "Epoch 5928/10000, Training Loss: 0.7215643525123596, Validation Loss: 0.6332427263259888\n",
      "Epoch 5929/10000, Training Loss: 0.6828353404998779, Validation Loss: 0.7320087552070618\n",
      "Epoch 5930/10000, Training Loss: 0.7912118434906006, Validation Loss: 0.9624557495117188\n",
      "Epoch 5931/10000, Training Loss: 0.7046082019805908, Validation Loss: 0.806486189365387\n",
      "Epoch 5932/10000, Training Loss: 0.6967967748641968, Validation Loss: 0.5822957158088684\n",
      "Epoch 5933/10000, Training Loss: 0.7538136839866638, Validation Loss: 1.0628589391708374\n",
      "Epoch 5934/10000, Training Loss: 0.7155435085296631, Validation Loss: 0.6668201088905334\n",
      "Epoch 5935/10000, Training Loss: 0.6463549137115479, Validation Loss: 0.5995582938194275\n",
      "Epoch 5936/10000, Training Loss: 0.7231975197792053, Validation Loss: 0.9423765540122986\n",
      "Epoch 5937/10000, Training Loss: 0.7188145518302917, Validation Loss: 0.7625884413719177\n",
      "Epoch 5938/10000, Training Loss: 0.6774755716323853, Validation Loss: 0.6405555605888367\n",
      "Epoch 5939/10000, Training Loss: 0.6966596841812134, Validation Loss: 0.5961534976959229\n",
      "Epoch 5940/10000, Training Loss: 0.6901905536651611, Validation Loss: 0.6284833550453186\n",
      "Epoch 5941/10000, Training Loss: 0.6797208189964294, Validation Loss: 0.7223594188690186\n",
      "Epoch 5942/10000, Training Loss: 0.6950559616088867, Validation Loss: 0.5899909138679504\n",
      "Epoch 5943/10000, Training Loss: 0.7022461295127869, Validation Loss: 0.9746344089508057\n",
      "Epoch 5944/10000, Training Loss: 0.6944854259490967, Validation Loss: 0.7211549282073975\n",
      "Epoch 5945/10000, Training Loss: 0.7246708273887634, Validation Loss: 0.8524930477142334\n",
      "Epoch 5946/10000, Training Loss: 0.719484269618988, Validation Loss: 0.8879878520965576\n",
      "Epoch 5947/10000, Training Loss: 0.688539445400238, Validation Loss: 0.820993185043335\n",
      "Epoch 5948/10000, Training Loss: 0.661203920841217, Validation Loss: 0.5848744511604309\n",
      "Epoch 5949/10000, Training Loss: 0.7041829824447632, Validation Loss: 0.6602668166160583\n",
      "Epoch 5950/10000, Training Loss: 0.6785443425178528, Validation Loss: 0.6507239937782288\n",
      "Epoch 5951/10000, Training Loss: 0.6384419798851013, Validation Loss: 0.6681447625160217\n",
      "Epoch 5952/10000, Training Loss: 0.714046061038971, Validation Loss: 1.0128346681594849\n",
      "Epoch 5953/10000, Training Loss: 0.6991644501686096, Validation Loss: 0.5367365479469299\n",
      "Epoch 5954/10000, Training Loss: 0.7021223902702332, Validation Loss: 0.7635776400566101\n",
      "Epoch 5955/10000, Training Loss: 0.6922705769538879, Validation Loss: 0.6391454339027405\n",
      "Epoch 5956/10000, Training Loss: 0.6887975931167603, Validation Loss: 0.7436001896858215\n",
      "Epoch 5957/10000, Training Loss: 0.6667551398277283, Validation Loss: 0.5780049562454224\n",
      "Epoch 5958/10000, Training Loss: 0.6547825336456299, Validation Loss: 0.5773563385009766\n",
      "Epoch 5959/10000, Training Loss: 0.6927992105484009, Validation Loss: 0.5869304537773132\n",
      "Epoch 5960/10000, Training Loss: 0.7037548422813416, Validation Loss: 0.8006530404090881\n",
      "Epoch 5961/10000, Training Loss: 0.7355981469154358, Validation Loss: 1.035382866859436\n",
      "Epoch 5962/10000, Training Loss: 0.7244558930397034, Validation Loss: 0.7603979706764221\n",
      "Epoch 5963/10000, Training Loss: 0.6628809571266174, Validation Loss: 0.6604004502296448\n",
      "Epoch 5964/10000, Training Loss: 0.6831140518188477, Validation Loss: 0.633644163608551\n",
      "Epoch 5965/10000, Training Loss: 0.6861795783042908, Validation Loss: 1.0868515968322754\n",
      "Epoch 5966/10000, Training Loss: 0.7017730474472046, Validation Loss: 0.8194093108177185\n",
      "Epoch 5967/10000, Training Loss: 0.6625750660896301, Validation Loss: 0.7355334162712097\n",
      "Epoch 5968/10000, Training Loss: 0.6916218400001526, Validation Loss: 0.8480899930000305\n",
      "Epoch 5969/10000, Training Loss: 0.728316605091095, Validation Loss: 0.7955493927001953\n",
      "Epoch 5970/10000, Training Loss: 0.6904333829879761, Validation Loss: 0.7577729821205139\n",
      "Epoch 5971/10000, Training Loss: 0.6374509334564209, Validation Loss: 0.6635972261428833\n",
      "Epoch 5972/10000, Training Loss: 0.6650383472442627, Validation Loss: 0.795337975025177\n",
      "Epoch 5973/10000, Training Loss: 0.6578547954559326, Validation Loss: 0.6585093140602112\n",
      "Epoch 5974/10000, Training Loss: 0.7668271660804749, Validation Loss: 0.7509284615516663\n",
      "Epoch 5975/10000, Training Loss: 0.7072811722755432, Validation Loss: 0.743791401386261\n",
      "Epoch 5976/10000, Training Loss: 0.6364105939865112, Validation Loss: 0.7047907710075378\n",
      "Epoch 5977/10000, Training Loss: 0.6855410933494568, Validation Loss: 0.755551815032959\n",
      "Epoch 5978/10000, Training Loss: 0.6797625422477722, Validation Loss: 0.6058724522590637\n",
      "Epoch 5979/10000, Training Loss: 0.6824798583984375, Validation Loss: 0.9611072540283203\n",
      "Epoch 5980/10000, Training Loss: 0.6490398645401001, Validation Loss: 0.7628580927848816\n",
      "Epoch 5981/10000, Training Loss: 0.6080490350723267, Validation Loss: 0.7826933264732361\n",
      "Epoch 5982/10000, Training Loss: 0.7157465219497681, Validation Loss: 0.611682116985321\n",
      "Epoch 5983/10000, Training Loss: 0.7164281606674194, Validation Loss: 0.71172696352005\n",
      "Epoch 5984/10000, Training Loss: 0.7101377844810486, Validation Loss: 1.0715322494506836\n",
      "Epoch 5985/10000, Training Loss: 0.7177997827529907, Validation Loss: 0.5885133743286133\n",
      "Epoch 5986/10000, Training Loss: 0.6763020753860474, Validation Loss: 0.6310631632804871\n",
      "Epoch 5987/10000, Training Loss: 0.6662464737892151, Validation Loss: 0.5958724617958069\n",
      "Epoch 5988/10000, Training Loss: 0.6726128458976746, Validation Loss: 0.8314257264137268\n",
      "Epoch 5989/10000, Training Loss: 0.7213319540023804, Validation Loss: 0.7826058268547058\n",
      "Epoch 5990/10000, Training Loss: 0.6948201060295105, Validation Loss: 0.6870923042297363\n",
      "Epoch 5991/10000, Training Loss: 0.6306942701339722, Validation Loss: 0.9421408176422119\n",
      "Epoch 5992/10000, Training Loss: 0.679047703742981, Validation Loss: 0.7195661664009094\n",
      "Epoch 5993/10000, Training Loss: 0.6948543190956116, Validation Loss: 0.7256679534912109\n",
      "Epoch 5994/10000, Training Loss: 0.6327788829803467, Validation Loss: 0.9227347373962402\n",
      "Epoch 5995/10000, Training Loss: 0.721336305141449, Validation Loss: 0.6870318055152893\n",
      "Epoch 5996/10000, Training Loss: 0.6459773182868958, Validation Loss: 0.4864370822906494\n",
      "Epoch 5997/10000, Training Loss: 0.7064756751060486, Validation Loss: 0.7485373616218567\n",
      "Epoch 5998/10000, Training Loss: 0.7270309925079346, Validation Loss: 1.0169646739959717\n",
      "Epoch 5999/10000, Training Loss: 0.6710605621337891, Validation Loss: 0.6490058302879333\n",
      "Epoch 6000/10000, Training Loss: 0.7283830642700195, Validation Loss: 1.0417760610580444\n",
      "Epoch 6001/10000, Training Loss: 0.6511825323104858, Validation Loss: 0.5245603919029236\n",
      "Epoch 6002/10000, Training Loss: 0.7510427832603455, Validation Loss: 0.8955467343330383\n",
      "Epoch 6003/10000, Training Loss: 0.6282249689102173, Validation Loss: 0.6136288046836853\n",
      "Epoch 6004/10000, Training Loss: 0.6433560252189636, Validation Loss: 0.6253551840782166\n",
      "Epoch 6005/10000, Training Loss: 0.6537832021713257, Validation Loss: 0.8279051184654236\n",
      "Epoch 6006/10000, Training Loss: 0.710232138633728, Validation Loss: 0.8630509972572327\n",
      "Epoch 6007/10000, Training Loss: 0.6863647103309631, Validation Loss: 0.7681562304496765\n",
      "Epoch 6008/10000, Training Loss: 0.6489824652671814, Validation Loss: 0.6364957690238953\n",
      "Epoch 6009/10000, Training Loss: 0.6741319894790649, Validation Loss: 0.7012960910797119\n",
      "Epoch 6010/10000, Training Loss: 0.7014433145523071, Validation Loss: 0.5302388072013855\n",
      "Epoch 6011/10000, Training Loss: 0.6594244241714478, Validation Loss: 0.6858784556388855\n",
      "Epoch 6012/10000, Training Loss: 0.6786159873008728, Validation Loss: 0.6638789176940918\n",
      "Epoch 6013/10000, Training Loss: 0.6560328006744385, Validation Loss: 0.7711639404296875\n",
      "Epoch 6014/10000, Training Loss: 0.681757390499115, Validation Loss: 0.7618221640586853\n",
      "Epoch 6015/10000, Training Loss: 0.6437451839447021, Validation Loss: 0.6681550145149231\n",
      "Epoch 6016/10000, Training Loss: 0.7122141122817993, Validation Loss: 0.5605724453926086\n",
      "Epoch 6017/10000, Training Loss: 0.7750750184059143, Validation Loss: 0.9154744744300842\n",
      "Epoch 6018/10000, Training Loss: 0.7238348722457886, Validation Loss: 0.7865216135978699\n",
      "Epoch 6019/10000, Training Loss: 0.6857911348342896, Validation Loss: 0.8398630619049072\n",
      "Epoch 6020/10000, Training Loss: 0.7129124402999878, Validation Loss: 0.8622658252716064\n",
      "Epoch 6021/10000, Training Loss: 0.6646715402603149, Validation Loss: 0.718036949634552\n",
      "Epoch 6022/10000, Training Loss: 0.7278818488121033, Validation Loss: 0.9075238108634949\n",
      "Epoch 6023/10000, Training Loss: 0.6159743070602417, Validation Loss: 1.3516303300857544\n",
      "Epoch 6024/10000, Training Loss: 0.7066689729690552, Validation Loss: 0.766625165939331\n",
      "Epoch 6025/10000, Training Loss: 0.6703125834465027, Validation Loss: 0.7465085983276367\n",
      "Epoch 6026/10000, Training Loss: 0.6803906559944153, Validation Loss: 0.8613156676292419\n",
      "Epoch 6027/10000, Training Loss: 0.6808375716209412, Validation Loss: 0.7606263160705566\n",
      "Epoch 6028/10000, Training Loss: 0.6760854125022888, Validation Loss: 0.6293557286262512\n",
      "Epoch 6029/10000, Training Loss: 0.6437282562255859, Validation Loss: 0.6276859641075134\n",
      "Epoch 6030/10000, Training Loss: 0.6986267566680908, Validation Loss: 0.7785165905952454\n",
      "Epoch 6031/10000, Training Loss: 0.8183851838111877, Validation Loss: 1.0205025672912598\n",
      "Epoch 6032/10000, Training Loss: 0.6971840262413025, Validation Loss: 0.6399065852165222\n",
      "Epoch 6033/10000, Training Loss: 0.7037528157234192, Validation Loss: 0.8867213726043701\n",
      "Epoch 6034/10000, Training Loss: 0.6987537741661072, Validation Loss: 1.01822030544281\n",
      "Epoch 6035/10000, Training Loss: 0.6692612767219543, Validation Loss: 0.8729035258293152\n",
      "Epoch 6036/10000, Training Loss: 0.6684904098510742, Validation Loss: 0.9395339488983154\n",
      "Epoch 6037/10000, Training Loss: 0.7210449576377869, Validation Loss: 0.8752623200416565\n",
      "Epoch 6038/10000, Training Loss: 0.7048240303993225, Validation Loss: 0.7850160002708435\n",
      "Epoch 6039/10000, Training Loss: 0.7475236654281616, Validation Loss: 0.9852340817451477\n",
      "Epoch 6040/10000, Training Loss: 0.6843133568763733, Validation Loss: 0.6516550183296204\n",
      "Epoch 6041/10000, Training Loss: 0.6507691740989685, Validation Loss: 0.8770337700843811\n",
      "Epoch 6042/10000, Training Loss: 0.7313754558563232, Validation Loss: 0.7866643071174622\n",
      "Epoch 6043/10000, Training Loss: 0.6750072836875916, Validation Loss: 0.6454671621322632\n",
      "Epoch 6044/10000, Training Loss: 0.6686968207359314, Validation Loss: 0.695167064666748\n",
      "Epoch 6045/10000, Training Loss: 0.7219856977462769, Validation Loss: 0.7817296385765076\n",
      "Epoch 6046/10000, Training Loss: 0.6437861919403076, Validation Loss: 0.6309972405433655\n",
      "Epoch 6047/10000, Training Loss: 0.6828290224075317, Validation Loss: 0.8194780349731445\n",
      "Epoch 6048/10000, Training Loss: 0.6251096129417419, Validation Loss: 0.6540706753730774\n",
      "Epoch 6049/10000, Training Loss: 0.7243096828460693, Validation Loss: 0.6474928855895996\n",
      "Epoch 6050/10000, Training Loss: 0.6826764345169067, Validation Loss: 0.7963969111442566\n",
      "Epoch 6051/10000, Training Loss: 0.6673621535301208, Validation Loss: 0.6279584169387817\n",
      "Epoch 6052/10000, Training Loss: 0.6941412687301636, Validation Loss: 0.7298714518547058\n",
      "Epoch 6053/10000, Training Loss: 0.7014958262443542, Validation Loss: 0.9165922999382019\n",
      "Epoch 6054/10000, Training Loss: 0.6725728511810303, Validation Loss: 0.5138000249862671\n",
      "Epoch 6055/10000, Training Loss: 0.6839390397071838, Validation Loss: 0.5484891533851624\n",
      "Epoch 6056/10000, Training Loss: 0.6625109910964966, Validation Loss: 0.7748655676841736\n",
      "Epoch 6057/10000, Training Loss: 0.6827100515365601, Validation Loss: 0.7870365977287292\n",
      "Epoch 6058/10000, Training Loss: 0.6509578227996826, Validation Loss: 0.666795551776886\n",
      "Epoch 6059/10000, Training Loss: 0.6725804805755615, Validation Loss: 0.9206321835517883\n",
      "Epoch 6060/10000, Training Loss: 0.6534926295280457, Validation Loss: 0.6482402086257935\n",
      "Epoch 6061/10000, Training Loss: 0.712394654750824, Validation Loss: 0.6920387148857117\n",
      "Epoch 6062/10000, Training Loss: 0.696929931640625, Validation Loss: 0.7104905247688293\n",
      "Epoch 6063/10000, Training Loss: 0.696757972240448, Validation Loss: 0.624873161315918\n",
      "Epoch 6064/10000, Training Loss: 0.6931247115135193, Validation Loss: 0.5474666953086853\n",
      "Epoch 6065/10000, Training Loss: 0.6803824305534363, Validation Loss: 0.7131469249725342\n",
      "Epoch 6066/10000, Training Loss: 0.6738672852516174, Validation Loss: 0.8909274935722351\n",
      "Epoch 6067/10000, Training Loss: 0.6415790915489197, Validation Loss: 0.6360257267951965\n",
      "Epoch 6068/10000, Training Loss: 0.7336121201515198, Validation Loss: 0.6680619120597839\n",
      "Epoch 6069/10000, Training Loss: 0.7197949886322021, Validation Loss: 0.6691422462463379\n",
      "Epoch 6070/10000, Training Loss: 0.6554380655288696, Validation Loss: 0.8226205706596375\n",
      "Epoch 6071/10000, Training Loss: 0.7227081060409546, Validation Loss: 0.669294536113739\n",
      "Epoch 6072/10000, Training Loss: 0.6791602373123169, Validation Loss: 0.8119639754295349\n",
      "Epoch 6073/10000, Training Loss: 0.7098738551139832, Validation Loss: 0.9012710452079773\n",
      "Epoch 6074/10000, Training Loss: 0.6934944987297058, Validation Loss: 0.7811686992645264\n",
      "Epoch 6075/10000, Training Loss: 0.6891971230506897, Validation Loss: 0.6890910267829895\n",
      "Epoch 6076/10000, Training Loss: 0.6489890813827515, Validation Loss: 0.6942052245140076\n",
      "Epoch 6077/10000, Training Loss: 0.6492735147476196, Validation Loss: 0.7579092979431152\n",
      "Epoch 6078/10000, Training Loss: 0.6813414692878723, Validation Loss: 0.4431062936782837\n",
      "Epoch 6079/10000, Training Loss: 0.6140506267547607, Validation Loss: 0.6494982838630676\n",
      "Epoch 6080/10000, Training Loss: 0.6386673450469971, Validation Loss: 0.6373576521873474\n",
      "Epoch 6081/10000, Training Loss: 0.7178735136985779, Validation Loss: 0.6490814685821533\n",
      "Epoch 6082/10000, Training Loss: 0.738894522190094, Validation Loss: 0.7501983642578125\n",
      "Epoch 6083/10000, Training Loss: 0.7249851226806641, Validation Loss: 0.6201775074005127\n",
      "Epoch 6084/10000, Training Loss: 0.7122509479522705, Validation Loss: 0.7853764891624451\n",
      "Epoch 6085/10000, Training Loss: 0.7700031995773315, Validation Loss: 0.6747066974639893\n",
      "Epoch 6086/10000, Training Loss: 0.7249562740325928, Validation Loss: 0.7673037648200989\n",
      "Epoch 6087/10000, Training Loss: 0.6597850322723389, Validation Loss: 0.6032187342643738\n",
      "Epoch 6088/10000, Training Loss: 0.6516322493553162, Validation Loss: 0.7641677856445312\n",
      "Epoch 6089/10000, Training Loss: 0.6744191646575928, Validation Loss: 0.639890193939209\n",
      "Epoch 6090/10000, Training Loss: 0.6708783507347107, Validation Loss: 0.796000063419342\n",
      "Epoch 6091/10000, Training Loss: 0.6359573006629944, Validation Loss: 0.7016169428825378\n",
      "Epoch 6092/10000, Training Loss: 0.6920451521873474, Validation Loss: 0.8384304642677307\n",
      "Epoch 6093/10000, Training Loss: 0.66973876953125, Validation Loss: 0.7473309636116028\n",
      "Epoch 6094/10000, Training Loss: 0.7392635345458984, Validation Loss: 0.6321441531181335\n",
      "Epoch 6095/10000, Training Loss: 0.6610056757926941, Validation Loss: 0.7722113728523254\n",
      "Epoch 6096/10000, Training Loss: 0.6630081534385681, Validation Loss: 0.5915798544883728\n",
      "Epoch 6097/10000, Training Loss: 0.6719611287117004, Validation Loss: 0.7774752974510193\n",
      "Epoch 6098/10000, Training Loss: 0.7313379645347595, Validation Loss: 0.6200078129768372\n",
      "Epoch 6099/10000, Training Loss: 0.6837872266769409, Validation Loss: 0.8718183636665344\n",
      "Epoch 6100/10000, Training Loss: 0.669928789138794, Validation Loss: 0.7411845326423645\n",
      "Epoch 6101/10000, Training Loss: 0.6468836069107056, Validation Loss: 0.937910795211792\n",
      "Epoch 6102/10000, Training Loss: 0.6203143000602722, Validation Loss: 0.5568957328796387\n",
      "Epoch 6103/10000, Training Loss: 0.6746972799301147, Validation Loss: 0.6757502555847168\n",
      "Epoch 6104/10000, Training Loss: 0.7003217935562134, Validation Loss: 0.5907542705535889\n",
      "Epoch 6105/10000, Training Loss: 0.6537438631057739, Validation Loss: 0.7801240086555481\n",
      "Epoch 6106/10000, Training Loss: 0.618302047252655, Validation Loss: 0.7425789833068848\n",
      "Epoch 6107/10000, Training Loss: 0.6943874359130859, Validation Loss: 0.7642123699188232\n",
      "Epoch 6108/10000, Training Loss: 0.6622610688209534, Validation Loss: 0.7262452244758606\n",
      "Epoch 6109/10000, Training Loss: 0.6901452541351318, Validation Loss: 0.6776139140129089\n",
      "Epoch 6110/10000, Training Loss: 0.6516921520233154, Validation Loss: 0.974684476852417\n",
      "Epoch 6111/10000, Training Loss: 0.6686710715293884, Validation Loss: 0.8982546925544739\n",
      "Epoch 6112/10000, Training Loss: 0.686154305934906, Validation Loss: 0.7462897896766663\n",
      "Epoch 6113/10000, Training Loss: 0.7039221525192261, Validation Loss: 0.5699393153190613\n",
      "Epoch 6114/10000, Training Loss: 0.6420755386352539, Validation Loss: 1.0911227464675903\n",
      "Epoch 6115/10000, Training Loss: 0.6764694452285767, Validation Loss: 0.6494722962379456\n",
      "Epoch 6116/10000, Training Loss: 0.6811103224754333, Validation Loss: 0.7239315509796143\n",
      "Epoch 6117/10000, Training Loss: 0.634308934211731, Validation Loss: 0.5485230088233948\n",
      "Epoch 6118/10000, Training Loss: 0.6933909058570862, Validation Loss: 0.7423193454742432\n",
      "Epoch 6119/10000, Training Loss: 0.719565749168396, Validation Loss: 0.7015482783317566\n",
      "Epoch 6120/10000, Training Loss: 0.709863543510437, Validation Loss: 0.7477971911430359\n",
      "Epoch 6121/10000, Training Loss: 0.6568077802658081, Validation Loss: 0.6695367693901062\n",
      "Epoch 6122/10000, Training Loss: 0.6658403873443604, Validation Loss: 0.5696636438369751\n",
      "Epoch 6123/10000, Training Loss: 0.6627317070960999, Validation Loss: 0.6117480397224426\n",
      "Epoch 6124/10000, Training Loss: 0.6869895458221436, Validation Loss: 0.6881475448608398\n",
      "Epoch 6125/10000, Training Loss: 0.7565438151359558, Validation Loss: 0.6764821410179138\n",
      "Epoch 6126/10000, Training Loss: 0.6626242399215698, Validation Loss: 0.8524311184883118\n",
      "Epoch 6127/10000, Training Loss: 0.6441785097122192, Validation Loss: 0.6168705821037292\n",
      "Epoch 6128/10000, Training Loss: 0.6537696123123169, Validation Loss: 0.47461947798728943\n",
      "Epoch 6129/10000, Training Loss: 0.6963621377944946, Validation Loss: 0.5511814951896667\n",
      "Epoch 6130/10000, Training Loss: 0.7110410332679749, Validation Loss: 1.0165274143218994\n",
      "Epoch 6131/10000, Training Loss: 0.6180365085601807, Validation Loss: 0.7471575140953064\n",
      "Epoch 6132/10000, Training Loss: 0.6857273578643799, Validation Loss: 0.6879724860191345\n",
      "Epoch 6133/10000, Training Loss: 0.6763015389442444, Validation Loss: 0.7293694019317627\n",
      "Epoch 6134/10000, Training Loss: 0.6539840698242188, Validation Loss: 0.6630824208259583\n",
      "Epoch 6135/10000, Training Loss: 0.683425784111023, Validation Loss: 0.5839719176292419\n",
      "Epoch 6136/10000, Training Loss: 0.6513843536376953, Validation Loss: 0.5962149500846863\n",
      "Epoch 6137/10000, Training Loss: 0.7111411690711975, Validation Loss: 0.7442271709442139\n",
      "Epoch 6138/10000, Training Loss: 0.6529322862625122, Validation Loss: 0.935349702835083\n",
      "Epoch 6139/10000, Training Loss: 0.6906504034996033, Validation Loss: 0.7111890316009521\n",
      "Epoch 6140/10000, Training Loss: 0.6919062733650208, Validation Loss: 0.839137852191925\n",
      "Epoch 6141/10000, Training Loss: 0.6651526093482971, Validation Loss: 0.5568998456001282\n",
      "Epoch 6142/10000, Training Loss: 0.6737388372421265, Validation Loss: 0.8629716038703918\n",
      "Epoch 6143/10000, Training Loss: 0.6939412355422974, Validation Loss: 0.5288817286491394\n",
      "Epoch 6144/10000, Training Loss: 0.656257688999176, Validation Loss: 0.7596896290779114\n",
      "Epoch 6145/10000, Training Loss: 0.6722710728645325, Validation Loss: 0.7054087519645691\n",
      "Epoch 6146/10000, Training Loss: 0.6860342621803284, Validation Loss: 0.7172598838806152\n",
      "Epoch 6147/10000, Training Loss: 0.6760470867156982, Validation Loss: 1.013074517250061\n",
      "Epoch 6148/10000, Training Loss: 0.6677569150924683, Validation Loss: 0.6843602061271667\n",
      "Epoch 6149/10000, Training Loss: 0.6507512331008911, Validation Loss: 0.3866492509841919\n",
      "Epoch 6150/10000, Training Loss: 0.6580307483673096, Validation Loss: 1.3848156929016113\n",
      "Epoch 6151/10000, Training Loss: 0.7031141519546509, Validation Loss: 0.7401408553123474\n",
      "Epoch 6152/10000, Training Loss: 0.6667322516441345, Validation Loss: 0.8053484559059143\n",
      "Epoch 6153/10000, Training Loss: 0.7168468832969666, Validation Loss: 1.1332459449768066\n",
      "Epoch 6154/10000, Training Loss: 0.7127966284751892, Validation Loss: 0.6332669854164124\n",
      "Epoch 6155/10000, Training Loss: 0.6715885996818542, Validation Loss: 0.5892392992973328\n",
      "Epoch 6156/10000, Training Loss: 0.6773477792739868, Validation Loss: 0.6866934299468994\n",
      "Epoch 6157/10000, Training Loss: 0.7524017095565796, Validation Loss: 0.862511157989502\n",
      "Epoch 6158/10000, Training Loss: 0.6826560497283936, Validation Loss: 0.8497353196144104\n",
      "Epoch 6159/10000, Training Loss: 0.6609026193618774, Validation Loss: 0.48664435744285583\n",
      "Epoch 6160/10000, Training Loss: 0.6861578226089478, Validation Loss: 0.8580791354179382\n",
      "Epoch 6161/10000, Training Loss: 0.6495179533958435, Validation Loss: 0.6818015575408936\n",
      "Epoch 6162/10000, Training Loss: 0.7203267216682434, Validation Loss: 0.707658588886261\n",
      "Epoch 6163/10000, Training Loss: 0.6984462141990662, Validation Loss: 0.5515003204345703\n",
      "Epoch 6164/10000, Training Loss: 0.6717568635940552, Validation Loss: 0.6353057026863098\n",
      "Epoch 6165/10000, Training Loss: 0.725717306137085, Validation Loss: 0.7371272444725037\n",
      "Epoch 6166/10000, Training Loss: 0.6793093085289001, Validation Loss: 0.6609383225440979\n",
      "Epoch 6167/10000, Training Loss: 0.6484935283660889, Validation Loss: 0.7564048171043396\n",
      "Epoch 6168/10000, Training Loss: 0.6722691655158997, Validation Loss: 0.7380916476249695\n",
      "Epoch 6169/10000, Training Loss: 0.7139648795127869, Validation Loss: 0.6157277226448059\n",
      "Epoch 6170/10000, Training Loss: 0.6773583292961121, Validation Loss: 0.718442976474762\n",
      "Epoch 6171/10000, Training Loss: 0.6588258743286133, Validation Loss: 1.0336018800735474\n",
      "Epoch 6172/10000, Training Loss: 0.6799315810203552, Validation Loss: 0.8089836239814758\n",
      "Epoch 6173/10000, Training Loss: 0.6895619034767151, Validation Loss: 0.6656195521354675\n",
      "Epoch 6174/10000, Training Loss: 0.7207567691802979, Validation Loss: 0.8712186217308044\n",
      "Epoch 6175/10000, Training Loss: 0.7236140370368958, Validation Loss: 0.7864231467247009\n",
      "Epoch 6176/10000, Training Loss: 0.6462324857711792, Validation Loss: 0.7440798878669739\n",
      "Epoch 6177/10000, Training Loss: 0.6894003748893738, Validation Loss: 0.653287410736084\n",
      "Epoch 6178/10000, Training Loss: 0.6703105568885803, Validation Loss: 0.8118109107017517\n",
      "Epoch 6179/10000, Training Loss: 0.6894565224647522, Validation Loss: 0.6916612982749939\n",
      "Epoch 6180/10000, Training Loss: 0.6862488389015198, Validation Loss: 0.9102816581726074\n",
      "Epoch 6181/10000, Training Loss: 0.7286611199378967, Validation Loss: 0.6245916485786438\n",
      "Epoch 6182/10000, Training Loss: 0.7078239917755127, Validation Loss: 0.7458006739616394\n",
      "Epoch 6183/10000, Training Loss: 0.6174237728118896, Validation Loss: 0.6026368737220764\n",
      "Epoch 6184/10000, Training Loss: 0.6912925243377686, Validation Loss: 0.7640396952629089\n",
      "Epoch 6185/10000, Training Loss: 0.6394615173339844, Validation Loss: 0.5811325907707214\n",
      "Epoch 6186/10000, Training Loss: 0.6970428228378296, Validation Loss: 0.7257927060127258\n",
      "Epoch 6187/10000, Training Loss: 0.6449811458587646, Validation Loss: 0.5741021633148193\n",
      "Epoch 6188/10000, Training Loss: 0.7051233649253845, Validation Loss: 0.5702070593833923\n",
      "Epoch 6189/10000, Training Loss: 0.7224909067153931, Validation Loss: 0.5528967976570129\n",
      "Epoch 6190/10000, Training Loss: 0.6511894464492798, Validation Loss: 0.7619912624359131\n",
      "Epoch 6191/10000, Training Loss: 0.6420866250991821, Validation Loss: 0.6106144189834595\n",
      "Epoch 6192/10000, Training Loss: 0.693129301071167, Validation Loss: 0.5557316541671753\n",
      "Epoch 6193/10000, Training Loss: 0.664867639541626, Validation Loss: 0.8122727870941162\n",
      "Epoch 6194/10000, Training Loss: 0.7078808546066284, Validation Loss: 0.8093550205230713\n",
      "Epoch 6195/10000, Training Loss: 0.6606763601303101, Validation Loss: 0.8782321810722351\n",
      "Epoch 6196/10000, Training Loss: 0.6430779695510864, Validation Loss: 0.7886986136436462\n",
      "Epoch 6197/10000, Training Loss: 0.6378799080848694, Validation Loss: 0.6136923432350159\n",
      "Epoch 6198/10000, Training Loss: 0.6997759342193604, Validation Loss: 0.7213528156280518\n",
      "Epoch 6199/10000, Training Loss: 0.6876840591430664, Validation Loss: 1.0060083866119385\n",
      "Epoch 6200/10000, Training Loss: 0.6807825565338135, Validation Loss: 0.6064340472221375\n",
      "Epoch 6201/10000, Training Loss: 0.6752461791038513, Validation Loss: 0.7782480716705322\n",
      "Epoch 6202/10000, Training Loss: 0.683455228805542, Validation Loss: 0.48841592669487\n",
      "Epoch 6203/10000, Training Loss: 0.6835203766822815, Validation Loss: 0.7562741637229919\n",
      "Epoch 6204/10000, Training Loss: 0.6584593653678894, Validation Loss: 0.6658096313476562\n",
      "Epoch 6205/10000, Training Loss: 0.6695481538772583, Validation Loss: 0.7912566661834717\n",
      "Epoch 6206/10000, Training Loss: 0.7194334864616394, Validation Loss: 0.6401150226593018\n",
      "Epoch 6207/10000, Training Loss: 0.6538801193237305, Validation Loss: 0.7439203262329102\n",
      "Epoch 6208/10000, Training Loss: 0.6825433373451233, Validation Loss: 0.691303014755249\n",
      "Epoch 6209/10000, Training Loss: 0.7336291670799255, Validation Loss: 0.8664731979370117\n",
      "Epoch 6210/10000, Training Loss: 0.6613919734954834, Validation Loss: 0.7775232195854187\n",
      "Epoch 6211/10000, Training Loss: 0.6971588134765625, Validation Loss: 0.9801248908042908\n",
      "Epoch 6212/10000, Training Loss: 0.6845936179161072, Validation Loss: 0.6918619275093079\n",
      "Epoch 6213/10000, Training Loss: 0.6738834977149963, Validation Loss: 0.8219146132469177\n",
      "Epoch 6214/10000, Training Loss: 0.6392326354980469, Validation Loss: 0.8433919548988342\n",
      "Epoch 6215/10000, Training Loss: 0.6837355494499207, Validation Loss: 0.7151841521263123\n",
      "Epoch 6216/10000, Training Loss: 0.6696605086326599, Validation Loss: 0.9060327410697937\n",
      "Epoch 6217/10000, Training Loss: 0.6564798951148987, Validation Loss: 0.6573320031166077\n",
      "Epoch 6218/10000, Training Loss: 0.6515109539031982, Validation Loss: 0.5919052958488464\n",
      "Epoch 6219/10000, Training Loss: 0.6937083601951599, Validation Loss: 0.7681126594543457\n",
      "Epoch 6220/10000, Training Loss: 0.6639831066131592, Validation Loss: 0.7440576553344727\n",
      "Epoch 6221/10000, Training Loss: 0.6785845160484314, Validation Loss: 0.7213737368583679\n",
      "Epoch 6222/10000, Training Loss: 0.6920415163040161, Validation Loss: 0.7496280670166016\n",
      "Epoch 6223/10000, Training Loss: 0.6504793763160706, Validation Loss: 0.6477242708206177\n",
      "Epoch 6224/10000, Training Loss: 0.6860306262969971, Validation Loss: 0.6706894040107727\n",
      "Epoch 6225/10000, Training Loss: 0.6505091190338135, Validation Loss: 0.6124274134635925\n",
      "Epoch 6226/10000, Training Loss: 0.6668589115142822, Validation Loss: 0.8504719138145447\n",
      "Epoch 6227/10000, Training Loss: 0.6716439723968506, Validation Loss: 0.7242400646209717\n",
      "Epoch 6228/10000, Training Loss: 0.6600748896598816, Validation Loss: 0.8786378502845764\n",
      "Epoch 6229/10000, Training Loss: 0.6945082545280457, Validation Loss: 0.8987041115760803\n",
      "Epoch 6230/10000, Training Loss: 0.6636214852333069, Validation Loss: 0.7081980109214783\n",
      "Epoch 6231/10000, Training Loss: 0.7056872844696045, Validation Loss: 0.7104840874671936\n",
      "Epoch 6232/10000, Training Loss: 0.6080482006072998, Validation Loss: 0.5728378891944885\n",
      "Epoch 6233/10000, Training Loss: 0.7801408171653748, Validation Loss: 0.6113763451576233\n",
      "Epoch 6234/10000, Training Loss: 0.7064745426177979, Validation Loss: 0.6992878913879395\n",
      "Epoch 6235/10000, Training Loss: 0.6578270196914673, Validation Loss: 0.6901235580444336\n",
      "Epoch 6236/10000, Training Loss: 0.6834738254547119, Validation Loss: 1.0039254426956177\n",
      "Epoch 6237/10000, Training Loss: 0.6712583899497986, Validation Loss: 0.8412813544273376\n",
      "Epoch 6238/10000, Training Loss: 0.7121245265007019, Validation Loss: 0.8205990195274353\n",
      "Epoch 6239/10000, Training Loss: 0.6410608291625977, Validation Loss: 0.7752397656440735\n",
      "Epoch 6240/10000, Training Loss: 0.6351181864738464, Validation Loss: 0.5542251467704773\n",
      "Epoch 6241/10000, Training Loss: 0.6971369981765747, Validation Loss: 0.6719490885734558\n",
      "Epoch 6242/10000, Training Loss: 0.7360976934432983, Validation Loss: 1.4154928922653198\n",
      "Epoch 6243/10000, Training Loss: 0.7171403169631958, Validation Loss: 0.6365614533424377\n",
      "Epoch 6244/10000, Training Loss: 0.6189001798629761, Validation Loss: 0.8138508796691895\n",
      "Epoch 6245/10000, Training Loss: 0.6732999086380005, Validation Loss: 0.6173605918884277\n",
      "Epoch 6246/10000, Training Loss: 0.6483958959579468, Validation Loss: 0.8394226431846619\n",
      "Epoch 6247/10000, Training Loss: 0.7004953026771545, Validation Loss: 0.6930233836174011\n",
      "Epoch 6248/10000, Training Loss: 0.6811579465866089, Validation Loss: 0.4155171811580658\n",
      "Epoch 6249/10000, Training Loss: 0.6457780599594116, Validation Loss: 1.0099642276763916\n",
      "Epoch 6250/10000, Training Loss: 0.7160564661026001, Validation Loss: 0.5946868658065796\n",
      "Epoch 6251/10000, Training Loss: 0.6965965628623962, Validation Loss: 0.8439825177192688\n",
      "Epoch 6252/10000, Training Loss: 0.6883872747421265, Validation Loss: 0.8849727511405945\n",
      "Epoch 6253/10000, Training Loss: 0.6734591722488403, Validation Loss: 0.7093678116798401\n",
      "Epoch 6254/10000, Training Loss: 0.6653392314910889, Validation Loss: 0.9844962954521179\n",
      "Epoch 6255/10000, Training Loss: 0.6362422704696655, Validation Loss: 0.7233904004096985\n",
      "Epoch 6256/10000, Training Loss: 0.6632816791534424, Validation Loss: 0.5377448201179504\n",
      "Epoch 6257/10000, Training Loss: 0.7070074677467346, Validation Loss: 0.8690494894981384\n",
      "Epoch 6258/10000, Training Loss: 0.6940547227859497, Validation Loss: 0.7388615608215332\n",
      "Epoch 6259/10000, Training Loss: 0.6777061820030212, Validation Loss: 0.489470511674881\n",
      "Epoch 6260/10000, Training Loss: 0.7093921303749084, Validation Loss: 0.663464367389679\n",
      "Epoch 6261/10000, Training Loss: 0.6717055439949036, Validation Loss: 0.5990811586380005\n",
      "Epoch 6262/10000, Training Loss: 0.6573871374130249, Validation Loss: 0.5884110331535339\n",
      "Epoch 6263/10000, Training Loss: 0.6975828409194946, Validation Loss: 0.5772824287414551\n",
      "Epoch 6264/10000, Training Loss: 0.6158859729766846, Validation Loss: 0.6997306942939758\n",
      "Epoch 6265/10000, Training Loss: 0.6801526546478271, Validation Loss: 1.0129203796386719\n",
      "Epoch 6266/10000, Training Loss: 0.6577295064926147, Validation Loss: 0.7789359092712402\n",
      "Epoch 6267/10000, Training Loss: 0.657655656337738, Validation Loss: 0.6623811721801758\n",
      "Epoch 6268/10000, Training Loss: 0.7059768438339233, Validation Loss: 0.643937885761261\n",
      "Epoch 6269/10000, Training Loss: 0.633149266242981, Validation Loss: 0.7988433241844177\n",
      "Epoch 6270/10000, Training Loss: 0.6364209055900574, Validation Loss: 0.634200394153595\n",
      "Epoch 6271/10000, Training Loss: 0.8328338265419006, Validation Loss: 0.8493552803993225\n",
      "Epoch 6272/10000, Training Loss: 0.6779290437698364, Validation Loss: 0.5007007718086243\n",
      "Epoch 6273/10000, Training Loss: 0.6709093451499939, Validation Loss: 0.9102204442024231\n",
      "Epoch 6274/10000, Training Loss: 0.6744274497032166, Validation Loss: 0.714923083782196\n",
      "Epoch 6275/10000, Training Loss: 0.6809822916984558, Validation Loss: 0.8096087574958801\n",
      "Epoch 6276/10000, Training Loss: 0.6559045314788818, Validation Loss: 0.6082109808921814\n",
      "Epoch 6277/10000, Training Loss: 0.6306458115577698, Validation Loss: 0.6806213855743408\n",
      "Epoch 6278/10000, Training Loss: 0.7046865224838257, Validation Loss: 0.6223356127738953\n",
      "Epoch 6279/10000, Training Loss: 0.6546093821525574, Validation Loss: 0.6746466159820557\n",
      "Epoch 6280/10000, Training Loss: 0.6284645795822144, Validation Loss: 0.6110579967498779\n",
      "Epoch 6281/10000, Training Loss: 0.6560245156288147, Validation Loss: 0.8775162696838379\n",
      "Epoch 6282/10000, Training Loss: 0.6872717142105103, Validation Loss: 0.9309576153755188\n",
      "Epoch 6283/10000, Training Loss: 0.7152839303016663, Validation Loss: 0.7952591776847839\n",
      "Epoch 6284/10000, Training Loss: 0.6954348683357239, Validation Loss: 0.8122584819793701\n",
      "Epoch 6285/10000, Training Loss: 0.6490474939346313, Validation Loss: 0.574532687664032\n",
      "Epoch 6286/10000, Training Loss: 0.6805499196052551, Validation Loss: 0.7796147465705872\n",
      "Epoch 6287/10000, Training Loss: 0.6276556253433228, Validation Loss: 0.5870175957679749\n",
      "Epoch 6288/10000, Training Loss: 0.6746896505355835, Validation Loss: 0.7775548100471497\n",
      "Epoch 6289/10000, Training Loss: 0.6452367305755615, Validation Loss: 0.8573516011238098\n",
      "Epoch 6290/10000, Training Loss: 0.6465672850608826, Validation Loss: 0.506685733795166\n",
      "Epoch 6291/10000, Training Loss: 0.6645017266273499, Validation Loss: 0.6558237671852112\n",
      "Epoch 6292/10000, Training Loss: 0.650038480758667, Validation Loss: 0.6522319316864014\n",
      "Epoch 6293/10000, Training Loss: 0.700749397277832, Validation Loss: 0.7015039920806885\n",
      "Epoch 6294/10000, Training Loss: 0.6834102272987366, Validation Loss: 0.7172636985778809\n",
      "Epoch 6295/10000, Training Loss: 0.6674695611000061, Validation Loss: 0.7700478434562683\n",
      "Epoch 6296/10000, Training Loss: 0.6822752356529236, Validation Loss: 0.748180091381073\n",
      "Epoch 6297/10000, Training Loss: 0.6954560875892639, Validation Loss: 0.4290028512477875\n",
      "Epoch 6298/10000, Training Loss: 0.7194527387619019, Validation Loss: 0.605374813079834\n",
      "Epoch 6299/10000, Training Loss: 0.639082670211792, Validation Loss: 0.7372247576713562\n",
      "Epoch 6300/10000, Training Loss: 0.6348842978477478, Validation Loss: 0.6198164820671082\n",
      "Epoch 6301/10000, Training Loss: 0.6470568776130676, Validation Loss: 0.9629075527191162\n",
      "Epoch 6302/10000, Training Loss: 0.6936410069465637, Validation Loss: 0.5802411437034607\n",
      "Epoch 6303/10000, Training Loss: 0.675442099571228, Validation Loss: 0.6869295239448547\n",
      "Epoch 6304/10000, Training Loss: 0.7198079824447632, Validation Loss: 0.7014657855033875\n",
      "Epoch 6305/10000, Training Loss: 0.695734977722168, Validation Loss: 0.7900967001914978\n",
      "Epoch 6306/10000, Training Loss: 0.6536865830421448, Validation Loss: 1.1551607847213745\n",
      "Epoch 6307/10000, Training Loss: 0.6808950901031494, Validation Loss: 0.5203538537025452\n",
      "Epoch 6308/10000, Training Loss: 0.6675961017608643, Validation Loss: 0.6431137919425964\n",
      "Epoch 6309/10000, Training Loss: 0.700891375541687, Validation Loss: 0.5215991139411926\n",
      "Epoch 6310/10000, Training Loss: 0.7087981104850769, Validation Loss: 0.5956597924232483\n",
      "Epoch 6311/10000, Training Loss: 0.6844362616539001, Validation Loss: 0.7474226355552673\n",
      "Epoch 6312/10000, Training Loss: 0.7113329172134399, Validation Loss: 0.750312864780426\n",
      "Epoch 6313/10000, Training Loss: 0.6753759980201721, Validation Loss: 0.8372431397438049\n",
      "Epoch 6314/10000, Training Loss: 0.6936057806015015, Validation Loss: 0.8475697636604309\n",
      "Epoch 6315/10000, Training Loss: 0.6632153391838074, Validation Loss: 0.5631894469261169\n",
      "Epoch 6316/10000, Training Loss: 0.6434650421142578, Validation Loss: 0.7871618866920471\n",
      "Epoch 6317/10000, Training Loss: 0.6350908875465393, Validation Loss: 0.7203654646873474\n",
      "Epoch 6318/10000, Training Loss: 0.6343235373497009, Validation Loss: 0.5425736904144287\n",
      "Epoch 6319/10000, Training Loss: 0.7158218026161194, Validation Loss: 0.7947292327880859\n",
      "Epoch 6320/10000, Training Loss: 0.6727454662322998, Validation Loss: 0.6501137614250183\n",
      "Epoch 6321/10000, Training Loss: 0.673798143863678, Validation Loss: 0.658523440361023\n",
      "Epoch 6322/10000, Training Loss: 0.7005465030670166, Validation Loss: 0.6464318633079529\n",
      "Epoch 6323/10000, Training Loss: 0.6345497369766235, Validation Loss: 0.9424886107444763\n",
      "Epoch 6324/10000, Training Loss: 0.7300485372543335, Validation Loss: 0.7213837504386902\n",
      "Epoch 6325/10000, Training Loss: 0.5927892327308655, Validation Loss: 0.9326291084289551\n",
      "Epoch 6326/10000, Training Loss: 0.6871569752693176, Validation Loss: 0.6206676959991455\n",
      "Epoch 6327/10000, Training Loss: 0.6415187120437622, Validation Loss: 0.9532493948936462\n",
      "Epoch 6328/10000, Training Loss: 0.6785008907318115, Validation Loss: 0.8337482810020447\n",
      "Epoch 6329/10000, Training Loss: 0.6536929607391357, Validation Loss: 0.685870885848999\n",
      "Epoch 6330/10000, Training Loss: 0.6670051217079163, Validation Loss: 0.6370270848274231\n",
      "Epoch 6331/10000, Training Loss: 0.7312530875205994, Validation Loss: 0.7348847389221191\n",
      "Epoch 6332/10000, Training Loss: 0.6611858010292053, Validation Loss: 0.7776048183441162\n",
      "Epoch 6333/10000, Training Loss: 0.6457030773162842, Validation Loss: 0.6167919635772705\n",
      "Epoch 6334/10000, Training Loss: 0.6709004640579224, Validation Loss: 0.9666860699653625\n",
      "Epoch 6335/10000, Training Loss: 0.6663039922714233, Validation Loss: 0.6470671892166138\n",
      "Epoch 6336/10000, Training Loss: 0.6596050262451172, Validation Loss: 0.67291259765625\n",
      "Epoch 6337/10000, Training Loss: 0.6919094324111938, Validation Loss: 0.860177755355835\n",
      "Epoch 6338/10000, Training Loss: 0.6446400880813599, Validation Loss: 0.5897274613380432\n",
      "Epoch 6339/10000, Training Loss: 0.67080157995224, Validation Loss: 0.6527247428894043\n",
      "Epoch 6340/10000, Training Loss: 0.6964130997657776, Validation Loss: 0.5821696519851685\n",
      "Epoch 6341/10000, Training Loss: 0.7045763731002808, Validation Loss: 0.8713827133178711\n",
      "Epoch 6342/10000, Training Loss: 0.7046101093292236, Validation Loss: 0.8680807948112488\n",
      "Epoch 6343/10000, Training Loss: 0.650043249130249, Validation Loss: 0.6788991093635559\n",
      "Epoch 6344/10000, Training Loss: 0.6882959008216858, Validation Loss: 0.9381871819496155\n",
      "Epoch 6345/10000, Training Loss: 0.6892315149307251, Validation Loss: 0.6854738593101501\n",
      "Epoch 6346/10000, Training Loss: 0.696951150894165, Validation Loss: 0.6591362357139587\n",
      "Epoch 6347/10000, Training Loss: 0.6505417227745056, Validation Loss: 0.5855174660682678\n",
      "Epoch 6348/10000, Training Loss: 0.6832537055015564, Validation Loss: 0.5723758935928345\n",
      "Epoch 6349/10000, Training Loss: 0.6730444431304932, Validation Loss: 0.7629801630973816\n",
      "Epoch 6350/10000, Training Loss: 0.6398385763168335, Validation Loss: 0.8656237125396729\n",
      "Epoch 6351/10000, Training Loss: 0.6258646845817566, Validation Loss: 0.682108461856842\n",
      "Epoch 6352/10000, Training Loss: 0.6434678435325623, Validation Loss: 0.8782011866569519\n",
      "Epoch 6353/10000, Training Loss: 0.6536877751350403, Validation Loss: 0.7639586329460144\n",
      "Epoch 6354/10000, Training Loss: 0.6549147367477417, Validation Loss: 0.8746934533119202\n",
      "Epoch 6355/10000, Training Loss: 0.682098925113678, Validation Loss: 0.838174045085907\n",
      "Epoch 6356/10000, Training Loss: 0.7004449367523193, Validation Loss: 0.7419010996818542\n",
      "Epoch 6357/10000, Training Loss: 0.6680775880813599, Validation Loss: 0.6642722487449646\n",
      "Epoch 6358/10000, Training Loss: 0.6660547256469727, Validation Loss: 0.6277278661727905\n",
      "Epoch 6359/10000, Training Loss: 0.6612156629562378, Validation Loss: 0.4515773355960846\n",
      "Epoch 6360/10000, Training Loss: 0.6407902240753174, Validation Loss: 0.8437261581420898\n",
      "Epoch 6361/10000, Training Loss: 0.6502325534820557, Validation Loss: 0.8850472569465637\n",
      "Epoch 6362/10000, Training Loss: 0.6923899054527283, Validation Loss: 1.0361260175704956\n",
      "Epoch 6363/10000, Training Loss: 0.6223751902580261, Validation Loss: 0.737923800945282\n",
      "Epoch 6364/10000, Training Loss: 0.6738519072532654, Validation Loss: 0.8204190731048584\n",
      "Epoch 6365/10000, Training Loss: 0.7131243944168091, Validation Loss: 0.7167704701423645\n",
      "Epoch 6366/10000, Training Loss: 0.6386827230453491, Validation Loss: 0.8454098105430603\n",
      "Epoch 6367/10000, Training Loss: 0.6794925332069397, Validation Loss: 0.6708650588989258\n",
      "Epoch 6368/10000, Training Loss: 0.6641842722892761, Validation Loss: 0.9215400815010071\n",
      "Epoch 6369/10000, Training Loss: 0.6609790325164795, Validation Loss: 0.7455347180366516\n",
      "Epoch 6370/10000, Training Loss: 0.6872538924217224, Validation Loss: 0.6974033713340759\n",
      "Epoch 6371/10000, Training Loss: 0.6450919508934021, Validation Loss: 0.6202303767204285\n",
      "Epoch 6372/10000, Training Loss: 0.6468493938446045, Validation Loss: 0.736207902431488\n",
      "Epoch 6373/10000, Training Loss: 0.7014968395233154, Validation Loss: 0.5640058517456055\n",
      "Epoch 6374/10000, Training Loss: 0.7082767486572266, Validation Loss: 0.8005614876747131\n",
      "Epoch 6375/10000, Training Loss: 0.697525143623352, Validation Loss: 0.7501400113105774\n",
      "Epoch 6376/10000, Training Loss: 0.6523099541664124, Validation Loss: 0.5808084607124329\n",
      "Epoch 6377/10000, Training Loss: 0.6798502802848816, Validation Loss: 0.5650253891944885\n",
      "Epoch 6378/10000, Training Loss: 0.7175170183181763, Validation Loss: 0.797809362411499\n",
      "Epoch 6379/10000, Training Loss: 0.6535921096801758, Validation Loss: 0.650246798992157\n",
      "Epoch 6380/10000, Training Loss: 0.6625270247459412, Validation Loss: 0.8476521372795105\n",
      "Epoch 6381/10000, Training Loss: 0.610550045967102, Validation Loss: 0.8104419112205505\n",
      "Epoch 6382/10000, Training Loss: 0.6500853300094604, Validation Loss: 0.9966958165168762\n",
      "Epoch 6383/10000, Training Loss: 0.6918968558311462, Validation Loss: 0.6492913365364075\n",
      "Epoch 6384/10000, Training Loss: 0.6682628989219666, Validation Loss: 0.6458714604377747\n",
      "Epoch 6385/10000, Training Loss: 0.7058786153793335, Validation Loss: 1.035177230834961\n",
      "Epoch 6386/10000, Training Loss: 0.6549227833747864, Validation Loss: 0.8716344237327576\n",
      "Epoch 6387/10000, Training Loss: 0.6661058664321899, Validation Loss: 0.7071714401245117\n",
      "Epoch 6388/10000, Training Loss: 0.6524387001991272, Validation Loss: 0.6469430327415466\n",
      "Epoch 6389/10000, Training Loss: 0.6928606629371643, Validation Loss: 0.7254332900047302\n",
      "Epoch 6390/10000, Training Loss: 0.702886164188385, Validation Loss: 0.8152332305908203\n",
      "Epoch 6391/10000, Training Loss: 0.6986096501350403, Validation Loss: 0.7602474689483643\n",
      "Epoch 6392/10000, Training Loss: 0.6626265048980713, Validation Loss: 0.7437347769737244\n",
      "Epoch 6393/10000, Training Loss: 0.6287370324134827, Validation Loss: 0.97981858253479\n",
      "Epoch 6394/10000, Training Loss: 0.6543152332305908, Validation Loss: 0.8835833668708801\n",
      "Epoch 6395/10000, Training Loss: 0.6802935004234314, Validation Loss: 0.7857235074043274\n",
      "Epoch 6396/10000, Training Loss: 0.7022648453712463, Validation Loss: 0.7153093218803406\n",
      "Epoch 6397/10000, Training Loss: 0.6279140710830688, Validation Loss: 0.5731031894683838\n",
      "Epoch 6398/10000, Training Loss: 0.6496687531471252, Validation Loss: 0.7587217688560486\n",
      "Epoch 6399/10000, Training Loss: 0.6763496398925781, Validation Loss: 0.7889924645423889\n",
      "Epoch 6400/10000, Training Loss: 0.6599699258804321, Validation Loss: 0.8577790856361389\n",
      "Epoch 6401/10000, Training Loss: 0.6711908578872681, Validation Loss: 0.8166061043739319\n",
      "Epoch 6402/10000, Training Loss: 0.664544403553009, Validation Loss: 0.6534721255302429\n",
      "Epoch 6403/10000, Training Loss: 0.682628333568573, Validation Loss: 0.668898344039917\n",
      "Epoch 6404/10000, Training Loss: 0.7300423979759216, Validation Loss: 0.7107701897621155\n",
      "Epoch 6405/10000, Training Loss: 0.6321776509284973, Validation Loss: 0.6392277479171753\n",
      "Epoch 6406/10000, Training Loss: 0.7065492868423462, Validation Loss: 0.7220476269721985\n",
      "Epoch 6407/10000, Training Loss: 0.6752541661262512, Validation Loss: 0.686237096786499\n",
      "Epoch 6408/10000, Training Loss: 0.6839634776115417, Validation Loss: 0.708620548248291\n",
      "Epoch 6409/10000, Training Loss: 0.6731612086296082, Validation Loss: 0.6101325154304504\n",
      "Epoch 6410/10000, Training Loss: 0.7565790414810181, Validation Loss: 0.4338032305240631\n",
      "Epoch 6411/10000, Training Loss: 0.6546099781990051, Validation Loss: 0.7990005612373352\n",
      "Epoch 6412/10000, Training Loss: 0.7683095932006836, Validation Loss: 0.6925714612007141\n",
      "Epoch 6413/10000, Training Loss: 0.6643921732902527, Validation Loss: 0.690369188785553\n",
      "Epoch 6414/10000, Training Loss: 0.6524803638458252, Validation Loss: 0.836632490158081\n",
      "Epoch 6415/10000, Training Loss: 0.677704393863678, Validation Loss: 0.7437734603881836\n",
      "Epoch 6416/10000, Training Loss: 0.7052252888679504, Validation Loss: 0.7361112236976624\n",
      "Epoch 6417/10000, Training Loss: 0.6535882353782654, Validation Loss: 0.8198227286338806\n",
      "Epoch 6418/10000, Training Loss: 0.6981988549232483, Validation Loss: 0.7387736439704895\n",
      "Epoch 6419/10000, Training Loss: 0.654746413230896, Validation Loss: 0.720892608165741\n",
      "Epoch 6420/10000, Training Loss: 0.7036465406417847, Validation Loss: 0.8575976490974426\n",
      "Epoch 6421/10000, Training Loss: 0.6988506317138672, Validation Loss: 0.750024139881134\n",
      "Epoch 6422/10000, Training Loss: 0.6306294798851013, Validation Loss: 0.9704790115356445\n",
      "Epoch 6423/10000, Training Loss: 0.6429818868637085, Validation Loss: 0.637890100479126\n",
      "Epoch 6424/10000, Training Loss: 0.6475206017494202, Validation Loss: 0.8722304701805115\n",
      "Epoch 6425/10000, Training Loss: 0.6493683457374573, Validation Loss: 0.8062668442726135\n",
      "Epoch 6426/10000, Training Loss: 0.6609089374542236, Validation Loss: 0.6769616603851318\n",
      "Epoch 6427/10000, Training Loss: 0.6580170392990112, Validation Loss: 0.7220687866210938\n",
      "Epoch 6428/10000, Training Loss: 0.6815186738967896, Validation Loss: 0.8540597558021545\n",
      "Epoch 6429/10000, Training Loss: 0.6704444885253906, Validation Loss: 0.4976635277271271\n",
      "Epoch 6430/10000, Training Loss: 0.7188702821731567, Validation Loss: 0.7464027404785156\n",
      "Epoch 6431/10000, Training Loss: 0.6635912656784058, Validation Loss: 0.8108852505683899\n",
      "Epoch 6432/10000, Training Loss: 0.6405563354492188, Validation Loss: 0.6959927678108215\n",
      "Epoch 6433/10000, Training Loss: 0.6604401469230652, Validation Loss: 0.8536109328269958\n",
      "Epoch 6434/10000, Training Loss: 0.650366485118866, Validation Loss: 0.7023193836212158\n",
      "Epoch 6435/10000, Training Loss: 0.6430289149284363, Validation Loss: 0.6348083019256592\n",
      "Epoch 6436/10000, Training Loss: 0.6954627633094788, Validation Loss: 0.6385121941566467\n",
      "Epoch 6437/10000, Training Loss: 0.6519575119018555, Validation Loss: 0.7102329134941101\n",
      "Epoch 6438/10000, Training Loss: 0.6528962850570679, Validation Loss: 0.7003520131111145\n",
      "Epoch 6439/10000, Training Loss: 0.6888501644134521, Validation Loss: 0.7376754879951477\n",
      "Epoch 6440/10000, Training Loss: 0.6895247101783752, Validation Loss: 0.8107843995094299\n",
      "Epoch 6441/10000, Training Loss: 0.6671270728111267, Validation Loss: 0.7350820899009705\n",
      "Epoch 6442/10000, Training Loss: 0.6987332701683044, Validation Loss: 0.6617467999458313\n",
      "Epoch 6443/10000, Training Loss: 0.6449635028839111, Validation Loss: 0.7814202308654785\n",
      "Epoch 6444/10000, Training Loss: 0.6766479015350342, Validation Loss: 0.9940813183784485\n",
      "Epoch 6445/10000, Training Loss: 0.6445073485374451, Validation Loss: 0.7344509959220886\n",
      "Epoch 6446/10000, Training Loss: 0.6624532341957092, Validation Loss: 0.5814395546913147\n",
      "Epoch 6447/10000, Training Loss: 0.6959441304206848, Validation Loss: 0.8125297427177429\n",
      "Epoch 6448/10000, Training Loss: 0.6779933571815491, Validation Loss: 0.6360929012298584\n",
      "Epoch 6449/10000, Training Loss: 0.6647452712059021, Validation Loss: 0.6165031790733337\n",
      "Epoch 6450/10000, Training Loss: 0.6689594984054565, Validation Loss: 0.7007095813751221\n",
      "Epoch 6451/10000, Training Loss: 0.658416748046875, Validation Loss: 0.9603148102760315\n",
      "Epoch 6452/10000, Training Loss: 0.6740190982818604, Validation Loss: 0.8331944346427917\n",
      "Epoch 6453/10000, Training Loss: 0.6658886671066284, Validation Loss: 0.8359191417694092\n",
      "Epoch 6454/10000, Training Loss: 0.6730313897132874, Validation Loss: 0.7509631514549255\n",
      "Epoch 6455/10000, Training Loss: 0.7023382186889648, Validation Loss: 0.5939226746559143\n",
      "Epoch 6456/10000, Training Loss: 0.7169719934463501, Validation Loss: 0.774655818939209\n",
      "Epoch 6457/10000, Training Loss: 0.6695057153701782, Validation Loss: 0.8537397980690002\n",
      "Epoch 6458/10000, Training Loss: 0.674552321434021, Validation Loss: 0.7535789012908936\n",
      "Epoch 6459/10000, Training Loss: 0.6773958206176758, Validation Loss: 0.5725782513618469\n",
      "Epoch 6460/10000, Training Loss: 0.6590153574943542, Validation Loss: 0.8445824980735779\n",
      "Epoch 6461/10000, Training Loss: 0.7591326832771301, Validation Loss: 0.8391736149787903\n",
      "Epoch 6462/10000, Training Loss: 0.6778625249862671, Validation Loss: 0.6709935069084167\n",
      "Epoch 6463/10000, Training Loss: 0.6800715327262878, Validation Loss: 0.6179508566856384\n",
      "Epoch 6464/10000, Training Loss: 0.7567927837371826, Validation Loss: 0.656492292881012\n",
      "Epoch 6465/10000, Training Loss: 0.6599956154823303, Validation Loss: 0.5731393694877625\n",
      "Epoch 6466/10000, Training Loss: 0.6416996717453003, Validation Loss: 0.7940363883972168\n",
      "Epoch 6467/10000, Training Loss: 0.617192268371582, Validation Loss: 0.6295561194419861\n",
      "Epoch 6468/10000, Training Loss: 0.666463315486908, Validation Loss: 0.7409979701042175\n",
      "Epoch 6469/10000, Training Loss: 0.6920841932296753, Validation Loss: 0.5083746910095215\n",
      "Epoch 6470/10000, Training Loss: 0.6742426156997681, Validation Loss: 1.1111589670181274\n",
      "Epoch 6471/10000, Training Loss: 0.71539306640625, Validation Loss: 0.540184736251831\n",
      "Epoch 6472/10000, Training Loss: 0.7014189958572388, Validation Loss: 0.7664864659309387\n",
      "Epoch 6473/10000, Training Loss: 0.6871346235275269, Validation Loss: 0.9893357157707214\n",
      "Epoch 6474/10000, Training Loss: 0.6775474548339844, Validation Loss: 0.7686040997505188\n",
      "Epoch 6475/10000, Training Loss: 0.6597621440887451, Validation Loss: 0.5973078608512878\n",
      "Epoch 6476/10000, Training Loss: 0.6803265810012817, Validation Loss: 0.762222945690155\n",
      "Epoch 6477/10000, Training Loss: 0.6952364444732666, Validation Loss: 0.628769040107727\n",
      "Epoch 6478/10000, Training Loss: 0.7155267000198364, Validation Loss: 0.9340155720710754\n",
      "Epoch 6479/10000, Training Loss: 0.6878006458282471, Validation Loss: 0.6661441922187805\n",
      "Epoch 6480/10000, Training Loss: 0.627234697341919, Validation Loss: 0.7508253455162048\n",
      "Epoch 6481/10000, Training Loss: 0.7216762900352478, Validation Loss: 0.9291766285896301\n",
      "Epoch 6482/10000, Training Loss: 0.6509844660758972, Validation Loss: 0.8582955002784729\n",
      "Epoch 6483/10000, Training Loss: 0.6808556914329529, Validation Loss: 0.6183788180351257\n",
      "Epoch 6484/10000, Training Loss: 0.6699408888816833, Validation Loss: 0.8533270955085754\n",
      "Epoch 6485/10000, Training Loss: 0.6559937000274658, Validation Loss: 0.7314586639404297\n",
      "Epoch 6486/10000, Training Loss: 0.6815304160118103, Validation Loss: 0.6596599221229553\n",
      "Epoch 6487/10000, Training Loss: 0.7003641724586487, Validation Loss: 0.657438337802887\n",
      "Epoch 6488/10000, Training Loss: 0.680619478225708, Validation Loss: 0.6335881352424622\n",
      "Epoch 6489/10000, Training Loss: 0.6400734782218933, Validation Loss: 0.8464112877845764\n",
      "Epoch 6490/10000, Training Loss: 0.6540930271148682, Validation Loss: 0.9232928156852722\n",
      "Epoch 6491/10000, Training Loss: 0.6939878463745117, Validation Loss: 0.6178224682807922\n",
      "Epoch 6492/10000, Training Loss: 0.6439000368118286, Validation Loss: 0.9813433289527893\n",
      "Epoch 6493/10000, Training Loss: 0.649079442024231, Validation Loss: 0.7642106413841248\n",
      "Epoch 6494/10000, Training Loss: 0.6459232568740845, Validation Loss: 0.7232070565223694\n",
      "Epoch 6495/10000, Training Loss: 0.7046734690666199, Validation Loss: 0.6086129546165466\n",
      "Epoch 6496/10000, Training Loss: 0.6896792054176331, Validation Loss: 0.9240586757659912\n",
      "Epoch 6497/10000, Training Loss: 0.6544507741928101, Validation Loss: 0.7124524712562561\n",
      "Epoch 6498/10000, Training Loss: 0.6905362010002136, Validation Loss: 0.7559047341346741\n",
      "Epoch 6499/10000, Training Loss: 0.637624204158783, Validation Loss: 0.6004266142845154\n",
      "Epoch 6500/10000, Training Loss: 0.7378929853439331, Validation Loss: 0.6737893223762512\n",
      "Epoch 6501/10000, Training Loss: 0.678101122379303, Validation Loss: 0.7127041816711426\n",
      "Epoch 6502/10000, Training Loss: 0.674211859703064, Validation Loss: 0.6902956962585449\n",
      "Epoch 6503/10000, Training Loss: 0.6482760906219482, Validation Loss: 0.6230361461639404\n",
      "Epoch 6504/10000, Training Loss: 0.6689801812171936, Validation Loss: 0.7877471446990967\n",
      "Epoch 6505/10000, Training Loss: 0.6577179431915283, Validation Loss: 0.7287485003471375\n",
      "Epoch 6506/10000, Training Loss: 0.6802419424057007, Validation Loss: 0.740182638168335\n",
      "Epoch 6507/10000, Training Loss: 0.6845335364341736, Validation Loss: 0.7181923985481262\n",
      "Epoch 6508/10000, Training Loss: 0.7075610756874084, Validation Loss: 0.8612298965454102\n",
      "Epoch 6509/10000, Training Loss: 0.6670456528663635, Validation Loss: 1.100082278251648\n",
      "Epoch 6510/10000, Training Loss: 0.6915342807769775, Validation Loss: 0.5582830309867859\n",
      "Epoch 6511/10000, Training Loss: 0.6942052245140076, Validation Loss: 0.7337439060211182\n",
      "Epoch 6512/10000, Training Loss: 0.6721686124801636, Validation Loss: 0.6496273279190063\n",
      "Epoch 6513/10000, Training Loss: 0.6829921007156372, Validation Loss: 0.5950650572776794\n",
      "Epoch 6514/10000, Training Loss: 0.59944748878479, Validation Loss: 0.5182145237922668\n",
      "Epoch 6515/10000, Training Loss: 0.6949476003646851, Validation Loss: 0.6922158598899841\n",
      "Epoch 6516/10000, Training Loss: 0.6612997651100159, Validation Loss: 0.6606823801994324\n",
      "Epoch 6517/10000, Training Loss: 0.6334790587425232, Validation Loss: 0.585429847240448\n",
      "Epoch 6518/10000, Training Loss: 0.6509897708892822, Validation Loss: 0.6317172050476074\n",
      "Epoch 6519/10000, Training Loss: 0.6621500849723816, Validation Loss: 0.6598370671272278\n",
      "Epoch 6520/10000, Training Loss: 0.6756883859634399, Validation Loss: 0.6827053427696228\n",
      "Epoch 6521/10000, Training Loss: 0.6335999965667725, Validation Loss: 0.6613942980766296\n",
      "Epoch 6522/10000, Training Loss: 0.7166233062744141, Validation Loss: 0.9779875874519348\n",
      "Epoch 6523/10000, Training Loss: 0.6674191355705261, Validation Loss: 0.6026263236999512\n",
      "Epoch 6524/10000, Training Loss: 0.6802045106887817, Validation Loss: 0.689866304397583\n",
      "Epoch 6525/10000, Training Loss: 0.6463024616241455, Validation Loss: 0.6618964076042175\n",
      "Epoch 6526/10000, Training Loss: 0.6445124745368958, Validation Loss: 0.7689992785453796\n",
      "Epoch 6527/10000, Training Loss: 0.6574790477752686, Validation Loss: 0.839759349822998\n",
      "Epoch 6528/10000, Training Loss: 0.7015368342399597, Validation Loss: 0.4741603434085846\n",
      "Epoch 6529/10000, Training Loss: 0.7062994837760925, Validation Loss: 0.9397595524787903\n",
      "Epoch 6530/10000, Training Loss: 0.6569420695304871, Validation Loss: 0.7316626906394958\n",
      "Epoch 6531/10000, Training Loss: 0.6817614436149597, Validation Loss: 0.8036409020423889\n",
      "Epoch 6532/10000, Training Loss: 0.6968882083892822, Validation Loss: 0.7531968951225281\n",
      "Epoch 6533/10000, Training Loss: 0.7072809934616089, Validation Loss: 0.8117830157279968\n",
      "Epoch 6534/10000, Training Loss: 0.6961859464645386, Validation Loss: 0.923460066318512\n",
      "Epoch 6535/10000, Training Loss: 0.6619715094566345, Validation Loss: 0.6842792630195618\n",
      "Epoch 6536/10000, Training Loss: 0.6609121561050415, Validation Loss: 0.6633785963058472\n",
      "Epoch 6537/10000, Training Loss: 0.7042573094367981, Validation Loss: 0.6852676868438721\n",
      "Epoch 6538/10000, Training Loss: 0.6846131086349487, Validation Loss: 0.6809206008911133\n",
      "Epoch 6539/10000, Training Loss: 0.7025147676467896, Validation Loss: 0.689582109451294\n",
      "Epoch 6540/10000, Training Loss: 0.6854971647262573, Validation Loss: 0.8193306922912598\n",
      "Epoch 6541/10000, Training Loss: 0.6839345693588257, Validation Loss: 0.7926707863807678\n",
      "Epoch 6542/10000, Training Loss: 0.644144594669342, Validation Loss: 0.668961763381958\n",
      "Epoch 6543/10000, Training Loss: 0.6591742634773254, Validation Loss: 0.9041693806648254\n",
      "Epoch 6544/10000, Training Loss: 0.6790046691894531, Validation Loss: 0.7164923548698425\n",
      "Epoch 6545/10000, Training Loss: 0.7245701551437378, Validation Loss: 0.7236394882202148\n",
      "Epoch 6546/10000, Training Loss: 0.7084254622459412, Validation Loss: 0.7915935516357422\n",
      "Epoch 6547/10000, Training Loss: 0.6544685959815979, Validation Loss: 0.6262651085853577\n",
      "Epoch 6548/10000, Training Loss: 0.6646366715431213, Validation Loss: 0.8316639065742493\n",
      "Epoch 6549/10000, Training Loss: 0.6678285002708435, Validation Loss: 0.7339733242988586\n",
      "Epoch 6550/10000, Training Loss: 0.6509097814559937, Validation Loss: 0.6950517296791077\n",
      "Epoch 6551/10000, Training Loss: 0.686273455619812, Validation Loss: 0.9126956462860107\n",
      "Epoch 6552/10000, Training Loss: 0.653999388217926, Validation Loss: 1.0703034400939941\n",
      "Epoch 6553/10000, Training Loss: 0.6684733629226685, Validation Loss: 0.8108696341514587\n",
      "Epoch 6554/10000, Training Loss: 0.6830918788909912, Validation Loss: 0.5403802990913391\n",
      "Epoch 6555/10000, Training Loss: 0.6731640100479126, Validation Loss: 0.7299375534057617\n",
      "Epoch 6556/10000, Training Loss: 0.6312644481658936, Validation Loss: 0.4952205419540405\n",
      "Epoch 6557/10000, Training Loss: 0.6835123300552368, Validation Loss: 0.7274002432823181\n",
      "Epoch 6558/10000, Training Loss: 0.6761108040809631, Validation Loss: 0.6795929074287415\n",
      "Epoch 6559/10000, Training Loss: 0.6696438193321228, Validation Loss: 0.7952650189399719\n",
      "Epoch 6560/10000, Training Loss: 0.6889145374298096, Validation Loss: 0.6355760097503662\n",
      "Epoch 6561/10000, Training Loss: 0.6613212823867798, Validation Loss: 0.6493269205093384\n",
      "Epoch 6562/10000, Training Loss: 0.6504681706428528, Validation Loss: 0.7517834305763245\n",
      "Epoch 6563/10000, Training Loss: 0.6472082138061523, Validation Loss: 0.5262615084648132\n",
      "Epoch 6564/10000, Training Loss: 0.6306370496749878, Validation Loss: 0.5750148892402649\n",
      "Epoch 6565/10000, Training Loss: 0.6626441478729248, Validation Loss: 0.6902249455451965\n",
      "Epoch 6566/10000, Training Loss: 0.6736043095588684, Validation Loss: 0.667008101940155\n",
      "Epoch 6567/10000, Training Loss: 0.6553856134414673, Validation Loss: 0.9851899147033691\n",
      "Epoch 6568/10000, Training Loss: 0.7017574906349182, Validation Loss: 0.8247668147087097\n",
      "Epoch 6569/10000, Training Loss: 0.6423991322517395, Validation Loss: 0.9441640377044678\n",
      "Epoch 6570/10000, Training Loss: 0.6610317230224609, Validation Loss: 0.9428303241729736\n",
      "Epoch 6571/10000, Training Loss: 0.691626250743866, Validation Loss: 0.7617483139038086\n",
      "Epoch 6572/10000, Training Loss: 0.6801906824111938, Validation Loss: 0.6849706768989563\n",
      "Epoch 6573/10000, Training Loss: 0.6379953026771545, Validation Loss: 0.8093494772911072\n",
      "Epoch 6574/10000, Training Loss: 0.6905335783958435, Validation Loss: 0.5876958966255188\n",
      "Epoch 6575/10000, Training Loss: 0.6627428531646729, Validation Loss: 0.8636186122894287\n",
      "Epoch 6576/10000, Training Loss: 0.6809298992156982, Validation Loss: 0.8008224368095398\n",
      "Epoch 6577/10000, Training Loss: 0.658966064453125, Validation Loss: 0.6539928317070007\n",
      "Epoch 6578/10000, Training Loss: 0.6781768798828125, Validation Loss: 0.6560201048851013\n",
      "Epoch 6579/10000, Training Loss: 0.6825472712516785, Validation Loss: 0.6954507231712341\n",
      "Epoch 6580/10000, Training Loss: 0.6234463453292847, Validation Loss: 0.7822582125663757\n",
      "Epoch 6581/10000, Training Loss: 0.7190605401992798, Validation Loss: 0.6835591793060303\n",
      "Epoch 6582/10000, Training Loss: 0.672717809677124, Validation Loss: 0.7283610701560974\n",
      "Epoch 6583/10000, Training Loss: 0.6253148317337036, Validation Loss: 0.6492601037025452\n",
      "Epoch 6584/10000, Training Loss: 0.654758095741272, Validation Loss: 0.6214972138404846\n",
      "Epoch 6585/10000, Training Loss: 0.6554192900657654, Validation Loss: 0.9080504775047302\n",
      "Epoch 6586/10000, Training Loss: 0.7183045744895935, Validation Loss: 0.6715131402015686\n",
      "Epoch 6587/10000, Training Loss: 0.6576858162879944, Validation Loss: 0.7693842053413391\n",
      "Epoch 6588/10000, Training Loss: 0.6246505379676819, Validation Loss: 0.8505129218101501\n",
      "Epoch 6589/10000, Training Loss: 0.7045742869377136, Validation Loss: 0.7374504208564758\n",
      "Epoch 6590/10000, Training Loss: 0.6415493488311768, Validation Loss: 0.7960364818572998\n",
      "Epoch 6591/10000, Training Loss: 0.6855566501617432, Validation Loss: 0.8677946925163269\n",
      "Epoch 6592/10000, Training Loss: 0.6570771932601929, Validation Loss: 0.6926434636116028\n",
      "Epoch 6593/10000, Training Loss: 0.653751015663147, Validation Loss: 0.9600794911384583\n",
      "Epoch 6594/10000, Training Loss: 0.6721904277801514, Validation Loss: 0.9325826168060303\n",
      "Epoch 6595/10000, Training Loss: 0.6560506224632263, Validation Loss: 0.6352663040161133\n",
      "Epoch 6596/10000, Training Loss: 0.6886147260665894, Validation Loss: 0.7689580917358398\n",
      "Epoch 6597/10000, Training Loss: 0.6737681031227112, Validation Loss: 0.8241171836853027\n",
      "Epoch 6598/10000, Training Loss: 0.6320399641990662, Validation Loss: 0.5115622878074646\n",
      "Epoch 6599/10000, Training Loss: 0.6202001571655273, Validation Loss: 0.7269639372825623\n",
      "Epoch 6600/10000, Training Loss: 0.6829543709754944, Validation Loss: 0.6417028307914734\n",
      "Epoch 6601/10000, Training Loss: 0.6556444764137268, Validation Loss: 0.5324041247367859\n",
      "Epoch 6602/10000, Training Loss: 0.6508079171180725, Validation Loss: 0.6698741912841797\n",
      "Epoch 6603/10000, Training Loss: 0.6733070015907288, Validation Loss: 1.0662124156951904\n",
      "Epoch 6604/10000, Training Loss: 0.6357638835906982, Validation Loss: 0.675783634185791\n",
      "Epoch 6605/10000, Training Loss: 0.6386966705322266, Validation Loss: 0.7608377933502197\n",
      "Epoch 6606/10000, Training Loss: 0.6666701436042786, Validation Loss: 0.931434690952301\n",
      "Epoch 6607/10000, Training Loss: 0.6469758749008179, Validation Loss: 0.703711986541748\n",
      "Epoch 6608/10000, Training Loss: 0.6667606830596924, Validation Loss: 0.5468061566352844\n",
      "Epoch 6609/10000, Training Loss: 0.658937394618988, Validation Loss: 0.6123520135879517\n",
      "Epoch 6610/10000, Training Loss: 0.6683680415153503, Validation Loss: 0.6514455676078796\n",
      "Epoch 6611/10000, Training Loss: 0.663040280342102, Validation Loss: 0.7850819230079651\n",
      "Epoch 6612/10000, Training Loss: 0.6648223400115967, Validation Loss: 0.7321782112121582\n",
      "Epoch 6613/10000, Training Loss: 0.6957524418830872, Validation Loss: 0.7532078623771667\n",
      "Epoch 6614/10000, Training Loss: 0.6242058873176575, Validation Loss: 0.5878953337669373\n",
      "Epoch 6615/10000, Training Loss: 0.6901407241821289, Validation Loss: 0.822638988494873\n",
      "Epoch 6616/10000, Training Loss: 0.6551735401153564, Validation Loss: 0.7128040194511414\n",
      "Epoch 6617/10000, Training Loss: 0.6723434925079346, Validation Loss: 0.6505253314971924\n",
      "Epoch 6618/10000, Training Loss: 0.6457664966583252, Validation Loss: 0.869745671749115\n",
      "Epoch 6619/10000, Training Loss: 0.6395222544670105, Validation Loss: 0.7702096104621887\n",
      "Epoch 6620/10000, Training Loss: 0.6655768156051636, Validation Loss: 0.6295287609100342\n",
      "Epoch 6621/10000, Training Loss: 0.7375237941741943, Validation Loss: 0.7879475951194763\n",
      "Epoch 6622/10000, Training Loss: 0.6404635906219482, Validation Loss: 0.7379381656646729\n",
      "Epoch 6623/10000, Training Loss: 0.6902885437011719, Validation Loss: 0.7109569907188416\n",
      "Epoch 6624/10000, Training Loss: 0.6702021956443787, Validation Loss: 0.6448261141777039\n",
      "Epoch 6625/10000, Training Loss: 0.6646289229393005, Validation Loss: 0.6701683402061462\n",
      "Epoch 6626/10000, Training Loss: 0.6435127854347229, Validation Loss: 0.9154568314552307\n",
      "Epoch 6627/10000, Training Loss: 0.6415235996246338, Validation Loss: 0.6716580986976624\n",
      "Epoch 6628/10000, Training Loss: 0.7256969809532166, Validation Loss: 0.8642077445983887\n",
      "Epoch 6629/10000, Training Loss: 0.6471003293991089, Validation Loss: 0.816722571849823\n",
      "Epoch 6630/10000, Training Loss: 0.6640777587890625, Validation Loss: 0.9965301156044006\n",
      "Epoch 6631/10000, Training Loss: 0.6526755690574646, Validation Loss: 0.8083634972572327\n",
      "Epoch 6632/10000, Training Loss: 0.6973950266838074, Validation Loss: 0.8394109606742859\n",
      "Epoch 6633/10000, Training Loss: 0.6864324808120728, Validation Loss: 0.6122863292694092\n",
      "Epoch 6634/10000, Training Loss: 0.6672555208206177, Validation Loss: 0.6677297949790955\n",
      "Epoch 6635/10000, Training Loss: 0.6999291777610779, Validation Loss: 0.7701897621154785\n",
      "Epoch 6636/10000, Training Loss: 0.6711359620094299, Validation Loss: 0.7030194401741028\n",
      "Epoch 6637/10000, Training Loss: 0.6648722290992737, Validation Loss: 0.8074212074279785\n",
      "Epoch 6638/10000, Training Loss: 0.7039108872413635, Validation Loss: 0.6530771851539612\n",
      "Epoch 6639/10000, Training Loss: 0.7469071745872498, Validation Loss: 0.7793416380882263\n",
      "Epoch 6640/10000, Training Loss: 0.6431968212127686, Validation Loss: 0.7457250952720642\n",
      "Epoch 6641/10000, Training Loss: 0.6568813920021057, Validation Loss: 0.9223744869232178\n",
      "Epoch 6642/10000, Training Loss: 0.6730320453643799, Validation Loss: 1.0905038118362427\n",
      "Epoch 6643/10000, Training Loss: 0.7051391005516052, Validation Loss: 0.7105628848075867\n",
      "Epoch 6644/10000, Training Loss: 0.6327641010284424, Validation Loss: 0.7524323463439941\n",
      "Epoch 6645/10000, Training Loss: 0.6600673198699951, Validation Loss: 0.7191285490989685\n",
      "Epoch 6646/10000, Training Loss: 0.6489855051040649, Validation Loss: 0.7496381402015686\n",
      "Epoch 6647/10000, Training Loss: 0.675142228603363, Validation Loss: 0.7309525012969971\n",
      "Epoch 6648/10000, Training Loss: 0.6313049793243408, Validation Loss: 0.6305451393127441\n",
      "Epoch 6649/10000, Training Loss: 0.6447038650512695, Validation Loss: 0.6513146758079529\n",
      "Epoch 6650/10000, Training Loss: 0.6571041345596313, Validation Loss: 0.5292921662330627\n",
      "Epoch 6651/10000, Training Loss: 0.6514657139778137, Validation Loss: 0.8761389851570129\n",
      "Epoch 6652/10000, Training Loss: 0.6360090970993042, Validation Loss: 0.7416672706604004\n",
      "Epoch 6653/10000, Training Loss: 0.6584240198135376, Validation Loss: 0.7224152684211731\n",
      "Epoch 6654/10000, Training Loss: 0.7017164826393127, Validation Loss: 0.780860185623169\n",
      "Epoch 6655/10000, Training Loss: 0.6853135824203491, Validation Loss: 0.7233803868293762\n",
      "Epoch 6656/10000, Training Loss: 0.6663881540298462, Validation Loss: 0.6217126846313477\n",
      "Epoch 6657/10000, Training Loss: 0.6673727035522461, Validation Loss: 0.9167923331260681\n",
      "Epoch 6658/10000, Training Loss: 0.6015498638153076, Validation Loss: 0.5055775046348572\n",
      "Epoch 6659/10000, Training Loss: 0.6715031266212463, Validation Loss: 0.638631284236908\n",
      "Epoch 6660/10000, Training Loss: 0.6629589796066284, Validation Loss: 0.7984726428985596\n",
      "Epoch 6661/10000, Training Loss: 0.6645948886871338, Validation Loss: 0.9466471672058105\n",
      "Epoch 6662/10000, Training Loss: 0.6426790952682495, Validation Loss: 0.8469698429107666\n",
      "Epoch 6663/10000, Training Loss: 0.6573328375816345, Validation Loss: 0.7270522713661194\n",
      "Epoch 6664/10000, Training Loss: 0.6667935848236084, Validation Loss: 0.8525650501251221\n",
      "Epoch 6665/10000, Training Loss: 0.643742024898529, Validation Loss: 0.6018698215484619\n",
      "Epoch 6666/10000, Training Loss: 0.6757655739784241, Validation Loss: 0.716093122959137\n",
      "Epoch 6667/10000, Training Loss: 0.6542568206787109, Validation Loss: 0.6290856003761292\n",
      "Epoch 6668/10000, Training Loss: 0.701659083366394, Validation Loss: 0.8477485775947571\n",
      "Epoch 6669/10000, Training Loss: 0.6451634764671326, Validation Loss: 0.6768927574157715\n",
      "Epoch 6670/10000, Training Loss: 0.6685158610343933, Validation Loss: 0.6379051804542542\n",
      "Epoch 6671/10000, Training Loss: 0.6553294062614441, Validation Loss: 0.7935950756072998\n",
      "Epoch 6672/10000, Training Loss: 0.6613105535507202, Validation Loss: 0.8959646821022034\n",
      "Epoch 6673/10000, Training Loss: 0.6729515790939331, Validation Loss: 0.687415599822998\n",
      "Epoch 6674/10000, Training Loss: 0.6700025200843811, Validation Loss: 0.8419187664985657\n",
      "Epoch 6675/10000, Training Loss: 0.6608518958091736, Validation Loss: 0.8329609036445618\n",
      "Epoch 6676/10000, Training Loss: 0.6217098236083984, Validation Loss: 0.7388251423835754\n",
      "Epoch 6677/10000, Training Loss: 0.659372091293335, Validation Loss: 0.7143638730049133\n",
      "Epoch 6678/10000, Training Loss: 0.621619462966919, Validation Loss: 0.6743804812431335\n",
      "Epoch 6679/10000, Training Loss: 0.673262357711792, Validation Loss: 0.6519103646278381\n",
      "Epoch 6680/10000, Training Loss: 0.712955892086029, Validation Loss: 0.8544394373893738\n",
      "Epoch 6681/10000, Training Loss: 0.6589905023574829, Validation Loss: 0.6525618433952332\n",
      "Epoch 6682/10000, Training Loss: 0.6239004135131836, Validation Loss: 0.9128880500793457\n",
      "Epoch 6683/10000, Training Loss: 0.6101561784744263, Validation Loss: 0.6552029252052307\n",
      "Epoch 6684/10000, Training Loss: 0.6912190914154053, Validation Loss: 0.7388732433319092\n",
      "Epoch 6685/10000, Training Loss: 0.6845918893814087, Validation Loss: 0.9734322428703308\n",
      "Epoch 6686/10000, Training Loss: 0.6738297343254089, Validation Loss: 0.6061965823173523\n",
      "Epoch 6687/10000, Training Loss: 0.6692764759063721, Validation Loss: 0.8335699439048767\n",
      "Epoch 6688/10000, Training Loss: 0.6700382828712463, Validation Loss: 0.7173001766204834\n",
      "Epoch 6689/10000, Training Loss: 0.6117122173309326, Validation Loss: 0.7139652371406555\n",
      "Epoch 6690/10000, Training Loss: 0.68812096118927, Validation Loss: 0.8523433804512024\n",
      "Epoch 6691/10000, Training Loss: 0.6452012658119202, Validation Loss: 0.6596405506134033\n",
      "Epoch 6692/10000, Training Loss: 0.6215234398841858, Validation Loss: 0.6684191823005676\n",
      "Epoch 6693/10000, Training Loss: 0.6514497995376587, Validation Loss: 0.5387726426124573\n",
      "Epoch 6694/10000, Training Loss: 0.6428301334381104, Validation Loss: 0.5121316313743591\n",
      "Epoch 6695/10000, Training Loss: 0.6872625946998596, Validation Loss: 0.7235075831413269\n",
      "Epoch 6696/10000, Training Loss: 0.7607778310775757, Validation Loss: 0.7405164837837219\n",
      "Epoch 6697/10000, Training Loss: 0.6404181122779846, Validation Loss: 0.6979948878288269\n",
      "Epoch 6698/10000, Training Loss: 0.6507743000984192, Validation Loss: 0.6124290227890015\n",
      "Epoch 6699/10000, Training Loss: 0.6351001262664795, Validation Loss: 0.7444496750831604\n",
      "Epoch 6700/10000, Training Loss: 0.6675165891647339, Validation Loss: 0.7202394008636475\n",
      "Epoch 6701/10000, Training Loss: 0.7182988524436951, Validation Loss: 0.8019209504127502\n",
      "Epoch 6702/10000, Training Loss: 0.6807001829147339, Validation Loss: 0.6033737659454346\n",
      "Epoch 6703/10000, Training Loss: 0.6883471608161926, Validation Loss: 0.7427451014518738\n",
      "Epoch 6704/10000, Training Loss: 0.6417202353477478, Validation Loss: 0.7879207730293274\n",
      "Epoch 6705/10000, Training Loss: 0.6498984098434448, Validation Loss: 0.782047688961029\n",
      "Epoch 6706/10000, Training Loss: 0.6818047761917114, Validation Loss: 0.7980080246925354\n",
      "Epoch 6707/10000, Training Loss: 0.6984474658966064, Validation Loss: 0.8372287750244141\n",
      "Epoch 6708/10000, Training Loss: 0.6784656047821045, Validation Loss: 0.6371428370475769\n",
      "Epoch 6709/10000, Training Loss: 0.6699439287185669, Validation Loss: 0.6377069354057312\n",
      "Epoch 6710/10000, Training Loss: 0.6758157014846802, Validation Loss: 0.5614878535270691\n",
      "Epoch 6711/10000, Training Loss: 0.6024197936058044, Validation Loss: 0.8881711363792419\n",
      "Epoch 6712/10000, Training Loss: 0.6986161470413208, Validation Loss: 0.9199683666229248\n",
      "Epoch 6713/10000, Training Loss: 0.7133826613426208, Validation Loss: 0.7849984765052795\n",
      "Epoch 6714/10000, Training Loss: 0.6467450857162476, Validation Loss: 0.9459969401359558\n",
      "Epoch 6715/10000, Training Loss: 0.6914684772491455, Validation Loss: 0.731797993183136\n",
      "Epoch 6716/10000, Training Loss: 0.6653649210929871, Validation Loss: 0.6196661591529846\n",
      "Epoch 6717/10000, Training Loss: 0.6564188599586487, Validation Loss: 0.8564031720161438\n",
      "Epoch 6718/10000, Training Loss: 0.6780853867530823, Validation Loss: 0.6188588738441467\n",
      "Epoch 6719/10000, Training Loss: 0.6283101439476013, Validation Loss: 0.700517475605011\n",
      "Epoch 6720/10000, Training Loss: 0.6693045496940613, Validation Loss: 0.4959987699985504\n",
      "Epoch 6721/10000, Training Loss: 0.653714120388031, Validation Loss: 0.8208723664283752\n",
      "Epoch 6722/10000, Training Loss: 0.6440805196762085, Validation Loss: 0.6402025818824768\n",
      "Epoch 6723/10000, Training Loss: 0.6483268737792969, Validation Loss: 0.7433421015739441\n",
      "Epoch 6724/10000, Training Loss: 0.6893419027328491, Validation Loss: 0.6613767147064209\n",
      "Epoch 6725/10000, Training Loss: 0.6464850902557373, Validation Loss: 0.729478657245636\n",
      "Epoch 6726/10000, Training Loss: 0.6507639288902283, Validation Loss: 0.8012738227844238\n",
      "Epoch 6727/10000, Training Loss: 0.642776608467102, Validation Loss: 0.6535931825637817\n",
      "Epoch 6728/10000, Training Loss: 0.6667559742927551, Validation Loss: 0.665971577167511\n",
      "Epoch 6729/10000, Training Loss: 0.6816902160644531, Validation Loss: 0.6314299702644348\n",
      "Epoch 6730/10000, Training Loss: 0.6491531729698181, Validation Loss: 0.7488343119621277\n",
      "Epoch 6731/10000, Training Loss: 0.6441489458084106, Validation Loss: 0.6196341514587402\n",
      "Epoch 6732/10000, Training Loss: 0.6883385181427002, Validation Loss: 0.6458491086959839\n",
      "Epoch 6733/10000, Training Loss: 0.6789723038673401, Validation Loss: 0.9043486714363098\n",
      "Epoch 6734/10000, Training Loss: 0.6784143447875977, Validation Loss: 0.649797797203064\n",
      "Epoch 6735/10000, Training Loss: 0.6810254454612732, Validation Loss: 0.8223837018013\n",
      "Epoch 6736/10000, Training Loss: 0.6687059998512268, Validation Loss: 0.7773081660270691\n",
      "Epoch 6737/10000, Training Loss: 0.6515175700187683, Validation Loss: 0.7827395796775818\n",
      "Epoch 6738/10000, Training Loss: 0.6747316122055054, Validation Loss: 0.6987488865852356\n",
      "Epoch 6739/10000, Training Loss: 0.659813642501831, Validation Loss: 0.6371541023254395\n",
      "Epoch 6740/10000, Training Loss: 0.7126449346542358, Validation Loss: 0.7131430506706238\n",
      "Epoch 6741/10000, Training Loss: 0.6490481495857239, Validation Loss: 0.6208874583244324\n",
      "Epoch 6742/10000, Training Loss: 0.6765328049659729, Validation Loss: 0.6033228039741516\n",
      "Epoch 6743/10000, Training Loss: 0.6573343873023987, Validation Loss: 0.8146854043006897\n",
      "Epoch 6744/10000, Training Loss: 0.6497151851654053, Validation Loss: 0.8672963976860046\n",
      "Epoch 6745/10000, Training Loss: 0.6721737384796143, Validation Loss: 0.7458366751670837\n",
      "Epoch 6746/10000, Training Loss: 0.665632963180542, Validation Loss: 0.7374083399772644\n",
      "Epoch 6747/10000, Training Loss: 0.6704431176185608, Validation Loss: 0.702511727809906\n",
      "Epoch 6748/10000, Training Loss: 0.6077552437782288, Validation Loss: 0.7374276518821716\n",
      "Epoch 6749/10000, Training Loss: 0.6620371341705322, Validation Loss: 0.8190165162086487\n",
      "Epoch 6750/10000, Training Loss: 0.6080184578895569, Validation Loss: 0.7237779498100281\n",
      "Epoch 6751/10000, Training Loss: 0.6145352125167847, Validation Loss: 0.7279396057128906\n",
      "Epoch 6752/10000, Training Loss: 0.6879403591156006, Validation Loss: 0.58994060754776\n",
      "Epoch 6753/10000, Training Loss: 0.662500262260437, Validation Loss: 0.7313017249107361\n",
      "Epoch 6754/10000, Training Loss: 0.6731351613998413, Validation Loss: 0.5614194869995117\n",
      "Epoch 6755/10000, Training Loss: 0.6749137043952942, Validation Loss: 0.804792582988739\n",
      "Epoch 6756/10000, Training Loss: 0.6859902739524841, Validation Loss: 0.7062974572181702\n",
      "Epoch 6757/10000, Training Loss: 0.6520833373069763, Validation Loss: 0.6806225776672363\n",
      "Epoch 6758/10000, Training Loss: 0.6921974420547485, Validation Loss: 0.7686334252357483\n",
      "Epoch 6759/10000, Training Loss: 0.6509386301040649, Validation Loss: 0.7088131308555603\n",
      "Epoch 6760/10000, Training Loss: 0.6542141437530518, Validation Loss: 0.7333337664604187\n",
      "Epoch 6761/10000, Training Loss: 0.6697550415992737, Validation Loss: 0.607683539390564\n",
      "Epoch 6762/10000, Training Loss: 0.6279252767562866, Validation Loss: 0.6325107216835022\n",
      "Epoch 6763/10000, Training Loss: 0.6558079123497009, Validation Loss: 0.7388914227485657\n",
      "Epoch 6764/10000, Training Loss: 0.6164656281471252, Validation Loss: 0.618010938167572\n",
      "Epoch 6765/10000, Training Loss: 0.6996183395385742, Validation Loss: 0.4952838122844696\n",
      "Epoch 6766/10000, Training Loss: 0.6954594850540161, Validation Loss: 0.8378857970237732\n",
      "Epoch 6767/10000, Training Loss: 0.6601556539535522, Validation Loss: 0.6667082905769348\n",
      "Epoch 6768/10000, Training Loss: 0.642114520072937, Validation Loss: 0.7689478993415833\n",
      "Epoch 6769/10000, Training Loss: 0.6879186034202576, Validation Loss: 0.7175328135490417\n",
      "Epoch 6770/10000, Training Loss: 0.7139902710914612, Validation Loss: 0.78054279088974\n",
      "Epoch 6771/10000, Training Loss: 0.6544404625892639, Validation Loss: 0.8224338889122009\n",
      "Epoch 6772/10000, Training Loss: 0.6968912482261658, Validation Loss: 0.7622652649879456\n",
      "Epoch 6773/10000, Training Loss: 0.6516799926757812, Validation Loss: 0.7049779891967773\n",
      "Epoch 6774/10000, Training Loss: 0.6719602346420288, Validation Loss: 0.6426569819450378\n",
      "Epoch 6775/10000, Training Loss: 0.6433365345001221, Validation Loss: 0.7607707977294922\n",
      "Epoch 6776/10000, Training Loss: 0.6520437598228455, Validation Loss: 0.8385940194129944\n",
      "Epoch 6777/10000, Training Loss: 0.6832895278930664, Validation Loss: 0.7359173893928528\n",
      "Epoch 6778/10000, Training Loss: 0.6737000346183777, Validation Loss: 0.7230692505836487\n",
      "Epoch 6779/10000, Training Loss: 0.6769657731056213, Validation Loss: 0.7305096983909607\n",
      "Epoch 6780/10000, Training Loss: 0.622502326965332, Validation Loss: 0.8523817658424377\n",
      "Epoch 6781/10000, Training Loss: 0.6543982625007629, Validation Loss: 0.8289787769317627\n",
      "Epoch 6782/10000, Training Loss: 0.6844190359115601, Validation Loss: 0.6898446083068848\n",
      "Epoch 6783/10000, Training Loss: 0.6603736281394958, Validation Loss: 0.5287346839904785\n",
      "Epoch 6784/10000, Training Loss: 0.6463795900344849, Validation Loss: 0.6470673680305481\n",
      "Epoch 6785/10000, Training Loss: 0.6669560074806213, Validation Loss: 0.8720772862434387\n",
      "Epoch 6786/10000, Training Loss: 0.6655439138412476, Validation Loss: 0.7601507306098938\n",
      "Epoch 6787/10000, Training Loss: 0.6115162372589111, Validation Loss: 1.0709565877914429\n",
      "Epoch 6788/10000, Training Loss: 0.6488797068595886, Validation Loss: 0.555259644985199\n",
      "Epoch 6789/10000, Training Loss: 0.6298093199729919, Validation Loss: 0.7602149844169617\n",
      "Epoch 6790/10000, Training Loss: 0.6818848252296448, Validation Loss: 0.8279613852500916\n",
      "Epoch 6791/10000, Training Loss: 0.6621752977371216, Validation Loss: 0.8217031359672546\n",
      "Epoch 6792/10000, Training Loss: 0.6640757322311401, Validation Loss: 0.722912073135376\n",
      "Epoch 6793/10000, Training Loss: 0.6598351001739502, Validation Loss: 0.7297940254211426\n",
      "Epoch 6794/10000, Training Loss: 0.6353002190589905, Validation Loss: 0.8891518712043762\n",
      "Epoch 6795/10000, Training Loss: 0.6329663991928101, Validation Loss: 0.6105751991271973\n",
      "Epoch 6796/10000, Training Loss: 0.6548592448234558, Validation Loss: 0.8139758110046387\n",
      "Epoch 6797/10000, Training Loss: 0.7116912603378296, Validation Loss: 0.6223264336585999\n",
      "Epoch 6798/10000, Training Loss: 0.6664581298828125, Validation Loss: 0.722057044506073\n",
      "Epoch 6799/10000, Training Loss: 0.6282210946083069, Validation Loss: 0.5835708975791931\n",
      "Epoch 6800/10000, Training Loss: 0.6561035513877869, Validation Loss: 0.6817994117736816\n",
      "Epoch 6801/10000, Training Loss: 0.65669846534729, Validation Loss: 0.7904308438301086\n",
      "Epoch 6802/10000, Training Loss: 0.6747792959213257, Validation Loss: 0.8444135189056396\n",
      "Epoch 6803/10000, Training Loss: 0.6310651302337646, Validation Loss: 0.8011951446533203\n",
      "Epoch 6804/10000, Training Loss: 0.632748544216156, Validation Loss: 0.7193682193756104\n",
      "Epoch 6805/10000, Training Loss: 0.6226667165756226, Validation Loss: 0.7685851454734802\n",
      "Epoch 6806/10000, Training Loss: 0.6633116006851196, Validation Loss: 0.7029182314872742\n",
      "Epoch 6807/10000, Training Loss: 0.6370778679847717, Validation Loss: 0.6871671676635742\n",
      "Epoch 6808/10000, Training Loss: 0.597305953502655, Validation Loss: 0.5895070433616638\n",
      "Epoch 6809/10000, Training Loss: 0.6646090149879456, Validation Loss: 0.5858295559883118\n",
      "Epoch 6810/10000, Training Loss: 0.679610550403595, Validation Loss: 0.6835609078407288\n",
      "Epoch 6811/10000, Training Loss: 0.6562854051589966, Validation Loss: 0.6203534007072449\n",
      "Epoch 6812/10000, Training Loss: 0.6471215486526489, Validation Loss: 0.6857455372810364\n",
      "Epoch 6813/10000, Training Loss: 0.6571369767189026, Validation Loss: 0.7867358326911926\n",
      "Epoch 6814/10000, Training Loss: 0.6894801259040833, Validation Loss: 0.7736462950706482\n",
      "Epoch 6815/10000, Training Loss: 0.589171826839447, Validation Loss: 0.637296199798584\n",
      "Epoch 6816/10000, Training Loss: 0.6626211404800415, Validation Loss: 0.560620129108429\n",
      "Epoch 6817/10000, Training Loss: 0.7494204640388489, Validation Loss: 0.9091828465461731\n",
      "Epoch 6818/10000, Training Loss: 0.672900915145874, Validation Loss: 0.6740725636482239\n",
      "Epoch 6819/10000, Training Loss: 0.6404415369033813, Validation Loss: 1.074126124382019\n",
      "Epoch 6820/10000, Training Loss: 0.6976786255836487, Validation Loss: 0.6881060600280762\n",
      "Epoch 6821/10000, Training Loss: 0.6232408881187439, Validation Loss: 0.7261786460876465\n",
      "Epoch 6822/10000, Training Loss: 0.6457507610321045, Validation Loss: 0.6918136477470398\n",
      "Epoch 6823/10000, Training Loss: 0.6543569564819336, Validation Loss: 0.6705185770988464\n",
      "Epoch 6824/10000, Training Loss: 0.6167031526565552, Validation Loss: 0.775829553604126\n",
      "Epoch 6825/10000, Training Loss: 0.7160539627075195, Validation Loss: 0.7142613530158997\n",
      "Epoch 6826/10000, Training Loss: 0.6763899326324463, Validation Loss: 0.8003094792366028\n",
      "Epoch 6827/10000, Training Loss: 0.6717535853385925, Validation Loss: 0.643291175365448\n",
      "Epoch 6828/10000, Training Loss: 0.6934742331504822, Validation Loss: 0.7737555503845215\n",
      "Epoch 6829/10000, Training Loss: 0.705552339553833, Validation Loss: 0.7082082629203796\n",
      "Epoch 6830/10000, Training Loss: 0.6100771427154541, Validation Loss: 0.6777172088623047\n",
      "Epoch 6831/10000, Training Loss: 0.695196270942688, Validation Loss: 0.7997910380363464\n",
      "Epoch 6832/10000, Training Loss: 0.6829952597618103, Validation Loss: 0.6967694163322449\n",
      "Epoch 6833/10000, Training Loss: 0.644359827041626, Validation Loss: 0.6856565475463867\n",
      "Epoch 6834/10000, Training Loss: 0.6515673995018005, Validation Loss: 0.8582850098609924\n",
      "Epoch 6835/10000, Training Loss: 0.6600509285926819, Validation Loss: 0.6884691119194031\n",
      "Epoch 6836/10000, Training Loss: 0.6644563674926758, Validation Loss: 0.559190571308136\n",
      "Epoch 6837/10000, Training Loss: 0.6687838435173035, Validation Loss: 0.6522290706634521\n",
      "Epoch 6838/10000, Training Loss: 0.7020276784896851, Validation Loss: 0.7632963061332703\n",
      "Epoch 6839/10000, Training Loss: 0.6510275602340698, Validation Loss: 0.8044900298118591\n",
      "Epoch 6840/10000, Training Loss: 0.6898247599601746, Validation Loss: 0.5783901214599609\n",
      "Epoch 6841/10000, Training Loss: 0.693248987197876, Validation Loss: 0.8206484913825989\n",
      "Epoch 6842/10000, Training Loss: 0.6519514918327332, Validation Loss: 0.8370132446289062\n",
      "Epoch 6843/10000, Training Loss: 0.6680201292037964, Validation Loss: 0.7008590698242188\n",
      "Epoch 6844/10000, Training Loss: 0.6909550428390503, Validation Loss: 0.7251903414726257\n",
      "Epoch 6845/10000, Training Loss: 0.6766164302825928, Validation Loss: 0.777285635471344\n",
      "Epoch 6846/10000, Training Loss: 0.6553717851638794, Validation Loss: 0.9100258350372314\n",
      "Epoch 6847/10000, Training Loss: 0.6245028376579285, Validation Loss: 0.7201411128044128\n",
      "Epoch 6848/10000, Training Loss: 0.647100567817688, Validation Loss: 0.5879837870597839\n",
      "Epoch 6849/10000, Training Loss: 0.6658541560173035, Validation Loss: 0.6755571365356445\n",
      "Epoch 6850/10000, Training Loss: 0.6533069014549255, Validation Loss: 0.8961362838745117\n",
      "Epoch 6851/10000, Training Loss: 0.6428133249282837, Validation Loss: 0.6340169906616211\n",
      "Epoch 6852/10000, Training Loss: 0.7123268842697144, Validation Loss: 0.5787920355796814\n",
      "Epoch 6853/10000, Training Loss: 0.6534948348999023, Validation Loss: 0.7361416816711426\n",
      "Epoch 6854/10000, Training Loss: 0.6780943274497986, Validation Loss: 0.6873059868812561\n",
      "Epoch 6855/10000, Training Loss: 0.6498835682868958, Validation Loss: 0.8384280204772949\n",
      "Epoch 6856/10000, Training Loss: 0.7101365327835083, Validation Loss: 0.8086819052696228\n",
      "Epoch 6857/10000, Training Loss: 0.651121973991394, Validation Loss: 0.5472729802131653\n",
      "Epoch 6858/10000, Training Loss: 0.6878258585929871, Validation Loss: 0.6243564486503601\n",
      "Epoch 6859/10000, Training Loss: 0.6526943445205688, Validation Loss: 0.7844443321228027\n",
      "Epoch 6860/10000, Training Loss: 0.610666036605835, Validation Loss: 0.6350798010826111\n",
      "Epoch 6861/10000, Training Loss: 0.7201376557350159, Validation Loss: 0.7748854160308838\n",
      "Epoch 6862/10000, Training Loss: 0.6905564069747925, Validation Loss: 0.7656964659690857\n",
      "Epoch 6863/10000, Training Loss: 0.6817054748535156, Validation Loss: 0.6424055695533752\n",
      "Epoch 6864/10000, Training Loss: 0.6690177917480469, Validation Loss: 0.6647005677223206\n",
      "Epoch 6865/10000, Training Loss: 0.6706934571266174, Validation Loss: 0.7070774435997009\n",
      "Epoch 6866/10000, Training Loss: 0.6509595513343811, Validation Loss: 0.7195195555686951\n",
      "Epoch 6867/10000, Training Loss: 0.6839247345924377, Validation Loss: 0.7114706039428711\n",
      "Epoch 6868/10000, Training Loss: 0.657669723033905, Validation Loss: 0.7341306209564209\n",
      "Epoch 6869/10000, Training Loss: 0.6362532377243042, Validation Loss: 0.8197619915008545\n",
      "Epoch 6870/10000, Training Loss: 0.6439211964607239, Validation Loss: 0.7347472310066223\n",
      "Epoch 6871/10000, Training Loss: 0.6849340796470642, Validation Loss: 0.7607737183570862\n",
      "Epoch 6872/10000, Training Loss: 0.6536186933517456, Validation Loss: 0.5906305313110352\n",
      "Epoch 6873/10000, Training Loss: 0.66439288854599, Validation Loss: 0.48645469546318054\n",
      "Epoch 6874/10000, Training Loss: 0.7040885090827942, Validation Loss: 0.849047839641571\n",
      "Epoch 6875/10000, Training Loss: 0.668572723865509, Validation Loss: 0.6423200964927673\n",
      "Epoch 6876/10000, Training Loss: 0.6684706807136536, Validation Loss: 0.662194550037384\n",
      "Epoch 6877/10000, Training Loss: 0.5700477957725525, Validation Loss: 0.6442561745643616\n",
      "Epoch 6878/10000, Training Loss: 0.6913928985595703, Validation Loss: 0.8350973725318909\n",
      "Epoch 6879/10000, Training Loss: 0.6700823307037354, Validation Loss: 0.6180611252784729\n",
      "Epoch 6880/10000, Training Loss: 0.6235809922218323, Validation Loss: 0.7734375\n",
      "Epoch 6881/10000, Training Loss: 0.6804843544960022, Validation Loss: 0.6185179948806763\n",
      "Epoch 6882/10000, Training Loss: 0.6121670007705688, Validation Loss: 0.6394790410995483\n",
      "Epoch 6883/10000, Training Loss: 0.6461542844772339, Validation Loss: 0.6554186344146729\n",
      "Epoch 6884/10000, Training Loss: 0.6632871627807617, Validation Loss: 0.7422671318054199\n",
      "Epoch 6885/10000, Training Loss: 0.6166447401046753, Validation Loss: 0.7708747982978821\n",
      "Epoch 6886/10000, Training Loss: 0.6684406995773315, Validation Loss: 0.795576810836792\n",
      "Epoch 6887/10000, Training Loss: 0.6168746948242188, Validation Loss: 0.7157137393951416\n",
      "Epoch 6888/10000, Training Loss: 0.6586657762527466, Validation Loss: 0.6119276881217957\n",
      "Epoch 6889/10000, Training Loss: 0.6514081358909607, Validation Loss: 0.7563979625701904\n",
      "Epoch 6890/10000, Training Loss: 0.6612563729286194, Validation Loss: 0.7469346523284912\n",
      "Epoch 6891/10000, Training Loss: 0.6995097398757935, Validation Loss: 0.9341852068901062\n",
      "Epoch 6892/10000, Training Loss: 0.6785525679588318, Validation Loss: 0.6180550456047058\n",
      "Epoch 6893/10000, Training Loss: 0.6325795650482178, Validation Loss: 0.9287102818489075\n",
      "Epoch 6894/10000, Training Loss: 0.6370056867599487, Validation Loss: 0.7147620320320129\n",
      "Epoch 6895/10000, Training Loss: 0.6186700463294983, Validation Loss: 0.5435275435447693\n",
      "Epoch 6896/10000, Training Loss: 0.6492817401885986, Validation Loss: 0.5993291735649109\n",
      "Epoch 6897/10000, Training Loss: 0.6311091184616089, Validation Loss: 0.6669910550117493\n",
      "Epoch 6898/10000, Training Loss: 0.7119370102882385, Validation Loss: 0.6719157695770264\n",
      "Epoch 6899/10000, Training Loss: 0.6704839468002319, Validation Loss: 0.7036221623420715\n",
      "Epoch 6900/10000, Training Loss: 0.6601155996322632, Validation Loss: 0.6555836796760559\n",
      "Epoch 6901/10000, Training Loss: 0.704849362373352, Validation Loss: 0.7052464485168457\n",
      "Epoch 6902/10000, Training Loss: 0.6960891485214233, Validation Loss: 0.7310599684715271\n",
      "Epoch 6903/10000, Training Loss: 0.6589170694351196, Validation Loss: 0.6538479924201965\n",
      "Epoch 6904/10000, Training Loss: 0.6364991664886475, Validation Loss: 0.8731317520141602\n",
      "Epoch 6905/10000, Training Loss: 0.6779047846794128, Validation Loss: 0.6317274570465088\n",
      "Epoch 6906/10000, Training Loss: 0.7060781717300415, Validation Loss: 0.676703929901123\n",
      "Epoch 6907/10000, Training Loss: 0.656630277633667, Validation Loss: 0.7755714058876038\n",
      "Epoch 6908/10000, Training Loss: 0.6667972207069397, Validation Loss: 0.6351327300071716\n",
      "Epoch 6909/10000, Training Loss: 0.6602776050567627, Validation Loss: 0.6628109216690063\n",
      "Epoch 6910/10000, Training Loss: 0.6527413725852966, Validation Loss: 0.5723703503608704\n",
      "Epoch 6911/10000, Training Loss: 0.678436279296875, Validation Loss: 0.729697048664093\n",
      "Epoch 6912/10000, Training Loss: 0.6824840307235718, Validation Loss: 0.6375232338905334\n",
      "Epoch 6913/10000, Training Loss: 0.697611927986145, Validation Loss: 0.7096940875053406\n",
      "Epoch 6914/10000, Training Loss: 0.6386706829071045, Validation Loss: 0.6038604378700256\n",
      "Epoch 6915/10000, Training Loss: 0.6966253519058228, Validation Loss: 0.8589441180229187\n",
      "Epoch 6916/10000, Training Loss: 0.6441126465797424, Validation Loss: 1.0288172960281372\n",
      "Epoch 6917/10000, Training Loss: 0.6762844920158386, Validation Loss: 0.8018891215324402\n",
      "Epoch 6918/10000, Training Loss: 0.6655480861663818, Validation Loss: 0.7055218815803528\n",
      "Epoch 6919/10000, Training Loss: 0.6651856303215027, Validation Loss: 0.6924582123756409\n",
      "Epoch 6920/10000, Training Loss: 0.6528483033180237, Validation Loss: 0.6191537380218506\n",
      "Epoch 6921/10000, Training Loss: 0.6893783211708069, Validation Loss: 0.756880521774292\n",
      "Epoch 6922/10000, Training Loss: 0.6826714873313904, Validation Loss: 0.6871421933174133\n",
      "Epoch 6923/10000, Training Loss: 0.6830923557281494, Validation Loss: 0.7960088849067688\n",
      "Epoch 6924/10000, Training Loss: 0.643035888671875, Validation Loss: 0.6996128559112549\n",
      "Epoch 6925/10000, Training Loss: 0.6447404026985168, Validation Loss: 0.7114267349243164\n",
      "Epoch 6926/10000, Training Loss: 0.6719257831573486, Validation Loss: 0.5434862971305847\n",
      "Epoch 6927/10000, Training Loss: 0.657573401927948, Validation Loss: 0.6619971394538879\n",
      "Epoch 6928/10000, Training Loss: 0.6502892971038818, Validation Loss: 0.7588772773742676\n",
      "Epoch 6929/10000, Training Loss: 0.6663591861724854, Validation Loss: 0.7127653956413269\n",
      "Epoch 6930/10000, Training Loss: 0.668116569519043, Validation Loss: 0.9886136054992676\n",
      "Epoch 6931/10000, Training Loss: 0.639725923538208, Validation Loss: 0.8071261048316956\n",
      "Epoch 6932/10000, Training Loss: 0.6575990319252014, Validation Loss: 0.5629742741584778\n",
      "Epoch 6933/10000, Training Loss: 0.6804660558700562, Validation Loss: 0.598041832447052\n",
      "Epoch 6934/10000, Training Loss: 0.6520887017250061, Validation Loss: 0.755746066570282\n",
      "Epoch 6935/10000, Training Loss: 0.6743503212928772, Validation Loss: 0.6496328711509705\n",
      "Epoch 6936/10000, Training Loss: 0.6839479804039001, Validation Loss: 0.699444591999054\n",
      "Epoch 6937/10000, Training Loss: 0.6593331694602966, Validation Loss: 0.7199252247810364\n",
      "Epoch 6938/10000, Training Loss: 0.6419790983200073, Validation Loss: 0.5577318072319031\n",
      "Epoch 6939/10000, Training Loss: 0.6906852722167969, Validation Loss: 0.6384172439575195\n",
      "Epoch 6940/10000, Training Loss: 0.654277503490448, Validation Loss: 0.6707646250724792\n",
      "Epoch 6941/10000, Training Loss: 0.6819881796836853, Validation Loss: 0.6856651902198792\n",
      "Epoch 6942/10000, Training Loss: 0.6734790802001953, Validation Loss: 0.7271595597267151\n",
      "Epoch 6943/10000, Training Loss: 0.6734189391136169, Validation Loss: 0.6455443501472473\n",
      "Epoch 6944/10000, Training Loss: 0.6328200101852417, Validation Loss: 0.9260482788085938\n",
      "Epoch 6945/10000, Training Loss: 0.6611548662185669, Validation Loss: 0.7412338852882385\n",
      "Epoch 6946/10000, Training Loss: 0.6335405707359314, Validation Loss: 0.9420700669288635\n",
      "Epoch 6947/10000, Training Loss: 0.6297743916511536, Validation Loss: 0.7034644484519958\n",
      "Epoch 6948/10000, Training Loss: 0.6203765273094177, Validation Loss: 0.7216627597808838\n",
      "Epoch 6949/10000, Training Loss: 0.7005884647369385, Validation Loss: 0.7064356207847595\n",
      "Epoch 6950/10000, Training Loss: 0.6546792387962341, Validation Loss: 0.6399765610694885\n",
      "Epoch 6951/10000, Training Loss: 0.6565836668014526, Validation Loss: 0.7601196765899658\n",
      "Epoch 6952/10000, Training Loss: 0.6633920669555664, Validation Loss: 0.6603127121925354\n",
      "Epoch 6953/10000, Training Loss: 0.6891207098960876, Validation Loss: 0.9133003354072571\n",
      "Epoch 6954/10000, Training Loss: 0.6663357615470886, Validation Loss: 0.8345947265625\n",
      "Epoch 6955/10000, Training Loss: 0.7053003311157227, Validation Loss: 0.7830542922019958\n",
      "Epoch 6956/10000, Training Loss: 0.6082528829574585, Validation Loss: 0.5464872717857361\n",
      "Epoch 6957/10000, Training Loss: 0.6501044631004333, Validation Loss: 0.5489495396614075\n",
      "Epoch 6958/10000, Training Loss: 0.6187185049057007, Validation Loss: 0.6404536366462708\n",
      "Epoch 6959/10000, Training Loss: 0.6853079199790955, Validation Loss: 0.7016168236732483\n",
      "Epoch 6960/10000, Training Loss: 0.6766284704208374, Validation Loss: 0.7826893329620361\n",
      "Epoch 6961/10000, Training Loss: 0.6739211678504944, Validation Loss: 0.671680748462677\n",
      "Epoch 6962/10000, Training Loss: 0.6290918588638306, Validation Loss: 0.8947534561157227\n",
      "Epoch 6963/10000, Training Loss: 0.6964526772499084, Validation Loss: 1.1145967245101929\n",
      "Epoch 6964/10000, Training Loss: 0.6671133041381836, Validation Loss: 0.6627031564712524\n",
      "Epoch 6965/10000, Training Loss: 0.6638033986091614, Validation Loss: 0.6799172759056091\n",
      "Epoch 6966/10000, Training Loss: 0.6496321558952332, Validation Loss: 0.6561188697814941\n",
      "Epoch 6967/10000, Training Loss: 0.6353880763053894, Validation Loss: 0.6278840899467468\n",
      "Epoch 6968/10000, Training Loss: 0.709039032459259, Validation Loss: 0.8314122557640076\n",
      "Epoch 6969/10000, Training Loss: 0.6360901594161987, Validation Loss: 0.7903690338134766\n",
      "Epoch 6970/10000, Training Loss: 0.6598002314567566, Validation Loss: 0.6011401414871216\n",
      "Epoch 6971/10000, Training Loss: 0.6829661130905151, Validation Loss: 0.6628603339195251\n",
      "Epoch 6972/10000, Training Loss: 0.6565946340560913, Validation Loss: 0.5500630736351013\n",
      "Epoch 6973/10000, Training Loss: 0.6518038511276245, Validation Loss: 0.6196131110191345\n",
      "Epoch 6974/10000, Training Loss: 0.6451391577720642, Validation Loss: 0.758735716342926\n",
      "Epoch 6975/10000, Training Loss: 0.6048010587692261, Validation Loss: 0.6427973508834839\n",
      "Epoch 6976/10000, Training Loss: 0.6643988490104675, Validation Loss: 0.7099725604057312\n",
      "Epoch 6977/10000, Training Loss: 0.631943941116333, Validation Loss: 0.655027449131012\n",
      "Epoch 6978/10000, Training Loss: 0.673416018486023, Validation Loss: 0.6508644819259644\n",
      "Epoch 6979/10000, Training Loss: 0.6600134372711182, Validation Loss: 0.6886616349220276\n",
      "Epoch 6980/10000, Training Loss: 0.5830577611923218, Validation Loss: 0.5712676644325256\n",
      "Epoch 6981/10000, Training Loss: 0.6672042012214661, Validation Loss: 0.5773299336433411\n",
      "Epoch 6982/10000, Training Loss: 0.6763439178466797, Validation Loss: 0.6758120656013489\n",
      "Epoch 6983/10000, Training Loss: 0.7051982283592224, Validation Loss: 0.7400423884391785\n",
      "Epoch 6984/10000, Training Loss: 0.6585420370101929, Validation Loss: 0.8156661987304688\n",
      "Epoch 6985/10000, Training Loss: 0.6899284720420837, Validation Loss: 0.6879375576972961\n",
      "Epoch 6986/10000, Training Loss: 0.6663363575935364, Validation Loss: 0.67433762550354\n",
      "Epoch 6987/10000, Training Loss: 0.7046385407447815, Validation Loss: 0.7404699921607971\n",
      "Epoch 6988/10000, Training Loss: 0.7094107270240784, Validation Loss: 0.8196485042572021\n",
      "Epoch 6989/10000, Training Loss: 0.6308804750442505, Validation Loss: 0.7047379016876221\n",
      "Epoch 6990/10000, Training Loss: 0.6356526613235474, Validation Loss: 0.6035470962524414\n",
      "Epoch 6991/10000, Training Loss: 0.6831452250480652, Validation Loss: 0.8508618474006653\n",
      "Epoch 6992/10000, Training Loss: 0.6552078723907471, Validation Loss: 0.5950396656990051\n",
      "Epoch 6993/10000, Training Loss: 0.6625229716300964, Validation Loss: 0.7074435353279114\n",
      "Epoch 6994/10000, Training Loss: 0.6341771483421326, Validation Loss: 0.656209409236908\n",
      "Epoch 6995/10000, Training Loss: 0.6589718461036682, Validation Loss: 0.7819125056266785\n",
      "Epoch 6996/10000, Training Loss: 0.6482120752334595, Validation Loss: 0.6426236033439636\n",
      "Epoch 6997/10000, Training Loss: 0.6519418954849243, Validation Loss: 0.6977997422218323\n",
      "Epoch 6998/10000, Training Loss: 0.6696738600730896, Validation Loss: 0.7495535016059875\n",
      "Epoch 6999/10000, Training Loss: 0.6864337921142578, Validation Loss: 0.7543397545814514\n",
      "Epoch 7000/10000, Training Loss: 0.654072105884552, Validation Loss: 0.6253583431243896\n",
      "Epoch 7001/10000, Training Loss: 0.6574666500091553, Validation Loss: 0.769913375377655\n",
      "Epoch 7002/10000, Training Loss: 0.6724681258201599, Validation Loss: 0.611204206943512\n",
      "Epoch 7003/10000, Training Loss: 0.6369790434837341, Validation Loss: 0.7958922982215881\n",
      "Epoch 7004/10000, Training Loss: 0.6368199586868286, Validation Loss: 0.7415822148323059\n",
      "Epoch 7005/10000, Training Loss: 0.6757646799087524, Validation Loss: 0.7585761547088623\n",
      "Epoch 7006/10000, Training Loss: 0.665177047252655, Validation Loss: 0.7178452014923096\n",
      "Epoch 7007/10000, Training Loss: 0.6854220628738403, Validation Loss: 0.8238664269447327\n",
      "Epoch 7008/10000, Training Loss: 0.6555770039558411, Validation Loss: 0.7138631343841553\n",
      "Epoch 7009/10000, Training Loss: 0.6347493529319763, Validation Loss: 0.8372419476509094\n",
      "Epoch 7010/10000, Training Loss: 0.6940590143203735, Validation Loss: 0.764837920665741\n",
      "Epoch 7011/10000, Training Loss: 0.6543827652931213, Validation Loss: 0.7892958521842957\n",
      "Epoch 7012/10000, Training Loss: 0.6530891060829163, Validation Loss: 0.5939733386039734\n",
      "Epoch 7013/10000, Training Loss: 0.6585232615470886, Validation Loss: 0.6188358664512634\n",
      "Epoch 7014/10000, Training Loss: 0.6553957462310791, Validation Loss: 0.6474649310112\n",
      "Epoch 7015/10000, Training Loss: 0.6484884023666382, Validation Loss: 0.8497053980827332\n",
      "Epoch 7016/10000, Training Loss: 0.6612130403518677, Validation Loss: 0.5028812289237976\n",
      "Epoch 7017/10000, Training Loss: 0.6512642502784729, Validation Loss: 0.5420368909835815\n",
      "Epoch 7018/10000, Training Loss: 0.6750067472457886, Validation Loss: 0.7422692775726318\n",
      "Epoch 7019/10000, Training Loss: 0.639708399772644, Validation Loss: 0.7128183245658875\n",
      "Epoch 7020/10000, Training Loss: 0.636915922164917, Validation Loss: 0.8149127960205078\n",
      "Epoch 7021/10000, Training Loss: 0.659866452217102, Validation Loss: 0.9470765590667725\n",
      "Epoch 7022/10000, Training Loss: 0.689192533493042, Validation Loss: 0.825714111328125\n",
      "Epoch 7023/10000, Training Loss: 0.6504159569740295, Validation Loss: 0.6505196690559387\n",
      "Epoch 7024/10000, Training Loss: 0.6501608490943909, Validation Loss: 0.7201943397521973\n",
      "Epoch 7025/10000, Training Loss: 0.6195528507232666, Validation Loss: 0.7813342213630676\n",
      "Epoch 7026/10000, Training Loss: 0.6825592517852783, Validation Loss: 0.5228580832481384\n",
      "Epoch 7027/10000, Training Loss: 0.6842638254165649, Validation Loss: 0.5952462553977966\n",
      "Epoch 7028/10000, Training Loss: 0.6478718519210815, Validation Loss: 0.7328700423240662\n",
      "Epoch 7029/10000, Training Loss: 0.6947547197341919, Validation Loss: 0.6948478817939758\n",
      "Epoch 7030/10000, Training Loss: 0.6697664260864258, Validation Loss: 0.6575126051902771\n",
      "Epoch 7031/10000, Training Loss: 0.6592522859573364, Validation Loss: 0.6664795875549316\n",
      "Epoch 7032/10000, Training Loss: 0.6051536202430725, Validation Loss: 0.6835421919822693\n",
      "Epoch 7033/10000, Training Loss: 0.6165403127670288, Validation Loss: 0.7514989376068115\n",
      "Epoch 7034/10000, Training Loss: 0.6544349193572998, Validation Loss: 0.9036098122596741\n",
      "Epoch 7035/10000, Training Loss: 0.6791094541549683, Validation Loss: 0.6989240646362305\n",
      "Epoch 7036/10000, Training Loss: 0.7052978873252869, Validation Loss: 0.5204800963401794\n",
      "Epoch 7037/10000, Training Loss: 0.6318754553794861, Validation Loss: 0.7550771236419678\n",
      "Epoch 7038/10000, Training Loss: 0.6556670069694519, Validation Loss: 1.0202369689941406\n",
      "Epoch 7039/10000, Training Loss: 0.6511954665184021, Validation Loss: 0.7913672924041748\n",
      "Epoch 7040/10000, Training Loss: 0.645599901676178, Validation Loss: 0.6268800497055054\n",
      "Epoch 7041/10000, Training Loss: 0.6206337213516235, Validation Loss: 0.5584403872489929\n",
      "Epoch 7042/10000, Training Loss: 0.6388956904411316, Validation Loss: 0.8330001831054688\n",
      "Epoch 7043/10000, Training Loss: 0.6654484272003174, Validation Loss: 0.7571606636047363\n",
      "Epoch 7044/10000, Training Loss: 0.6630917191505432, Validation Loss: 0.6735315322875977\n",
      "Epoch 7045/10000, Training Loss: 0.6205103993415833, Validation Loss: 0.6256380081176758\n",
      "Epoch 7046/10000, Training Loss: 0.6461309790611267, Validation Loss: 0.5943217277526855\n",
      "Epoch 7047/10000, Training Loss: 0.6545670628547668, Validation Loss: 0.7811377644538879\n",
      "Epoch 7048/10000, Training Loss: 0.6354426145553589, Validation Loss: 0.6557820439338684\n",
      "Epoch 7049/10000, Training Loss: 0.7263835072517395, Validation Loss: 0.6722244620323181\n",
      "Epoch 7050/10000, Training Loss: 0.6683654189109802, Validation Loss: 0.6874403953552246\n",
      "Epoch 7051/10000, Training Loss: 0.646648645401001, Validation Loss: 0.8174135088920593\n",
      "Epoch 7052/10000, Training Loss: 0.6486362218856812, Validation Loss: 0.733192503452301\n",
      "Epoch 7053/10000, Training Loss: 0.6296253204345703, Validation Loss: 0.9335222244262695\n",
      "Epoch 7054/10000, Training Loss: 0.6763079762458801, Validation Loss: 1.177916169166565\n",
      "Epoch 7055/10000, Training Loss: 0.6902364492416382, Validation Loss: 0.7619923949241638\n",
      "Epoch 7056/10000, Training Loss: 0.6993570923805237, Validation Loss: 0.6553701162338257\n",
      "Epoch 7057/10000, Training Loss: 0.6562252044677734, Validation Loss: 0.8992511630058289\n",
      "Epoch 7058/10000, Training Loss: 0.654906153678894, Validation Loss: 0.7165774703025818\n",
      "Epoch 7059/10000, Training Loss: 0.6653778553009033, Validation Loss: 0.7343667149543762\n",
      "Epoch 7060/10000, Training Loss: 0.6781005859375, Validation Loss: 0.9743711352348328\n",
      "Epoch 7061/10000, Training Loss: 0.6689510345458984, Validation Loss: 0.7315515875816345\n",
      "Epoch 7062/10000, Training Loss: 0.5983346700668335, Validation Loss: 0.7384130358695984\n",
      "Epoch 7063/10000, Training Loss: 0.6680668592453003, Validation Loss: 0.5290114283561707\n",
      "Epoch 7064/10000, Training Loss: 0.6848270893096924, Validation Loss: 0.8136175274848938\n",
      "Epoch 7065/10000, Training Loss: 0.6724582314491272, Validation Loss: 0.8968737721443176\n",
      "Epoch 7066/10000, Training Loss: 0.6338155269622803, Validation Loss: 0.6541362404823303\n",
      "Epoch 7067/10000, Training Loss: 0.7253046631813049, Validation Loss: 0.5734781622886658\n",
      "Epoch 7068/10000, Training Loss: 0.6292788982391357, Validation Loss: 0.551680326461792\n",
      "Epoch 7069/10000, Training Loss: 0.6169966459274292, Validation Loss: 0.6705650687217712\n",
      "Epoch 7070/10000, Training Loss: 0.6084486246109009, Validation Loss: 0.6290661692619324\n",
      "Epoch 7071/10000, Training Loss: 0.659299910068512, Validation Loss: 0.7014184594154358\n",
      "Epoch 7072/10000, Training Loss: 0.6583912372589111, Validation Loss: 0.6972033977508545\n",
      "Epoch 7073/10000, Training Loss: 0.7014802694320679, Validation Loss: 0.8242261409759521\n",
      "Epoch 7074/10000, Training Loss: 0.6513932347297668, Validation Loss: 0.9061102867126465\n",
      "Epoch 7075/10000, Training Loss: 0.6450853943824768, Validation Loss: 0.7208617329597473\n",
      "Epoch 7076/10000, Training Loss: 0.6598809957504272, Validation Loss: 0.7676377892494202\n",
      "Epoch 7077/10000, Training Loss: 0.6405599117279053, Validation Loss: 0.7142991423606873\n",
      "Epoch 7078/10000, Training Loss: 0.6741473078727722, Validation Loss: 0.7083603739738464\n",
      "Epoch 7079/10000, Training Loss: 0.6380163431167603, Validation Loss: 0.7040100693702698\n",
      "Epoch 7080/10000, Training Loss: 0.7286268472671509, Validation Loss: 0.64262455701828\n",
      "Epoch 7081/10000, Training Loss: 0.6353216767311096, Validation Loss: 0.6433420777320862\n",
      "Epoch 7082/10000, Training Loss: 0.65300452709198, Validation Loss: 0.8034160733222961\n",
      "Epoch 7083/10000, Training Loss: 0.6533873677253723, Validation Loss: 0.7107657790184021\n",
      "Epoch 7084/10000, Training Loss: 0.6618673801422119, Validation Loss: 0.5902000665664673\n",
      "Epoch 7085/10000, Training Loss: 0.6721869707107544, Validation Loss: 0.6844834685325623\n",
      "Epoch 7086/10000, Training Loss: 0.6684089303016663, Validation Loss: 0.6880699992179871\n",
      "Epoch 7087/10000, Training Loss: 0.6782804131507874, Validation Loss: 0.8585003018379211\n",
      "Epoch 7088/10000, Training Loss: 0.6643633246421814, Validation Loss: 0.5043874382972717\n",
      "Epoch 7089/10000, Training Loss: 0.6687854528427124, Validation Loss: 0.5506100654602051\n",
      "Epoch 7090/10000, Training Loss: 0.6827284097671509, Validation Loss: 0.6219100952148438\n",
      "Epoch 7091/10000, Training Loss: 0.6559399962425232, Validation Loss: 0.7119181752204895\n",
      "Epoch 7092/10000, Training Loss: 0.657623291015625, Validation Loss: 0.7610654830932617\n",
      "Epoch 7093/10000, Training Loss: 0.6660140752792358, Validation Loss: 0.6975657343864441\n",
      "Epoch 7094/10000, Training Loss: 0.7087222337722778, Validation Loss: 0.7636306285858154\n",
      "Epoch 7095/10000, Training Loss: 0.6545226573944092, Validation Loss: 0.7722675204277039\n",
      "Epoch 7096/10000, Training Loss: 0.6423785090446472, Validation Loss: 0.7115450501441956\n",
      "Epoch 7097/10000, Training Loss: 0.6567350625991821, Validation Loss: 0.6772578358650208\n",
      "Epoch 7098/10000, Training Loss: 0.6763648390769958, Validation Loss: 0.7209230065345764\n",
      "Epoch 7099/10000, Training Loss: 0.649369478225708, Validation Loss: 0.543084442615509\n",
      "Epoch 7100/10000, Training Loss: 0.6799798607826233, Validation Loss: 0.6773821711540222\n",
      "Epoch 7101/10000, Training Loss: 0.6625522971153259, Validation Loss: 0.803525447845459\n",
      "Epoch 7102/10000, Training Loss: 0.6433998346328735, Validation Loss: 0.7953888773918152\n",
      "Epoch 7103/10000, Training Loss: 0.662742018699646, Validation Loss: 0.8093196749687195\n",
      "Epoch 7104/10000, Training Loss: 0.6250912547111511, Validation Loss: 0.8147003650665283\n",
      "Epoch 7105/10000, Training Loss: 0.6600167751312256, Validation Loss: 0.6061345338821411\n",
      "Epoch 7106/10000, Training Loss: 0.6763820648193359, Validation Loss: 0.8837624192237854\n",
      "Epoch 7107/10000, Training Loss: 0.6658099889755249, Validation Loss: 0.7642533183097839\n",
      "Epoch 7108/10000, Training Loss: 0.6622214913368225, Validation Loss: 0.5625123381614685\n",
      "Epoch 7109/10000, Training Loss: 0.6283659338951111, Validation Loss: 0.7406396865844727\n",
      "Epoch 7110/10000, Training Loss: 0.6486720442771912, Validation Loss: 0.7271426320075989\n",
      "Epoch 7111/10000, Training Loss: 0.6575022339820862, Validation Loss: 0.6494495272636414\n",
      "Epoch 7112/10000, Training Loss: 0.644338071346283, Validation Loss: 0.786210834980011\n",
      "Epoch 7113/10000, Training Loss: 0.6280083060264587, Validation Loss: 0.8306373953819275\n",
      "Epoch 7114/10000, Training Loss: 0.6485488414764404, Validation Loss: 0.7395376563072205\n",
      "Epoch 7115/10000, Training Loss: 0.6820647716522217, Validation Loss: 0.8777176737785339\n",
      "Epoch 7116/10000, Training Loss: 0.6570773720741272, Validation Loss: 0.6759719252586365\n",
      "Epoch 7117/10000, Training Loss: 0.6402151584625244, Validation Loss: 0.6798562407493591\n",
      "Epoch 7118/10000, Training Loss: 0.6706545948982239, Validation Loss: 0.6960132718086243\n",
      "Epoch 7119/10000, Training Loss: 0.6406930088996887, Validation Loss: 0.7341251373291016\n",
      "Epoch 7120/10000, Training Loss: 0.6274634003639221, Validation Loss: 1.1274632215499878\n",
      "Epoch 7121/10000, Training Loss: 0.607042670249939, Validation Loss: 0.5941404700279236\n",
      "Epoch 7122/10000, Training Loss: 0.6604183912277222, Validation Loss: 0.7478683590888977\n",
      "Epoch 7123/10000, Training Loss: 0.6731933355331421, Validation Loss: 0.8443830609321594\n",
      "Epoch 7124/10000, Training Loss: 0.6529737710952759, Validation Loss: 0.639367401599884\n",
      "Epoch 7125/10000, Training Loss: 0.6489206552505493, Validation Loss: 0.746344268321991\n",
      "Epoch 7126/10000, Training Loss: 0.6515435576438904, Validation Loss: 0.5368165373802185\n",
      "Epoch 7127/10000, Training Loss: 0.6703796982765198, Validation Loss: 0.6704927086830139\n",
      "Epoch 7128/10000, Training Loss: 0.6643995642662048, Validation Loss: 0.6979191899299622\n",
      "Epoch 7129/10000, Training Loss: 0.6276886463165283, Validation Loss: 0.6656070351600647\n",
      "Epoch 7130/10000, Training Loss: 0.6233660578727722, Validation Loss: 0.525474488735199\n",
      "Epoch 7131/10000, Training Loss: 0.6487741470336914, Validation Loss: 0.7271650433540344\n",
      "Epoch 7132/10000, Training Loss: 0.6790825128555298, Validation Loss: 0.6037251353263855\n",
      "Epoch 7133/10000, Training Loss: 0.6708354353904724, Validation Loss: 0.7213584780693054\n",
      "Epoch 7134/10000, Training Loss: 0.6682763695716858, Validation Loss: 0.7101221084594727\n",
      "Epoch 7135/10000, Training Loss: 0.6372050642967224, Validation Loss: 0.6932904720306396\n",
      "Epoch 7136/10000, Training Loss: 0.6502071022987366, Validation Loss: 0.6344650983810425\n",
      "Epoch 7137/10000, Training Loss: 0.6187813878059387, Validation Loss: 0.7733126282691956\n",
      "Epoch 7138/10000, Training Loss: 0.6458233594894409, Validation Loss: 0.8198168277740479\n",
      "Epoch 7139/10000, Training Loss: 0.6610449552536011, Validation Loss: 0.7599653601646423\n",
      "Epoch 7140/10000, Training Loss: 0.6816810965538025, Validation Loss: 0.7973534464836121\n",
      "Epoch 7141/10000, Training Loss: 0.6558226346969604, Validation Loss: 0.857518196105957\n",
      "Epoch 7142/10000, Training Loss: 0.687149167060852, Validation Loss: 0.6594800353050232\n",
      "Epoch 7143/10000, Training Loss: 0.614495575428009, Validation Loss: 0.6233147978782654\n",
      "Epoch 7144/10000, Training Loss: 0.6370773315429688, Validation Loss: 0.5900209546089172\n",
      "Epoch 7145/10000, Training Loss: 0.6858537197113037, Validation Loss: 0.6704201698303223\n",
      "Epoch 7146/10000, Training Loss: 0.6739225387573242, Validation Loss: 0.6550140380859375\n",
      "Epoch 7147/10000, Training Loss: 0.6623280644416809, Validation Loss: 0.6556230783462524\n",
      "Epoch 7148/10000, Training Loss: 0.6639078855514526, Validation Loss: 0.761101245880127\n",
      "Epoch 7149/10000, Training Loss: 0.678990364074707, Validation Loss: 0.7615416049957275\n",
      "Epoch 7150/10000, Training Loss: 0.6563730835914612, Validation Loss: 0.5748147368431091\n",
      "Epoch 7151/10000, Training Loss: 0.6716522574424744, Validation Loss: 0.6079298853874207\n",
      "Epoch 7152/10000, Training Loss: 0.6622289419174194, Validation Loss: 0.916157066822052\n",
      "Epoch 7153/10000, Training Loss: 0.6880659461021423, Validation Loss: 0.6955820918083191\n",
      "Epoch 7154/10000, Training Loss: 0.6374251842498779, Validation Loss: 0.7194098830223083\n",
      "Epoch 7155/10000, Training Loss: 0.6435036063194275, Validation Loss: 0.6439823508262634\n",
      "Epoch 7156/10000, Training Loss: 0.6564343571662903, Validation Loss: 0.7473458647727966\n",
      "Epoch 7157/10000, Training Loss: 0.6528781056404114, Validation Loss: 0.7164266705513\n",
      "Epoch 7158/10000, Training Loss: 0.6530786156654358, Validation Loss: 0.7701916098594666\n",
      "Epoch 7159/10000, Training Loss: 0.6589109897613525, Validation Loss: 0.7444679737091064\n",
      "Epoch 7160/10000, Training Loss: 0.6754804253578186, Validation Loss: 0.6536714434623718\n",
      "Epoch 7161/10000, Training Loss: 0.6489061117172241, Validation Loss: 0.8608834743499756\n",
      "Epoch 7162/10000, Training Loss: 0.6187179088592529, Validation Loss: 0.6885797381401062\n",
      "Epoch 7163/10000, Training Loss: 0.6440082788467407, Validation Loss: 0.596531093120575\n",
      "Epoch 7164/10000, Training Loss: 0.6832565665245056, Validation Loss: 0.7638368606567383\n",
      "Epoch 7165/10000, Training Loss: 0.6855536103248596, Validation Loss: 0.7946917414665222\n",
      "Epoch 7166/10000, Training Loss: 0.6712770462036133, Validation Loss: 0.6594282984733582\n",
      "Epoch 7167/10000, Training Loss: 0.6687151193618774, Validation Loss: 0.6312414407730103\n",
      "Epoch 7168/10000, Training Loss: 0.6595152616500854, Validation Loss: 0.851186215877533\n",
      "Epoch 7169/10000, Training Loss: 0.6499877572059631, Validation Loss: 0.7593922019004822\n",
      "Epoch 7170/10000, Training Loss: 0.6468444466590881, Validation Loss: 0.7128807902336121\n",
      "Epoch 7171/10000, Training Loss: 0.6564697623252869, Validation Loss: 0.7857988476753235\n",
      "Epoch 7172/10000, Training Loss: 0.6609228849411011, Validation Loss: 0.9504947662353516\n",
      "Epoch 7173/10000, Training Loss: 0.6786926984786987, Validation Loss: 0.7377106547355652\n",
      "Epoch 7174/10000, Training Loss: 0.6487293243408203, Validation Loss: 0.5761730670928955\n",
      "Epoch 7175/10000, Training Loss: 0.6664667129516602, Validation Loss: 0.6267063021659851\n",
      "Epoch 7176/10000, Training Loss: 0.6543945670127869, Validation Loss: 0.7065754532814026\n",
      "Epoch 7177/10000, Training Loss: 0.6563583612442017, Validation Loss: 0.7197208404541016\n",
      "Epoch 7178/10000, Training Loss: 0.6575040817260742, Validation Loss: 0.9040743708610535\n",
      "Epoch 7179/10000, Training Loss: 0.6452387571334839, Validation Loss: 0.809934139251709\n",
      "Epoch 7180/10000, Training Loss: 0.6801950931549072, Validation Loss: 0.747938871383667\n",
      "Epoch 7181/10000, Training Loss: 0.6528334021568298, Validation Loss: 0.6530253291130066\n",
      "Epoch 7182/10000, Training Loss: 0.6686762571334839, Validation Loss: 0.7223005294799805\n",
      "Epoch 7183/10000, Training Loss: 0.6262487769126892, Validation Loss: 0.7635663151741028\n",
      "Epoch 7184/10000, Training Loss: 0.6469621658325195, Validation Loss: 0.7352296710014343\n",
      "Epoch 7185/10000, Training Loss: 0.6724883317947388, Validation Loss: 0.6547330021858215\n",
      "Epoch 7186/10000, Training Loss: 0.651742696762085, Validation Loss: 0.6980554461479187\n",
      "Epoch 7187/10000, Training Loss: 0.6953067779541016, Validation Loss: 0.7449957728385925\n",
      "Epoch 7188/10000, Training Loss: 0.6258764863014221, Validation Loss: 0.8904886841773987\n",
      "Epoch 7189/10000, Training Loss: 0.6693946123123169, Validation Loss: 0.6624239087104797\n",
      "Epoch 7190/10000, Training Loss: 0.662105917930603, Validation Loss: 0.5436201691627502\n",
      "Epoch 7191/10000, Training Loss: 0.6649460196495056, Validation Loss: 0.6360095143318176\n",
      "Epoch 7192/10000, Training Loss: 0.6969061493873596, Validation Loss: 0.63313227891922\n",
      "Epoch 7193/10000, Training Loss: 0.6746432781219482, Validation Loss: 0.471787691116333\n",
      "Epoch 7194/10000, Training Loss: 0.6616767644882202, Validation Loss: 0.7717304229736328\n",
      "Epoch 7195/10000, Training Loss: 0.6624540090560913, Validation Loss: 0.7456049919128418\n",
      "Epoch 7196/10000, Training Loss: 0.6749368906021118, Validation Loss: 0.5691856741905212\n",
      "Epoch 7197/10000, Training Loss: 0.6846980452537537, Validation Loss: 0.6678102612495422\n",
      "Epoch 7198/10000, Training Loss: 0.6347616910934448, Validation Loss: 0.7290115356445312\n",
      "Epoch 7199/10000, Training Loss: 0.6418637633323669, Validation Loss: 0.5567751526832581\n",
      "Epoch 7200/10000, Training Loss: 0.6400811076164246, Validation Loss: 0.6663448214530945\n",
      "Epoch 7201/10000, Training Loss: 0.6650909781455994, Validation Loss: 0.7099409103393555\n",
      "Epoch 7202/10000, Training Loss: 0.6963649988174438, Validation Loss: 0.7470734119415283\n",
      "Epoch 7203/10000, Training Loss: 0.7040528059005737, Validation Loss: 0.6178649067878723\n",
      "Epoch 7204/10000, Training Loss: 0.6655389070510864, Validation Loss: 0.617636501789093\n",
      "Epoch 7205/10000, Training Loss: 0.6669191122055054, Validation Loss: 0.6945441365242004\n",
      "Epoch 7206/10000, Training Loss: 0.6606689691543579, Validation Loss: 0.7920102477073669\n",
      "Epoch 7207/10000, Training Loss: 0.6632924675941467, Validation Loss: 0.7756933569908142\n",
      "Epoch 7208/10000, Training Loss: 0.6251490712165833, Validation Loss: 0.5410658717155457\n",
      "Epoch 7209/10000, Training Loss: 0.7206064462661743, Validation Loss: 0.9733406901359558\n",
      "Epoch 7210/10000, Training Loss: 0.6392017602920532, Validation Loss: 0.7817981243133545\n",
      "Epoch 7211/10000, Training Loss: 0.639707624912262, Validation Loss: 0.6175153851509094\n",
      "Epoch 7212/10000, Training Loss: 0.6427265405654907, Validation Loss: 1.2048834562301636\n",
      "Epoch 7213/10000, Training Loss: 0.6600633263587952, Validation Loss: 0.5405863523483276\n",
      "Epoch 7214/10000, Training Loss: 0.6421803832054138, Validation Loss: 0.754263162612915\n",
      "Epoch 7215/10000, Training Loss: 0.6691470742225647, Validation Loss: 0.6801474690437317\n",
      "Epoch 7216/10000, Training Loss: 0.6433891654014587, Validation Loss: 0.5238959789276123\n",
      "Epoch 7217/10000, Training Loss: 0.6619466543197632, Validation Loss: 0.6791849732398987\n",
      "Epoch 7218/10000, Training Loss: 0.6549440622329712, Validation Loss: 0.6350775361061096\n",
      "Epoch 7219/10000, Training Loss: 0.6583477854728699, Validation Loss: 0.5986736416816711\n",
      "Epoch 7220/10000, Training Loss: 0.6564359068870544, Validation Loss: 0.6004745364189148\n",
      "Epoch 7221/10000, Training Loss: 0.6844486594200134, Validation Loss: 0.7705788016319275\n",
      "Epoch 7222/10000, Training Loss: 0.6965523958206177, Validation Loss: 0.7098097801208496\n",
      "Epoch 7223/10000, Training Loss: 0.7003942728042603, Validation Loss: 0.7511847019195557\n",
      "Epoch 7224/10000, Training Loss: 0.6428241729736328, Validation Loss: 0.7624335885047913\n",
      "Epoch 7225/10000, Training Loss: 0.6632645130157471, Validation Loss: 0.6603339314460754\n",
      "Epoch 7226/10000, Training Loss: 0.6667889952659607, Validation Loss: 0.7113559246063232\n",
      "Epoch 7227/10000, Training Loss: 0.7132205963134766, Validation Loss: 0.7943816781044006\n",
      "Epoch 7228/10000, Training Loss: 0.6771918535232544, Validation Loss: 1.0549120903015137\n",
      "Epoch 7229/10000, Training Loss: 0.6608205437660217, Validation Loss: 0.755709171295166\n",
      "Epoch 7230/10000, Training Loss: 0.6731544733047485, Validation Loss: 0.9169785976409912\n",
      "Epoch 7231/10000, Training Loss: 0.694599449634552, Validation Loss: 0.7928358912467957\n",
      "Epoch 7232/10000, Training Loss: 0.6535864472389221, Validation Loss: 0.8024188876152039\n",
      "Epoch 7233/10000, Training Loss: 0.673039436340332, Validation Loss: 0.7251214981079102\n",
      "Epoch 7234/10000, Training Loss: 0.6782355904579163, Validation Loss: 0.6894934773445129\n",
      "Epoch 7235/10000, Training Loss: 0.651886522769928, Validation Loss: 0.5881831645965576\n",
      "Epoch 7236/10000, Training Loss: 0.6418648362159729, Validation Loss: 0.676256000995636\n",
      "Epoch 7237/10000, Training Loss: 0.6330579519271851, Validation Loss: 0.6531832814216614\n",
      "Epoch 7238/10000, Training Loss: 0.6524609327316284, Validation Loss: 0.7573015689849854\n",
      "Epoch 7239/10000, Training Loss: 0.7006919384002686, Validation Loss: 0.690909206867218\n",
      "Epoch 7240/10000, Training Loss: 0.6274471282958984, Validation Loss: 0.8338370323181152\n",
      "Epoch 7241/10000, Training Loss: 0.6577167510986328, Validation Loss: 0.806176483631134\n",
      "Epoch 7242/10000, Training Loss: 0.6632689237594604, Validation Loss: 0.6859223246574402\n",
      "Epoch 7243/10000, Training Loss: 0.6363792419433594, Validation Loss: 0.7148820757865906\n",
      "Epoch 7244/10000, Training Loss: 0.6965622305870056, Validation Loss: 0.6700718402862549\n",
      "Epoch 7245/10000, Training Loss: 0.7059264779090881, Validation Loss: 0.6288926005363464\n",
      "Epoch 7246/10000, Training Loss: 0.660925567150116, Validation Loss: 0.7719612717628479\n",
      "Epoch 7247/10000, Training Loss: 0.6868159174919128, Validation Loss: 0.6402615904808044\n",
      "Epoch 7248/10000, Training Loss: 0.617229163646698, Validation Loss: 0.6804727911949158\n",
      "Epoch 7249/10000, Training Loss: 0.6151140332221985, Validation Loss: 0.6386788487434387\n",
      "Epoch 7250/10000, Training Loss: 0.6805632710456848, Validation Loss: 0.6835359930992126\n",
      "Epoch 7251/10000, Training Loss: 0.650575578212738, Validation Loss: 0.6552425026893616\n",
      "Epoch 7252/10000, Training Loss: 0.6470769047737122, Validation Loss: 0.6715421080589294\n",
      "Epoch 7253/10000, Training Loss: 0.6308543086051941, Validation Loss: 0.6800788044929504\n",
      "Epoch 7254/10000, Training Loss: 0.6411756277084351, Validation Loss: 1.029187560081482\n",
      "Epoch 7255/10000, Training Loss: 0.6163638234138489, Validation Loss: 0.7172853946685791\n",
      "Epoch 7256/10000, Training Loss: 0.6526234745979309, Validation Loss: 0.5998396873474121\n",
      "Epoch 7257/10000, Training Loss: 0.6510290503501892, Validation Loss: 0.7093856334686279\n",
      "Epoch 7258/10000, Training Loss: 0.6395969390869141, Validation Loss: 0.7150669097900391\n",
      "Epoch 7259/10000, Training Loss: 0.6456437706947327, Validation Loss: 0.8542571663856506\n",
      "Epoch 7260/10000, Training Loss: 0.6623094081878662, Validation Loss: 0.6439355611801147\n",
      "Epoch 7261/10000, Training Loss: 0.6781132221221924, Validation Loss: 0.6822502613067627\n",
      "Epoch 7262/10000, Training Loss: 0.6697964072227478, Validation Loss: 0.6428984999656677\n",
      "Epoch 7263/10000, Training Loss: 0.642818808555603, Validation Loss: 0.7652701735496521\n",
      "Epoch 7264/10000, Training Loss: 0.6563825607299805, Validation Loss: 0.647516667842865\n",
      "Epoch 7265/10000, Training Loss: 0.6677418947219849, Validation Loss: 0.5564311146736145\n",
      "Epoch 7266/10000, Training Loss: 0.6291508078575134, Validation Loss: 0.7470261454582214\n",
      "Epoch 7267/10000, Training Loss: 0.6742222905158997, Validation Loss: 0.7090952396392822\n",
      "Epoch 7268/10000, Training Loss: 0.62894207239151, Validation Loss: 0.5627923011779785\n",
      "Epoch 7269/10000, Training Loss: 0.6530417203903198, Validation Loss: 0.732936680316925\n",
      "Epoch 7270/10000, Training Loss: 0.6225411295890808, Validation Loss: 0.6277225613594055\n",
      "Epoch 7271/10000, Training Loss: 0.6641606688499451, Validation Loss: 0.5270776152610779\n",
      "Epoch 7272/10000, Training Loss: 0.6286294460296631, Validation Loss: 0.6357478499412537\n",
      "Epoch 7273/10000, Training Loss: 0.6413720846176147, Validation Loss: 0.6831915974617004\n",
      "Epoch 7274/10000, Training Loss: 0.6363033652305603, Validation Loss: 0.5757686495780945\n",
      "Epoch 7275/10000, Training Loss: 0.65064936876297, Validation Loss: 0.6701421737670898\n",
      "Epoch 7276/10000, Training Loss: 0.6756900548934937, Validation Loss: 0.6922040581703186\n",
      "Epoch 7277/10000, Training Loss: 0.6854568123817444, Validation Loss: 0.6569402813911438\n",
      "Epoch 7278/10000, Training Loss: 0.6479565501213074, Validation Loss: 0.5731516480445862\n",
      "Epoch 7279/10000, Training Loss: 0.6220589280128479, Validation Loss: 0.6279802322387695\n",
      "Epoch 7280/10000, Training Loss: 0.6453297734260559, Validation Loss: 0.6661660671234131\n",
      "Epoch 7281/10000, Training Loss: 0.667366623878479, Validation Loss: 0.9272546768188477\n",
      "Epoch 7282/10000, Training Loss: 0.652194082736969, Validation Loss: 0.9475628733634949\n",
      "Epoch 7283/10000, Training Loss: 0.6351365447044373, Validation Loss: 0.77897709608078\n",
      "Epoch 7284/10000, Training Loss: 0.6572765111923218, Validation Loss: 0.6499196887016296\n",
      "Epoch 7285/10000, Training Loss: 0.6665969491004944, Validation Loss: 0.6065550446510315\n",
      "Epoch 7286/10000, Training Loss: 0.6366590261459351, Validation Loss: 0.6353470683097839\n",
      "Epoch 7287/10000, Training Loss: 0.663366436958313, Validation Loss: 0.6875794529914856\n",
      "Epoch 7288/10000, Training Loss: 0.697413980960846, Validation Loss: 0.6684231162071228\n",
      "Epoch 7289/10000, Training Loss: 0.6582095623016357, Validation Loss: 0.5620318651199341\n",
      "Epoch 7290/10000, Training Loss: 0.6201813220977783, Validation Loss: 0.5989890694618225\n",
      "Epoch 7291/10000, Training Loss: 0.6278998851776123, Validation Loss: 0.7998736500740051\n",
      "Epoch 7292/10000, Training Loss: 0.6515530943870544, Validation Loss: 0.7698442935943604\n",
      "Epoch 7293/10000, Training Loss: 0.6729071140289307, Validation Loss: 0.6394062638282776\n",
      "Epoch 7294/10000, Training Loss: 0.6756385564804077, Validation Loss: 0.791694164276123\n",
      "Epoch 7295/10000, Training Loss: 0.6080753803253174, Validation Loss: 0.5843682289123535\n",
      "Epoch 7296/10000, Training Loss: 0.6575465798377991, Validation Loss: 0.6838510632514954\n",
      "Epoch 7297/10000, Training Loss: 0.6487482786178589, Validation Loss: 0.6394788026809692\n",
      "Epoch 7298/10000, Training Loss: 0.6354017853736877, Validation Loss: 0.7248156070709229\n",
      "Epoch 7299/10000, Training Loss: 0.6583274602890015, Validation Loss: 0.6462218165397644\n",
      "Epoch 7300/10000, Training Loss: 0.6844881772994995, Validation Loss: 0.5875161290168762\n",
      "Epoch 7301/10000, Training Loss: 0.6535466313362122, Validation Loss: 0.5186562538146973\n",
      "Epoch 7302/10000, Training Loss: 0.6442725658416748, Validation Loss: 0.7043871879577637\n",
      "Epoch 7303/10000, Training Loss: 0.6854397058486938, Validation Loss: 0.7135298848152161\n",
      "Epoch 7304/10000, Training Loss: 0.634022057056427, Validation Loss: 0.6211117506027222\n",
      "Epoch 7305/10000, Training Loss: 0.6789778470993042, Validation Loss: 0.7686467170715332\n",
      "Epoch 7306/10000, Training Loss: 0.6783053278923035, Validation Loss: 0.5772770047187805\n",
      "Epoch 7307/10000, Training Loss: 0.7010573148727417, Validation Loss: 0.6931302547454834\n",
      "Epoch 7308/10000, Training Loss: 0.6660820245742798, Validation Loss: 0.7396560311317444\n",
      "Epoch 7309/10000, Training Loss: 0.6264367699623108, Validation Loss: 0.5748874545097351\n",
      "Epoch 7310/10000, Training Loss: 0.6535727381706238, Validation Loss: 0.5998594164848328\n",
      "Epoch 7311/10000, Training Loss: 0.6542739868164062, Validation Loss: 0.5957785248756409\n",
      "Epoch 7312/10000, Training Loss: 0.6595453023910522, Validation Loss: 0.6995167136192322\n",
      "Epoch 7313/10000, Training Loss: 0.6790896654129028, Validation Loss: 0.6415032744407654\n",
      "Epoch 7314/10000, Training Loss: 0.6736327409744263, Validation Loss: 0.8645777702331543\n",
      "Epoch 7315/10000, Training Loss: 0.6263335347175598, Validation Loss: 0.8677566647529602\n",
      "Epoch 7316/10000, Training Loss: 0.7000217437744141, Validation Loss: 0.466359943151474\n",
      "Epoch 7317/10000, Training Loss: 0.614347517490387, Validation Loss: 0.7978699803352356\n",
      "Epoch 7318/10000, Training Loss: 0.6427128314971924, Validation Loss: 0.5764068961143494\n",
      "Epoch 7319/10000, Training Loss: 0.6927469968795776, Validation Loss: 0.7752519249916077\n",
      "Epoch 7320/10000, Training Loss: 0.6250664591789246, Validation Loss: 0.6429693102836609\n",
      "Epoch 7321/10000, Training Loss: 0.6862806081771851, Validation Loss: 0.5276375412940979\n",
      "Epoch 7322/10000, Training Loss: 0.6874077320098877, Validation Loss: 0.6675085425376892\n",
      "Epoch 7323/10000, Training Loss: 0.6975768804550171, Validation Loss: 0.6010145545005798\n",
      "Epoch 7324/10000, Training Loss: 0.6893136501312256, Validation Loss: 0.6185877323150635\n",
      "Epoch 7325/10000, Training Loss: 0.6657261252403259, Validation Loss: 0.6699516773223877\n",
      "Epoch 7326/10000, Training Loss: 0.6585178375244141, Validation Loss: 0.6733269691467285\n",
      "Epoch 7327/10000, Training Loss: 0.6579665541648865, Validation Loss: 0.6970070004463196\n",
      "Epoch 7328/10000, Training Loss: 0.6983228325843811, Validation Loss: 0.6684724688529968\n",
      "Epoch 7329/10000, Training Loss: 0.6760661005973816, Validation Loss: 0.6216540932655334\n",
      "Epoch 7330/10000, Training Loss: 0.6448150873184204, Validation Loss: 0.588254988193512\n",
      "Epoch 7331/10000, Training Loss: 0.6413564682006836, Validation Loss: 0.6938745379447937\n",
      "Epoch 7332/10000, Training Loss: 0.6338505148887634, Validation Loss: 0.7128925323486328\n",
      "Epoch 7333/10000, Training Loss: 0.6405763626098633, Validation Loss: 0.6385976672172546\n",
      "Epoch 7334/10000, Training Loss: 0.6346309185028076, Validation Loss: 0.82082200050354\n",
      "Epoch 7335/10000, Training Loss: 0.6656681895256042, Validation Loss: 0.5382037162780762\n",
      "Epoch 7336/10000, Training Loss: 0.6210423707962036, Validation Loss: 0.6652655005455017\n",
      "Epoch 7337/10000, Training Loss: 0.6644164323806763, Validation Loss: 0.8480197787284851\n",
      "Epoch 7338/10000, Training Loss: 0.6314086318016052, Validation Loss: 0.7496256232261658\n",
      "Epoch 7339/10000, Training Loss: 0.6715092062950134, Validation Loss: 0.6287660598754883\n",
      "Epoch 7340/10000, Training Loss: 0.63290935754776, Validation Loss: 0.5755482316017151\n",
      "Epoch 7341/10000, Training Loss: 0.6905186772346497, Validation Loss: 0.8476480841636658\n",
      "Epoch 7342/10000, Training Loss: 0.6641210913658142, Validation Loss: 0.7681710720062256\n",
      "Epoch 7343/10000, Training Loss: 0.6801791787147522, Validation Loss: 0.6388049125671387\n",
      "Epoch 7344/10000, Training Loss: 0.6312580108642578, Validation Loss: 0.617576003074646\n",
      "Epoch 7345/10000, Training Loss: 0.6593790650367737, Validation Loss: 0.8112180233001709\n",
      "Epoch 7346/10000, Training Loss: 0.6405003070831299, Validation Loss: 0.7192301154136658\n",
      "Epoch 7347/10000, Training Loss: 0.6632805466651917, Validation Loss: 0.9132834076881409\n",
      "Epoch 7348/10000, Training Loss: 0.6443681120872498, Validation Loss: 0.743567168712616\n",
      "Epoch 7349/10000, Training Loss: 0.7323611378669739, Validation Loss: 0.7906883358955383\n",
      "Epoch 7350/10000, Training Loss: 0.6573806405067444, Validation Loss: 0.6399990320205688\n",
      "Epoch 7351/10000, Training Loss: 0.6638298630714417, Validation Loss: 0.7987349629402161\n",
      "Epoch 7352/10000, Training Loss: 0.6623370051383972, Validation Loss: 0.8922973275184631\n",
      "Epoch 7353/10000, Training Loss: 0.626583993434906, Validation Loss: 0.7957160472869873\n",
      "Epoch 7354/10000, Training Loss: 0.6394148468971252, Validation Loss: 0.7432969212532043\n",
      "Epoch 7355/10000, Training Loss: 0.6624304056167603, Validation Loss: 0.7317409515380859\n",
      "Epoch 7356/10000, Training Loss: 0.6605864763259888, Validation Loss: 0.6296530365943909\n",
      "Epoch 7357/10000, Training Loss: 0.6667829155921936, Validation Loss: 0.7854526042938232\n",
      "Epoch 7358/10000, Training Loss: 0.6441402435302734, Validation Loss: 0.7340421080589294\n",
      "Epoch 7359/10000, Training Loss: 0.6709885001182556, Validation Loss: 0.7063387036323547\n",
      "Epoch 7360/10000, Training Loss: 0.6511126160621643, Validation Loss: 0.8810948729515076\n",
      "Epoch 7361/10000, Training Loss: 0.6559549570083618, Validation Loss: 0.6935701966285706\n",
      "Epoch 7362/10000, Training Loss: 0.6325219869613647, Validation Loss: 0.9759072661399841\n",
      "Epoch 7363/10000, Training Loss: 0.6578301787376404, Validation Loss: 0.7014484405517578\n",
      "Epoch 7364/10000, Training Loss: 0.6804711818695068, Validation Loss: 0.58316969871521\n",
      "Epoch 7365/10000, Training Loss: 0.6429812908172607, Validation Loss: 0.5984863638877869\n",
      "Epoch 7366/10000, Training Loss: 0.6325284242630005, Validation Loss: 0.7841041088104248\n",
      "Epoch 7367/10000, Training Loss: 0.6455513834953308, Validation Loss: 0.8775345683097839\n",
      "Epoch 7368/10000, Training Loss: 0.6629223227500916, Validation Loss: 0.7934326529502869\n",
      "Epoch 7369/10000, Training Loss: 0.6359688639640808, Validation Loss: 0.8392155766487122\n",
      "Epoch 7370/10000, Training Loss: 0.6546456217765808, Validation Loss: 0.6871301531791687\n",
      "Epoch 7371/10000, Training Loss: 0.6448519825935364, Validation Loss: 0.6759673953056335\n",
      "Epoch 7372/10000, Training Loss: 0.6737087965011597, Validation Loss: 0.6371287107467651\n",
      "Epoch 7373/10000, Training Loss: 0.647705078125, Validation Loss: 0.7886168956756592\n",
      "Epoch 7374/10000, Training Loss: 0.6419230103492737, Validation Loss: 0.5788899064064026\n",
      "Epoch 7375/10000, Training Loss: 0.6413419246673584, Validation Loss: 0.9784815311431885\n",
      "Epoch 7376/10000, Training Loss: 0.6264077425003052, Validation Loss: 0.6479756832122803\n",
      "Epoch 7377/10000, Training Loss: 0.6914809346199036, Validation Loss: 0.6285057663917542\n",
      "Epoch 7378/10000, Training Loss: 0.6384003162384033, Validation Loss: 0.7516422867774963\n",
      "Epoch 7379/10000, Training Loss: 0.659295916557312, Validation Loss: 0.7025968432426453\n",
      "Epoch 7380/10000, Training Loss: 0.6540711522102356, Validation Loss: 0.5586929321289062\n",
      "Epoch 7381/10000, Training Loss: 0.6653302907943726, Validation Loss: 0.6495311856269836\n",
      "Epoch 7382/10000, Training Loss: 0.6440815329551697, Validation Loss: 0.6326873898506165\n",
      "Epoch 7383/10000, Training Loss: 0.6633080840110779, Validation Loss: 0.7210467457771301\n",
      "Epoch 7384/10000, Training Loss: 0.6485500335693359, Validation Loss: 0.6296204924583435\n",
      "Epoch 7385/10000, Training Loss: 0.6223108768463135, Validation Loss: 0.7399689555168152\n",
      "Epoch 7386/10000, Training Loss: 0.6231569051742554, Validation Loss: 0.7161819338798523\n",
      "Epoch 7387/10000, Training Loss: 0.6358969807624817, Validation Loss: 0.7282831072807312\n",
      "Epoch 7388/10000, Training Loss: 0.6359244585037231, Validation Loss: 0.7322003841400146\n",
      "Epoch 7389/10000, Training Loss: 0.6768550872802734, Validation Loss: 0.6965301632881165\n",
      "Epoch 7390/10000, Training Loss: 0.6335697770118713, Validation Loss: 0.796179473400116\n",
      "Epoch 7391/10000, Training Loss: 0.6467809081077576, Validation Loss: 0.5854487419128418\n",
      "Epoch 7392/10000, Training Loss: 0.639967679977417, Validation Loss: 0.6985757350921631\n",
      "Epoch 7393/10000, Training Loss: 0.659885048866272, Validation Loss: 0.6984050869941711\n",
      "Epoch 7394/10000, Training Loss: 0.6529866456985474, Validation Loss: 1.00983726978302\n",
      "Epoch 7395/10000, Training Loss: 0.628006637096405, Validation Loss: 0.7021016478538513\n",
      "Epoch 7396/10000, Training Loss: 0.6311531662940979, Validation Loss: 0.6575462818145752\n",
      "Epoch 7397/10000, Training Loss: 0.599005937576294, Validation Loss: 0.5489684343338013\n",
      "Epoch 7398/10000, Training Loss: 0.6482124328613281, Validation Loss: 0.608414351940155\n",
      "Epoch 7399/10000, Training Loss: 0.6380101442337036, Validation Loss: 0.5605244040489197\n",
      "Epoch 7400/10000, Training Loss: 0.6742994785308838, Validation Loss: 0.6720681190490723\n",
      "Epoch 7401/10000, Training Loss: 0.6577404737472534, Validation Loss: 0.6905032992362976\n",
      "Epoch 7402/10000, Training Loss: 0.6782001256942749, Validation Loss: 0.7003578543663025\n",
      "Epoch 7403/10000, Training Loss: 0.6878188848495483, Validation Loss: 0.6568823456764221\n",
      "Epoch 7404/10000, Training Loss: 0.6690041422843933, Validation Loss: 0.7433174252510071\n",
      "Epoch 7405/10000, Training Loss: 0.6290587186813354, Validation Loss: 0.5283290147781372\n",
      "Epoch 7406/10000, Training Loss: 0.673455536365509, Validation Loss: 0.6103326678276062\n",
      "Epoch 7407/10000, Training Loss: 0.6520010828971863, Validation Loss: 0.6774111390113831\n",
      "Epoch 7408/10000, Training Loss: 0.6535661220550537, Validation Loss: 0.9961502552032471\n",
      "Epoch 7409/10000, Training Loss: 0.6363251805305481, Validation Loss: 0.7239980101585388\n",
      "Epoch 7410/10000, Training Loss: 0.6510812044143677, Validation Loss: 0.7729538083076477\n",
      "Epoch 7411/10000, Training Loss: 0.6857712864875793, Validation Loss: 0.6750583052635193\n",
      "Epoch 7412/10000, Training Loss: 0.6718064546585083, Validation Loss: 0.7591102123260498\n",
      "Epoch 7413/10000, Training Loss: 0.6557769775390625, Validation Loss: 0.6880555748939514\n",
      "Epoch 7414/10000, Training Loss: 0.6431271433830261, Validation Loss: 0.6129008531570435\n",
      "Epoch 7415/10000, Training Loss: 0.671458899974823, Validation Loss: 0.6366426944732666\n",
      "Epoch 7416/10000, Training Loss: 0.6525024175643921, Validation Loss: 0.8064956068992615\n",
      "Epoch 7417/10000, Training Loss: 0.6489858627319336, Validation Loss: 0.82126784324646\n",
      "Epoch 7418/10000, Training Loss: 0.611519455909729, Validation Loss: 0.8712408542633057\n",
      "Epoch 7419/10000, Training Loss: 0.6281924843788147, Validation Loss: 0.5299075245857239\n",
      "Epoch 7420/10000, Training Loss: 0.6835451722145081, Validation Loss: 0.6233444809913635\n",
      "Epoch 7421/10000, Training Loss: 0.6486117243766785, Validation Loss: 0.6849103569984436\n",
      "Epoch 7422/10000, Training Loss: 0.6665270924568176, Validation Loss: 0.676968514919281\n",
      "Epoch 7423/10000, Training Loss: 0.6500619649887085, Validation Loss: 0.738710880279541\n",
      "Epoch 7424/10000, Training Loss: 0.7011836171150208, Validation Loss: 0.5341947078704834\n",
      "Epoch 7425/10000, Training Loss: 0.6711736917495728, Validation Loss: 0.5496416091918945\n",
      "Epoch 7426/10000, Training Loss: 0.6960595846176147, Validation Loss: 0.7820332646369934\n",
      "Epoch 7427/10000, Training Loss: 0.599303662776947, Validation Loss: 0.5133485794067383\n",
      "Epoch 7428/10000, Training Loss: 0.6524320244789124, Validation Loss: 0.7388467788696289\n",
      "Epoch 7429/10000, Training Loss: 0.6964864134788513, Validation Loss: 0.7226808071136475\n",
      "Epoch 7430/10000, Training Loss: 0.6860828995704651, Validation Loss: 0.7180569171905518\n",
      "Epoch 7431/10000, Training Loss: 0.6133494973182678, Validation Loss: 0.8008221983909607\n",
      "Epoch 7432/10000, Training Loss: 0.6767475605010986, Validation Loss: 0.6662656664848328\n",
      "Epoch 7433/10000, Training Loss: 0.6827553510665894, Validation Loss: 0.6684959530830383\n",
      "Epoch 7434/10000, Training Loss: 0.6557855010032654, Validation Loss: 0.7040316462516785\n",
      "Epoch 7435/10000, Training Loss: 0.6325876712799072, Validation Loss: 0.7060306072235107\n",
      "Epoch 7436/10000, Training Loss: 0.678843080997467, Validation Loss: 0.6898207068443298\n",
      "Epoch 7437/10000, Training Loss: 0.6608668565750122, Validation Loss: 0.8069233298301697\n",
      "Epoch 7438/10000, Training Loss: 0.6806442737579346, Validation Loss: 0.7616918683052063\n",
      "Epoch 7439/10000, Training Loss: 0.6396885514259338, Validation Loss: 0.5265612006187439\n",
      "Epoch 7440/10000, Training Loss: 0.6776852011680603, Validation Loss: 0.6647177338600159\n",
      "Epoch 7441/10000, Training Loss: 0.6482877731323242, Validation Loss: 0.7133355140686035\n",
      "Epoch 7442/10000, Training Loss: 0.6827118396759033, Validation Loss: 0.6669736504554749\n",
      "Epoch 7443/10000, Training Loss: 0.6320507526397705, Validation Loss: 0.6483368277549744\n",
      "Epoch 7444/10000, Training Loss: 0.6596047282218933, Validation Loss: 0.5773076415061951\n",
      "Epoch 7445/10000, Training Loss: 0.6425732374191284, Validation Loss: 0.6176279187202454\n",
      "Epoch 7446/10000, Training Loss: 0.6371978521347046, Validation Loss: 0.5852153897285461\n",
      "Epoch 7447/10000, Training Loss: 0.6374445557594299, Validation Loss: 0.6582165956497192\n",
      "Epoch 7448/10000, Training Loss: 0.6288250684738159, Validation Loss: 0.6606011986732483\n",
      "Epoch 7449/10000, Training Loss: 0.6733661890029907, Validation Loss: 0.533427357673645\n",
      "Epoch 7450/10000, Training Loss: 0.6511737108230591, Validation Loss: 0.7810586094856262\n",
      "Epoch 7451/10000, Training Loss: 0.6821365356445312, Validation Loss: 0.7695443630218506\n",
      "Epoch 7452/10000, Training Loss: 0.6525375843048096, Validation Loss: 0.683068573474884\n",
      "Epoch 7453/10000, Training Loss: 0.6742086410522461, Validation Loss: 0.6080125570297241\n",
      "Epoch 7454/10000, Training Loss: 0.6287230849266052, Validation Loss: 1.02671217918396\n",
      "Epoch 7455/10000, Training Loss: 0.6343562006950378, Validation Loss: 0.6803544163703918\n",
      "Epoch 7456/10000, Training Loss: 0.6495484709739685, Validation Loss: 0.9320648312568665\n",
      "Epoch 7457/10000, Training Loss: 0.6733952164649963, Validation Loss: 0.7671208381652832\n",
      "Epoch 7458/10000, Training Loss: 0.6302270293235779, Validation Loss: 0.6977939009666443\n",
      "Epoch 7459/10000, Training Loss: 0.6474323868751526, Validation Loss: 0.6752057075500488\n",
      "Epoch 7460/10000, Training Loss: 0.6584959030151367, Validation Loss: 0.7110090851783752\n",
      "Epoch 7461/10000, Training Loss: 0.6481587886810303, Validation Loss: 0.7554956078529358\n",
      "Epoch 7462/10000, Training Loss: 0.6332025527954102, Validation Loss: 0.8520814776420593\n",
      "Epoch 7463/10000, Training Loss: 0.6579195857048035, Validation Loss: 0.4553474485874176\n",
      "Epoch 7464/10000, Training Loss: 0.661422073841095, Validation Loss: 0.7283837199211121\n",
      "Epoch 7465/10000, Training Loss: 0.6691322922706604, Validation Loss: 0.657767117023468\n",
      "Epoch 7466/10000, Training Loss: 0.6412093639373779, Validation Loss: 0.6007382869720459\n",
      "Epoch 7467/10000, Training Loss: 0.6808178424835205, Validation Loss: 0.6411413550376892\n",
      "Epoch 7468/10000, Training Loss: 0.6829416751861572, Validation Loss: 0.6979809403419495\n",
      "Epoch 7469/10000, Training Loss: 0.659058690071106, Validation Loss: 1.0402153730392456\n",
      "Epoch 7470/10000, Training Loss: 0.6738450527191162, Validation Loss: 0.8180568814277649\n",
      "Epoch 7471/10000, Training Loss: 0.6745942831039429, Validation Loss: 0.6757338047027588\n",
      "Epoch 7472/10000, Training Loss: 0.615520179271698, Validation Loss: 0.7259156703948975\n",
      "Epoch 7473/10000, Training Loss: 0.648115873336792, Validation Loss: 0.8826707005500793\n",
      "Epoch 7474/10000, Training Loss: 0.6773779988288879, Validation Loss: 0.7581498026847839\n",
      "Epoch 7475/10000, Training Loss: 0.6246347427368164, Validation Loss: 0.7132382392883301\n",
      "Epoch 7476/10000, Training Loss: 0.6719906330108643, Validation Loss: 0.8518933653831482\n",
      "Epoch 7477/10000, Training Loss: 0.6398234963417053, Validation Loss: 0.7139571309089661\n",
      "Epoch 7478/10000, Training Loss: 0.6687241792678833, Validation Loss: 0.6201602816581726\n",
      "Epoch 7479/10000, Training Loss: 0.6573069095611572, Validation Loss: 0.6773456931114197\n",
      "Epoch 7480/10000, Training Loss: 0.6851548552513123, Validation Loss: 0.6081234812736511\n",
      "Epoch 7481/10000, Training Loss: 0.67878258228302, Validation Loss: 0.6906188130378723\n",
      "Epoch 7482/10000, Training Loss: 0.6117750406265259, Validation Loss: 0.7215090394020081\n",
      "Epoch 7483/10000, Training Loss: 0.6839560866355896, Validation Loss: 0.35913267731666565\n",
      "Epoch 7484/10000, Training Loss: 0.6611096262931824, Validation Loss: 1.2367545366287231\n",
      "Epoch 7485/10000, Training Loss: 0.6548817157745361, Validation Loss: 0.9306159019470215\n",
      "Epoch 7486/10000, Training Loss: 0.6546908617019653, Validation Loss: 0.7459618449211121\n",
      "Epoch 7487/10000, Training Loss: 0.6568701267242432, Validation Loss: 0.6890178322792053\n",
      "Epoch 7488/10000, Training Loss: 0.6522274613380432, Validation Loss: 0.6338716745376587\n",
      "Epoch 7489/10000, Training Loss: 0.6622445583343506, Validation Loss: 0.8133959174156189\n",
      "Epoch 7490/10000, Training Loss: 0.6354873180389404, Validation Loss: 0.6651110053062439\n",
      "Epoch 7491/10000, Training Loss: 0.6293259263038635, Validation Loss: 0.7958579659461975\n",
      "Epoch 7492/10000, Training Loss: 0.6268187165260315, Validation Loss: 0.6278979778289795\n",
      "Epoch 7493/10000, Training Loss: 0.607647716999054, Validation Loss: 0.7402892112731934\n",
      "Epoch 7494/10000, Training Loss: 0.6480795741081238, Validation Loss: 0.7201163172721863\n",
      "Epoch 7495/10000, Training Loss: 0.6620197892189026, Validation Loss: 0.8975456357002258\n",
      "Epoch 7496/10000, Training Loss: 0.6726483702659607, Validation Loss: 0.7501181960105896\n",
      "Epoch 7497/10000, Training Loss: 0.69376140832901, Validation Loss: 0.9327316880226135\n",
      "Epoch 7498/10000, Training Loss: 0.6324450969696045, Validation Loss: 0.6352600455284119\n",
      "Epoch 7499/10000, Training Loss: 0.6847448945045471, Validation Loss: 0.4720979928970337\n",
      "Epoch 7500/10000, Training Loss: 0.6663635969161987, Validation Loss: 0.5901122689247131\n",
      "Epoch 7501/10000, Training Loss: 0.6706701517105103, Validation Loss: 0.7845039963722229\n",
      "Epoch 7502/10000, Training Loss: 0.6087325811386108, Validation Loss: 0.6211525797843933\n",
      "Epoch 7503/10000, Training Loss: 0.7043673396110535, Validation Loss: 0.4291500747203827\n",
      "Epoch 7504/10000, Training Loss: 0.6663001179695129, Validation Loss: 0.6772308349609375\n",
      "Epoch 7505/10000, Training Loss: 0.6683399677276611, Validation Loss: 0.6465035676956177\n",
      "Epoch 7506/10000, Training Loss: 0.6253026127815247, Validation Loss: 0.6919184327125549\n",
      "Epoch 7507/10000, Training Loss: 0.6483472585678101, Validation Loss: 0.7058172821998596\n",
      "Epoch 7508/10000, Training Loss: 0.6313657164573669, Validation Loss: 0.6764042973518372\n",
      "Epoch 7509/10000, Training Loss: 0.6270377039909363, Validation Loss: 0.6440545320510864\n",
      "Epoch 7510/10000, Training Loss: 0.6627597212791443, Validation Loss: 0.583552360534668\n",
      "Epoch 7511/10000, Training Loss: 0.630459725856781, Validation Loss: 0.7905866503715515\n",
      "Epoch 7512/10000, Training Loss: 0.6214545965194702, Validation Loss: 0.7699742913246155\n",
      "Epoch 7513/10000, Training Loss: 0.6184715032577515, Validation Loss: 0.6696825623512268\n",
      "Epoch 7514/10000, Training Loss: 0.6160169839859009, Validation Loss: 0.6264365315437317\n",
      "Epoch 7515/10000, Training Loss: 0.6530987024307251, Validation Loss: 0.5681814551353455\n",
      "Epoch 7516/10000, Training Loss: 0.6524055004119873, Validation Loss: 0.581699788570404\n",
      "Epoch 7517/10000, Training Loss: 0.6179621815681458, Validation Loss: 0.752824604511261\n",
      "Epoch 7518/10000, Training Loss: 0.6674072742462158, Validation Loss: 0.6643829941749573\n",
      "Epoch 7519/10000, Training Loss: 0.6406916975975037, Validation Loss: 0.641819179058075\n",
      "Epoch 7520/10000, Training Loss: 0.6683940887451172, Validation Loss: 0.7495611310005188\n",
      "Epoch 7521/10000, Training Loss: 0.6710442304611206, Validation Loss: 0.7071905732154846\n",
      "Epoch 7522/10000, Training Loss: 0.6725249886512756, Validation Loss: 0.7485297322273254\n",
      "Epoch 7523/10000, Training Loss: 0.6819784641265869, Validation Loss: 0.7107391953468323\n",
      "Epoch 7524/10000, Training Loss: 0.610612690448761, Validation Loss: 0.5068583488464355\n",
      "Epoch 7525/10000, Training Loss: 0.6152971982955933, Validation Loss: 0.6226885914802551\n",
      "Epoch 7526/10000, Training Loss: 0.6472644805908203, Validation Loss: 0.8747920989990234\n",
      "Epoch 7527/10000, Training Loss: 0.662570595741272, Validation Loss: 0.6723076701164246\n",
      "Epoch 7528/10000, Training Loss: 0.6380729675292969, Validation Loss: 0.5984438061714172\n",
      "Epoch 7529/10000, Training Loss: 0.6340116858482361, Validation Loss: 0.7363144755363464\n",
      "Epoch 7530/10000, Training Loss: 0.6639301776885986, Validation Loss: 0.6206386685371399\n",
      "Epoch 7531/10000, Training Loss: 0.6638892292976379, Validation Loss: 0.558009922504425\n",
      "Epoch 7532/10000, Training Loss: 0.6474757790565491, Validation Loss: 0.8525959849357605\n",
      "Epoch 7533/10000, Training Loss: 0.6337184309959412, Validation Loss: 0.7548995614051819\n",
      "Epoch 7534/10000, Training Loss: 0.6621461510658264, Validation Loss: 0.5606275200843811\n",
      "Epoch 7535/10000, Training Loss: 0.66241455078125, Validation Loss: 0.6774092316627502\n",
      "Epoch 7536/10000, Training Loss: 0.6624473333358765, Validation Loss: 0.6585776209831238\n",
      "Epoch 7537/10000, Training Loss: 0.6257320046424866, Validation Loss: 0.6112237572669983\n",
      "Epoch 7538/10000, Training Loss: 0.6308444738388062, Validation Loss: 0.8027563095092773\n",
      "Epoch 7539/10000, Training Loss: 0.6296831369400024, Validation Loss: 0.44919636845588684\n",
      "Epoch 7540/10000, Training Loss: 0.6444903612136841, Validation Loss: 0.6609236598014832\n",
      "Epoch 7541/10000, Training Loss: 0.7073764801025391, Validation Loss: 0.8555707931518555\n",
      "Epoch 7542/10000, Training Loss: 0.640522837638855, Validation Loss: 0.655403196811676\n",
      "Epoch 7543/10000, Training Loss: 0.6365372538566589, Validation Loss: 0.7081983685493469\n",
      "Epoch 7544/10000, Training Loss: 0.6650238037109375, Validation Loss: 0.6964707374572754\n",
      "Epoch 7545/10000, Training Loss: 0.6306104063987732, Validation Loss: 0.7497754693031311\n",
      "Epoch 7546/10000, Training Loss: 0.629539430141449, Validation Loss: 0.5358831286430359\n",
      "Epoch 7547/10000, Training Loss: 0.6827468276023865, Validation Loss: 1.0869752168655396\n",
      "Epoch 7548/10000, Training Loss: 0.6704181432723999, Validation Loss: 0.6718614101409912\n",
      "Epoch 7549/10000, Training Loss: 0.625992476940155, Validation Loss: 0.7129303812980652\n",
      "Epoch 7550/10000, Training Loss: 0.6403226256370544, Validation Loss: 0.6859204769134521\n",
      "Epoch 7551/10000, Training Loss: 0.6608569622039795, Validation Loss: 0.5681549906730652\n",
      "Epoch 7552/10000, Training Loss: 0.6743885278701782, Validation Loss: 0.5824863910675049\n",
      "Epoch 7553/10000, Training Loss: 0.6259143352508545, Validation Loss: 0.48621633648872375\n",
      "Epoch 7554/10000, Training Loss: 0.6847830414772034, Validation Loss: 0.7728590965270996\n",
      "Epoch 7555/10000, Training Loss: 0.6169811487197876, Validation Loss: 0.48305392265319824\n",
      "Epoch 7556/10000, Training Loss: 0.6449206471443176, Validation Loss: 0.7140558362007141\n",
      "Epoch 7557/10000, Training Loss: 0.6301120519638062, Validation Loss: 0.5883545875549316\n",
      "Epoch 7558/10000, Training Loss: 0.7113484144210815, Validation Loss: 0.7031273245811462\n",
      "Epoch 7559/10000, Training Loss: 0.6237053275108337, Validation Loss: 0.6913049817085266\n",
      "Epoch 7560/10000, Training Loss: 0.6622965335845947, Validation Loss: 0.6876047253608704\n",
      "Epoch 7561/10000, Training Loss: 0.6676731109619141, Validation Loss: 0.601142168045044\n",
      "Epoch 7562/10000, Training Loss: 0.6158158779144287, Validation Loss: 0.46321412920951843\n",
      "Epoch 7563/10000, Training Loss: 0.6388589143753052, Validation Loss: 0.61117023229599\n",
      "Epoch 7564/10000, Training Loss: 0.6413534879684448, Validation Loss: 0.6813510060310364\n",
      "Epoch 7565/10000, Training Loss: 0.6478146910667419, Validation Loss: 0.6452719569206238\n",
      "Epoch 7566/10000, Training Loss: 0.6765413284301758, Validation Loss: 0.845081090927124\n",
      "Epoch 7567/10000, Training Loss: 0.6691548228263855, Validation Loss: 0.667710542678833\n",
      "Epoch 7568/10000, Training Loss: 0.6198335886001587, Validation Loss: 0.6350448131561279\n",
      "Epoch 7569/10000, Training Loss: 0.6235084533691406, Validation Loss: 0.711716890335083\n",
      "Epoch 7570/10000, Training Loss: 0.6437237858772278, Validation Loss: 0.5365654826164246\n",
      "Epoch 7571/10000, Training Loss: 0.6502364873886108, Validation Loss: 0.5746259093284607\n",
      "Epoch 7572/10000, Training Loss: 0.6627986431121826, Validation Loss: 0.6287510991096497\n",
      "Epoch 7573/10000, Training Loss: 0.6491900682449341, Validation Loss: 0.5692186951637268\n",
      "Epoch 7574/10000, Training Loss: 0.6622124314308167, Validation Loss: 0.7269024848937988\n",
      "Epoch 7575/10000, Training Loss: 0.6356079578399658, Validation Loss: 0.6983391642570496\n",
      "Epoch 7576/10000, Training Loss: 0.6886845827102661, Validation Loss: 0.9317560195922852\n",
      "Epoch 7577/10000, Training Loss: 0.6385886669158936, Validation Loss: 0.881946861743927\n",
      "Epoch 7578/10000, Training Loss: 0.6576486825942993, Validation Loss: 0.5811607241630554\n",
      "Epoch 7579/10000, Training Loss: 0.6308937072753906, Validation Loss: 0.8790368437767029\n",
      "Epoch 7580/10000, Training Loss: 0.6209713816642761, Validation Loss: 0.5822662711143494\n",
      "Epoch 7581/10000, Training Loss: 0.6190257668495178, Validation Loss: 0.7752342224121094\n",
      "Epoch 7582/10000, Training Loss: 0.6796541213989258, Validation Loss: 0.7724420428276062\n",
      "Epoch 7583/10000, Training Loss: 0.6663230657577515, Validation Loss: 0.602624237537384\n",
      "Epoch 7584/10000, Training Loss: 0.6585232019424438, Validation Loss: 0.6833731532096863\n",
      "Epoch 7585/10000, Training Loss: 0.6355897188186646, Validation Loss: 0.7118055820465088\n",
      "Epoch 7586/10000, Training Loss: 0.672033429145813, Validation Loss: 0.744147002696991\n",
      "Epoch 7587/10000, Training Loss: 0.6511484384536743, Validation Loss: 0.6516197323799133\n",
      "Epoch 7588/10000, Training Loss: 0.6107982397079468, Validation Loss: 0.7503862380981445\n",
      "Epoch 7589/10000, Training Loss: 0.6921694278717041, Validation Loss: 0.7497808337211609\n",
      "Epoch 7590/10000, Training Loss: 0.6540294289588928, Validation Loss: 0.6304191946983337\n",
      "Epoch 7591/10000, Training Loss: 0.6552597880363464, Validation Loss: 0.6413261294364929\n",
      "Epoch 7592/10000, Training Loss: 0.6764222383499146, Validation Loss: 0.6978145241737366\n",
      "Epoch 7593/10000, Training Loss: 0.6651356220245361, Validation Loss: 0.7048029899597168\n",
      "Epoch 7594/10000, Training Loss: 0.6504576206207275, Validation Loss: 0.6219108700752258\n",
      "Epoch 7595/10000, Training Loss: 0.6683012843132019, Validation Loss: 0.6672784686088562\n",
      "Epoch 7596/10000, Training Loss: 0.6175689697265625, Validation Loss: 0.6910199522972107\n",
      "Epoch 7597/10000, Training Loss: 0.6559134125709534, Validation Loss: 0.5735957026481628\n",
      "Epoch 7598/10000, Training Loss: 0.6533118486404419, Validation Loss: 0.6380882263183594\n",
      "Epoch 7599/10000, Training Loss: 0.6669905185699463, Validation Loss: 0.7586150169372559\n",
      "Epoch 7600/10000, Training Loss: 0.620235800743103, Validation Loss: 0.8491613268852234\n",
      "Epoch 7601/10000, Training Loss: 0.590961217880249, Validation Loss: 0.5090257525444031\n",
      "Epoch 7602/10000, Training Loss: 0.6218429207801819, Validation Loss: 0.48030009865760803\n",
      "Epoch 7603/10000, Training Loss: 0.6803030371665955, Validation Loss: 0.5503162741661072\n",
      "Epoch 7604/10000, Training Loss: 0.6159323453903198, Validation Loss: 0.7261686325073242\n",
      "Epoch 7605/10000, Training Loss: 0.6242297291755676, Validation Loss: 0.8108274340629578\n",
      "Epoch 7606/10000, Training Loss: 0.6254972219467163, Validation Loss: 0.7395679950714111\n",
      "Epoch 7607/10000, Training Loss: 0.6504668593406677, Validation Loss: 0.6192185282707214\n",
      "Epoch 7608/10000, Training Loss: 0.6507412195205688, Validation Loss: 0.5339378714561462\n",
      "Epoch 7609/10000, Training Loss: 0.7075035572052002, Validation Loss: 0.9334499835968018\n",
      "Epoch 7610/10000, Training Loss: 0.6337096691131592, Validation Loss: 0.632413387298584\n",
      "Epoch 7611/10000, Training Loss: 0.6451810002326965, Validation Loss: 0.47883978486061096\n",
      "Epoch 7612/10000, Training Loss: 0.6436929702758789, Validation Loss: 0.9150468707084656\n",
      "Epoch 7613/10000, Training Loss: 0.6453752517700195, Validation Loss: 0.6349031329154968\n",
      "Epoch 7614/10000, Training Loss: 0.6588534712791443, Validation Loss: 0.6300029158592224\n",
      "Epoch 7615/10000, Training Loss: 0.6465782523155212, Validation Loss: 0.7452672123908997\n",
      "Epoch 7616/10000, Training Loss: 0.7143551707267761, Validation Loss: 0.6982176303863525\n",
      "Epoch 7617/10000, Training Loss: 0.6501255631446838, Validation Loss: 0.5950514674186707\n",
      "Epoch 7618/10000, Training Loss: 0.6440281271934509, Validation Loss: 0.7055597901344299\n",
      "Epoch 7619/10000, Training Loss: 0.6002310514450073, Validation Loss: 0.7826993465423584\n",
      "Epoch 7620/10000, Training Loss: 0.6569554805755615, Validation Loss: 0.5877965092658997\n",
      "Epoch 7621/10000, Training Loss: 0.6616398692131042, Validation Loss: 0.6415545344352722\n",
      "Epoch 7622/10000, Training Loss: 0.6545663475990295, Validation Loss: 0.7121031284332275\n",
      "Epoch 7623/10000, Training Loss: 0.6490221619606018, Validation Loss: 0.7626922130584717\n",
      "Epoch 7624/10000, Training Loss: 0.6215115189552307, Validation Loss: 0.7259294390678406\n",
      "Epoch 7625/10000, Training Loss: 0.643151044845581, Validation Loss: 0.594967246055603\n",
      "Epoch 7626/10000, Training Loss: 0.7120051980018616, Validation Loss: 0.7986962199211121\n",
      "Epoch 7627/10000, Training Loss: 0.626813530921936, Validation Loss: 0.6702939867973328\n",
      "Epoch 7628/10000, Training Loss: 0.6547275185585022, Validation Loss: 0.8707646727561951\n",
      "Epoch 7629/10000, Training Loss: 0.6512613892555237, Validation Loss: 0.692941427230835\n",
      "Epoch 7630/10000, Training Loss: 0.6143316030502319, Validation Loss: 0.7171481251716614\n",
      "Epoch 7631/10000, Training Loss: 0.6775075197219849, Validation Loss: 0.7036139369010925\n",
      "Epoch 7632/10000, Training Loss: 0.6401197910308838, Validation Loss: 0.7277142405509949\n",
      "Epoch 7633/10000, Training Loss: 0.6065717339515686, Validation Loss: 0.7001391053199768\n",
      "Epoch 7634/10000, Training Loss: 0.6506818532943726, Validation Loss: 0.687471330165863\n",
      "Epoch 7635/10000, Training Loss: 0.6235418319702148, Validation Loss: 0.7905136942863464\n",
      "Epoch 7636/10000, Training Loss: 0.6614196300506592, Validation Loss: 0.7110514044761658\n",
      "Epoch 7637/10000, Training Loss: 0.6523231863975525, Validation Loss: 0.7355342507362366\n",
      "Epoch 7638/10000, Training Loss: 0.6645992994308472, Validation Loss: 0.8223443031311035\n",
      "Epoch 7639/10000, Training Loss: 0.6424804329872131, Validation Loss: 0.670008659362793\n",
      "Epoch 7640/10000, Training Loss: 0.6709073781967163, Validation Loss: 0.5616238713264465\n",
      "Epoch 7641/10000, Training Loss: 0.6759669780731201, Validation Loss: 0.725879430770874\n",
      "Epoch 7642/10000, Training Loss: 0.6486895084381104, Validation Loss: 0.515572726726532\n",
      "Epoch 7643/10000, Training Loss: 0.6457469463348389, Validation Loss: 0.5961605906486511\n",
      "Epoch 7644/10000, Training Loss: 0.6757863163948059, Validation Loss: 0.8163971304893494\n",
      "Epoch 7645/10000, Training Loss: 0.6274022459983826, Validation Loss: 0.6473067402839661\n",
      "Epoch 7646/10000, Training Loss: 0.6443163156509399, Validation Loss: 0.8894783854484558\n",
      "Epoch 7647/10000, Training Loss: 0.6379969716072083, Validation Loss: 0.6482176184654236\n",
      "Epoch 7648/10000, Training Loss: 0.6536072492599487, Validation Loss: 0.7515540719032288\n",
      "Epoch 7649/10000, Training Loss: 0.6392351388931274, Validation Loss: 0.8176746368408203\n",
      "Epoch 7650/10000, Training Loss: 0.611636757850647, Validation Loss: 0.5642542243003845\n",
      "Epoch 7651/10000, Training Loss: 0.6428745985031128, Validation Loss: 0.7329398989677429\n",
      "Epoch 7652/10000, Training Loss: 0.6385619044303894, Validation Loss: 0.6014300584793091\n",
      "Epoch 7653/10000, Training Loss: 0.6434579491615295, Validation Loss: 0.7815639972686768\n",
      "Epoch 7654/10000, Training Loss: 0.664107620716095, Validation Loss: 0.592708170413971\n",
      "Epoch 7655/10000, Training Loss: 0.6296453475952148, Validation Loss: 0.5651789307594299\n",
      "Epoch 7656/10000, Training Loss: 0.6474889516830444, Validation Loss: 0.9277922511100769\n",
      "Epoch 7657/10000, Training Loss: 0.6338932514190674, Validation Loss: 0.7155893445014954\n",
      "Epoch 7658/10000, Training Loss: 0.6523844599723816, Validation Loss: 0.6226372122764587\n",
      "Epoch 7659/10000, Training Loss: 0.6624419093132019, Validation Loss: 0.8403280377388\n",
      "Epoch 7660/10000, Training Loss: 0.6675469279289246, Validation Loss: 0.6050284504890442\n",
      "Epoch 7661/10000, Training Loss: 0.6917446851730347, Validation Loss: 0.8889550566673279\n",
      "Epoch 7662/10000, Training Loss: 0.6734100580215454, Validation Loss: 0.8784928321838379\n",
      "Epoch 7663/10000, Training Loss: 0.662861168384552, Validation Loss: 0.6759399771690369\n",
      "Epoch 7664/10000, Training Loss: 0.6579723358154297, Validation Loss: 0.8156265616416931\n",
      "Epoch 7665/10000, Training Loss: 0.6875442862510681, Validation Loss: 0.7292527556419373\n",
      "Epoch 7666/10000, Training Loss: 0.6634533405303955, Validation Loss: 0.5336628556251526\n",
      "Epoch 7667/10000, Training Loss: 0.665957510471344, Validation Loss: 0.793058454990387\n",
      "Epoch 7668/10000, Training Loss: 0.6887627840042114, Validation Loss: 1.1377391815185547\n",
      "Epoch 7669/10000, Training Loss: 0.637201726436615, Validation Loss: 0.84145188331604\n",
      "Epoch 7670/10000, Training Loss: 0.6485514044761658, Validation Loss: 0.41150161623954773\n",
      "Epoch 7671/10000, Training Loss: 0.6591492295265198, Validation Loss: 0.6070913672447205\n",
      "Epoch 7672/10000, Training Loss: 0.6263462901115417, Validation Loss: 0.6978160738945007\n",
      "Epoch 7673/10000, Training Loss: 0.611291766166687, Validation Loss: 0.4925372302532196\n",
      "Epoch 7674/10000, Training Loss: 0.6721810102462769, Validation Loss: 0.6764857172966003\n",
      "Epoch 7675/10000, Training Loss: 0.6856441497802734, Validation Loss: 0.7647516131401062\n",
      "Epoch 7676/10000, Training Loss: 0.6709628701210022, Validation Loss: 0.6535916924476624\n",
      "Epoch 7677/10000, Training Loss: 0.6842404007911682, Validation Loss: 0.5565081834793091\n",
      "Epoch 7678/10000, Training Loss: 0.6616596579551697, Validation Loss: 0.7164933085441589\n",
      "Epoch 7679/10000, Training Loss: 0.6459783911705017, Validation Loss: 0.5980860590934753\n",
      "Epoch 7680/10000, Training Loss: 0.6508774161338806, Validation Loss: 0.6416056752204895\n",
      "Epoch 7681/10000, Training Loss: 0.6474379301071167, Validation Loss: 0.6213387846946716\n",
      "Epoch 7682/10000, Training Loss: 0.646016538143158, Validation Loss: 0.575347363948822\n",
      "Epoch 7683/10000, Training Loss: 0.6278808116912842, Validation Loss: 0.7900025248527527\n",
      "Epoch 7684/10000, Training Loss: 0.6811226606369019, Validation Loss: 0.6992622017860413\n",
      "Epoch 7685/10000, Training Loss: 0.6645426154136658, Validation Loss: 0.6827216744422913\n",
      "Epoch 7686/10000, Training Loss: 0.6318731307983398, Validation Loss: 0.6770427823066711\n",
      "Epoch 7687/10000, Training Loss: 0.6541274189949036, Validation Loss: 0.6223273873329163\n",
      "Epoch 7688/10000, Training Loss: 0.6575384736061096, Validation Loss: 0.6836549639701843\n",
      "Epoch 7689/10000, Training Loss: 0.6734222173690796, Validation Loss: 0.639494776725769\n",
      "Epoch 7690/10000, Training Loss: 0.6353184580802917, Validation Loss: 0.5926610827445984\n",
      "Epoch 7691/10000, Training Loss: 0.6420873999595642, Validation Loss: 0.8057792782783508\n",
      "Epoch 7692/10000, Training Loss: 0.6570113897323608, Validation Loss: 0.6618501543998718\n",
      "Epoch 7693/10000, Training Loss: 0.6542237401008606, Validation Loss: 0.6037200689315796\n",
      "Epoch 7694/10000, Training Loss: 0.6640986800193787, Validation Loss: 0.6450801491737366\n",
      "Epoch 7695/10000, Training Loss: 0.6393714547157288, Validation Loss: 0.5868898034095764\n",
      "Epoch 7696/10000, Training Loss: 0.6620581746101379, Validation Loss: 0.7520163059234619\n",
      "Epoch 7697/10000, Training Loss: 0.6395953893661499, Validation Loss: 0.7080219388008118\n",
      "Epoch 7698/10000, Training Loss: 0.5886167287826538, Validation Loss: 0.837285041809082\n",
      "Epoch 7699/10000, Training Loss: 0.6753227710723877, Validation Loss: 0.9660994410514832\n",
      "Epoch 7700/10000, Training Loss: 0.6348178386688232, Validation Loss: 0.5009368658065796\n",
      "Epoch 7701/10000, Training Loss: 0.7049754858016968, Validation Loss: 0.6678282618522644\n",
      "Epoch 7702/10000, Training Loss: 0.5989596247673035, Validation Loss: 0.5391445755958557\n",
      "Epoch 7703/10000, Training Loss: 0.607659101486206, Validation Loss: 0.7651203274726868\n",
      "Epoch 7704/10000, Training Loss: 0.6647483110427856, Validation Loss: 1.0610560178756714\n",
      "Epoch 7705/10000, Training Loss: 0.6155901551246643, Validation Loss: 0.805634081363678\n",
      "Epoch 7706/10000, Training Loss: 0.6652311086654663, Validation Loss: 0.8048718571662903\n",
      "Epoch 7707/10000, Training Loss: 0.6558980941772461, Validation Loss: 0.7720497250556946\n",
      "Epoch 7708/10000, Training Loss: 0.6501525044441223, Validation Loss: 0.7057127356529236\n",
      "Epoch 7709/10000, Training Loss: 0.6330484747886658, Validation Loss: 0.727632999420166\n",
      "Epoch 7710/10000, Training Loss: 0.6244305372238159, Validation Loss: 0.6925923228263855\n",
      "Epoch 7711/10000, Training Loss: 0.6722939610481262, Validation Loss: 0.6355010867118835\n",
      "Epoch 7712/10000, Training Loss: 0.6449810862541199, Validation Loss: 0.6668196320533752\n",
      "Epoch 7713/10000, Training Loss: 0.6620054841041565, Validation Loss: 0.41381311416625977\n",
      "Epoch 7714/10000, Training Loss: 0.6774779558181763, Validation Loss: 0.7056090831756592\n",
      "Epoch 7715/10000, Training Loss: 0.6608451008796692, Validation Loss: 0.5911833047866821\n",
      "Epoch 7716/10000, Training Loss: 0.662112832069397, Validation Loss: 0.7552517056465149\n",
      "Epoch 7717/10000, Training Loss: 0.6550005674362183, Validation Loss: 0.5607431530952454\n",
      "Epoch 7718/10000, Training Loss: 0.6359864473342896, Validation Loss: 0.5292702317237854\n",
      "Epoch 7719/10000, Training Loss: 0.6877920627593994, Validation Loss: 0.6766586303710938\n",
      "Epoch 7720/10000, Training Loss: 0.6595043540000916, Validation Loss: 0.5667266249656677\n",
      "Epoch 7721/10000, Training Loss: 0.6606500148773193, Validation Loss: 0.8839275240898132\n",
      "Epoch 7722/10000, Training Loss: 0.6210495829582214, Validation Loss: 0.5859478712081909\n",
      "Epoch 7723/10000, Training Loss: 0.6705933213233948, Validation Loss: 0.7453272342681885\n",
      "Epoch 7724/10000, Training Loss: 0.6430472731590271, Validation Loss: 0.7319889664649963\n",
      "Epoch 7725/10000, Training Loss: 0.6551002264022827, Validation Loss: 0.7925145030021667\n",
      "Epoch 7726/10000, Training Loss: 0.647389829158783, Validation Loss: 0.7326993942260742\n",
      "Epoch 7727/10000, Training Loss: 0.6567302942276001, Validation Loss: 0.70597243309021\n",
      "Epoch 7728/10000, Training Loss: 0.6146442890167236, Validation Loss: 0.6475043892860413\n",
      "Epoch 7729/10000, Training Loss: 0.6495921611785889, Validation Loss: 0.6162926554679871\n",
      "Epoch 7730/10000, Training Loss: 0.7109517455101013, Validation Loss: 0.5000545382499695\n",
      "Epoch 7731/10000, Training Loss: 0.6655523777008057, Validation Loss: 0.5391918420791626\n",
      "Epoch 7732/10000, Training Loss: 0.6510620713233948, Validation Loss: 0.8491694927215576\n",
      "Epoch 7733/10000, Training Loss: 0.6745955348014832, Validation Loss: 0.6317625641822815\n",
      "Epoch 7734/10000, Training Loss: 0.643500030040741, Validation Loss: 0.7790679931640625\n",
      "Epoch 7735/10000, Training Loss: 0.6334741115570068, Validation Loss: 0.703791081905365\n",
      "Epoch 7736/10000, Training Loss: 0.668971598148346, Validation Loss: 0.7211845517158508\n",
      "Epoch 7737/10000, Training Loss: 0.6593307852745056, Validation Loss: 0.67411869764328\n",
      "Epoch 7738/10000, Training Loss: 0.6325310468673706, Validation Loss: 0.6125162243843079\n",
      "Epoch 7739/10000, Training Loss: 0.6240666508674622, Validation Loss: 0.777696430683136\n",
      "Epoch 7740/10000, Training Loss: 0.631632924079895, Validation Loss: 0.7303214073181152\n",
      "Epoch 7741/10000, Training Loss: 0.6566185355186462, Validation Loss: 0.6630304455757141\n",
      "Epoch 7742/10000, Training Loss: 0.653049111366272, Validation Loss: 0.9248730540275574\n",
      "Epoch 7743/10000, Training Loss: 0.617251455783844, Validation Loss: 0.7191062569618225\n",
      "Epoch 7744/10000, Training Loss: 0.6393557190895081, Validation Loss: 0.5388352870941162\n",
      "Epoch 7745/10000, Training Loss: 0.6704577207565308, Validation Loss: 0.69157475233078\n",
      "Epoch 7746/10000, Training Loss: 0.6272208094596863, Validation Loss: 0.6312516331672668\n",
      "Epoch 7747/10000, Training Loss: 0.653534471988678, Validation Loss: 0.7581415772438049\n",
      "Epoch 7748/10000, Training Loss: 0.683194100856781, Validation Loss: 0.8822973370552063\n",
      "Epoch 7749/10000, Training Loss: 0.6367383599281311, Validation Loss: 0.8053269982337952\n",
      "Epoch 7750/10000, Training Loss: 0.6204481720924377, Validation Loss: 0.9202128052711487\n",
      "Epoch 7751/10000, Training Loss: 0.650079071521759, Validation Loss: 0.7919650077819824\n",
      "Epoch 7752/10000, Training Loss: 0.6327890753746033, Validation Loss: 0.6255410313606262\n",
      "Epoch 7753/10000, Training Loss: 0.6382453441619873, Validation Loss: 0.6204501986503601\n",
      "Epoch 7754/10000, Training Loss: 0.6610504388809204, Validation Loss: 0.4559955894947052\n",
      "Epoch 7755/10000, Training Loss: 0.6539188623428345, Validation Loss: 0.43889299035072327\n",
      "Epoch 7756/10000, Training Loss: 0.651371419429779, Validation Loss: 0.6777684092521667\n",
      "Epoch 7757/10000, Training Loss: 0.6069777607917786, Validation Loss: 0.7565834522247314\n",
      "Epoch 7758/10000, Training Loss: 0.6190081238746643, Validation Loss: 0.5431855320930481\n",
      "Epoch 7759/10000, Training Loss: 0.624645471572876, Validation Loss: 0.7521755695343018\n",
      "Epoch 7760/10000, Training Loss: 0.6261897683143616, Validation Loss: 0.6082543730735779\n",
      "Epoch 7761/10000, Training Loss: 0.6287254691123962, Validation Loss: 0.8189125657081604\n",
      "Epoch 7762/10000, Training Loss: 0.6584553122520447, Validation Loss: 0.7341840863227844\n",
      "Epoch 7763/10000, Training Loss: 0.6533575654029846, Validation Loss: 0.5942732691764832\n",
      "Epoch 7764/10000, Training Loss: 0.6256036162376404, Validation Loss: 0.595899760723114\n",
      "Epoch 7765/10000, Training Loss: 0.6651740670204163, Validation Loss: 0.7353715300559998\n",
      "Epoch 7766/10000, Training Loss: 0.6597071290016174, Validation Loss: 0.5465940833091736\n",
      "Epoch 7767/10000, Training Loss: 0.6981820464134216, Validation Loss: 0.5367448925971985\n",
      "Epoch 7768/10000, Training Loss: 0.6359080076217651, Validation Loss: 0.7720253467559814\n",
      "Epoch 7769/10000, Training Loss: 0.6500942707061768, Validation Loss: 0.7247961163520813\n",
      "Epoch 7770/10000, Training Loss: 0.651012659072876, Validation Loss: 0.6019707322120667\n",
      "Epoch 7771/10000, Training Loss: 0.6392952799797058, Validation Loss: 0.5976948738098145\n",
      "Epoch 7772/10000, Training Loss: 0.6366634964942932, Validation Loss: 0.691777229309082\n",
      "Epoch 7773/10000, Training Loss: 0.6885671615600586, Validation Loss: 0.8041574954986572\n",
      "Epoch 7774/10000, Training Loss: 0.6422861814498901, Validation Loss: 0.5542595982551575\n",
      "Epoch 7775/10000, Training Loss: 0.6448377370834351, Validation Loss: 0.6635543704032898\n",
      "Epoch 7776/10000, Training Loss: 0.638367235660553, Validation Loss: 0.5889923572540283\n",
      "Epoch 7777/10000, Training Loss: 0.5970075726509094, Validation Loss: 0.5271069407463074\n",
      "Epoch 7778/10000, Training Loss: 0.6676909923553467, Validation Loss: 0.6830155849456787\n",
      "Epoch 7779/10000, Training Loss: 0.7081881165504456, Validation Loss: 0.6253179907798767\n",
      "Epoch 7780/10000, Training Loss: 0.6269481182098389, Validation Loss: 0.7109208106994629\n",
      "Epoch 7781/10000, Training Loss: 0.6392660140991211, Validation Loss: 0.6617069840431213\n",
      "Epoch 7782/10000, Training Loss: 0.6389280557632446, Validation Loss: 0.6051843762397766\n",
      "Epoch 7783/10000, Training Loss: 0.6288818717002869, Validation Loss: 0.8612773418426514\n",
      "Epoch 7784/10000, Training Loss: 0.6519298553466797, Validation Loss: 0.8221015334129333\n",
      "Epoch 7785/10000, Training Loss: 0.6400502920150757, Validation Loss: 0.714716374874115\n",
      "Epoch 7786/10000, Training Loss: 0.664473295211792, Validation Loss: 0.6434218287467957\n",
      "Epoch 7787/10000, Training Loss: 0.667439341545105, Validation Loss: 0.6824796795845032\n",
      "Epoch 7788/10000, Training Loss: 0.6725519895553589, Validation Loss: 0.6268470883369446\n",
      "Epoch 7789/10000, Training Loss: 0.6299469470977783, Validation Loss: 0.8001186847686768\n",
      "Epoch 7790/10000, Training Loss: 0.6446129679679871, Validation Loss: 0.6763122081756592\n",
      "Epoch 7791/10000, Training Loss: 0.6627302169799805, Validation Loss: 0.5743608474731445\n",
      "Epoch 7792/10000, Training Loss: 0.6547864675521851, Validation Loss: 0.6559494733810425\n",
      "Epoch 7793/10000, Training Loss: 0.6784957051277161, Validation Loss: 0.7032143473625183\n",
      "Epoch 7794/10000, Training Loss: 0.6498304605484009, Validation Loss: 1.0057724714279175\n",
      "Epoch 7795/10000, Training Loss: 0.6345067620277405, Validation Loss: 0.6524118781089783\n",
      "Epoch 7796/10000, Training Loss: 0.6506496071815491, Validation Loss: 0.6463750004768372\n",
      "Epoch 7797/10000, Training Loss: 0.6446093320846558, Validation Loss: 0.6976039409637451\n",
      "Epoch 7798/10000, Training Loss: 0.6160906553268433, Validation Loss: 0.6474074721336365\n",
      "Epoch 7799/10000, Training Loss: 0.6449171304702759, Validation Loss: 0.6669618487358093\n",
      "Epoch 7800/10000, Training Loss: 0.6699492931365967, Validation Loss: 0.51467365026474\n",
      "Epoch 7801/10000, Training Loss: 0.642608106136322, Validation Loss: 0.6024907827377319\n",
      "Epoch 7802/10000, Training Loss: 0.6631523370742798, Validation Loss: 0.667830228805542\n",
      "Epoch 7803/10000, Training Loss: 0.6964392066001892, Validation Loss: 1.3441416025161743\n",
      "Epoch 7804/10000, Training Loss: 0.6451082825660706, Validation Loss: 0.7869094014167786\n",
      "Epoch 7805/10000, Training Loss: 0.626294732093811, Validation Loss: 0.5190098285675049\n",
      "Epoch 7806/10000, Training Loss: 0.6485620141029358, Validation Loss: 0.7473801970481873\n",
      "Epoch 7807/10000, Training Loss: 0.646558403968811, Validation Loss: 0.6489429473876953\n",
      "Epoch 7808/10000, Training Loss: 0.6476010680198669, Validation Loss: 0.7199872136116028\n",
      "Epoch 7809/10000, Training Loss: 0.6539347767829895, Validation Loss: 0.5497161149978638\n",
      "Epoch 7810/10000, Training Loss: 0.6682738065719604, Validation Loss: 0.859982967376709\n",
      "Epoch 7811/10000, Training Loss: 0.6376017928123474, Validation Loss: 0.6947360038757324\n",
      "Epoch 7812/10000, Training Loss: 0.6391427516937256, Validation Loss: 0.6850132942199707\n",
      "Epoch 7813/10000, Training Loss: 0.6987684965133667, Validation Loss: 0.6607868075370789\n",
      "Epoch 7814/10000, Training Loss: 0.6196704506874084, Validation Loss: 0.8530468940734863\n",
      "Epoch 7815/10000, Training Loss: 0.6795664429664612, Validation Loss: 0.8203719258308411\n",
      "Epoch 7816/10000, Training Loss: 0.6589236855506897, Validation Loss: 0.8582663536071777\n",
      "Epoch 7817/10000, Training Loss: 0.6164427399635315, Validation Loss: 0.6042136549949646\n",
      "Epoch 7818/10000, Training Loss: 0.6366204619407654, Validation Loss: 0.7698493003845215\n",
      "Epoch 7819/10000, Training Loss: 0.6689494848251343, Validation Loss: 0.8393556475639343\n",
      "Epoch 7820/10000, Training Loss: 0.6471084356307983, Validation Loss: 0.5756370425224304\n",
      "Epoch 7821/10000, Training Loss: 0.616287887096405, Validation Loss: 0.6064286231994629\n",
      "Epoch 7822/10000, Training Loss: 0.6551742553710938, Validation Loss: 0.5911784172058105\n",
      "Epoch 7823/10000, Training Loss: 0.6003045439720154, Validation Loss: 0.6896921992301941\n",
      "Epoch 7824/10000, Training Loss: 0.6672546863555908, Validation Loss: 0.6003496646881104\n",
      "Epoch 7825/10000, Training Loss: 0.6577152609825134, Validation Loss: 0.6988482475280762\n",
      "Epoch 7826/10000, Training Loss: 0.6478809714317322, Validation Loss: 0.7486611008644104\n",
      "Epoch 7827/10000, Training Loss: 0.6432549357414246, Validation Loss: 0.8002374768257141\n",
      "Epoch 7828/10000, Training Loss: 0.616371214389801, Validation Loss: 0.610091507434845\n",
      "Epoch 7829/10000, Training Loss: 0.6527283191680908, Validation Loss: 0.8205191493034363\n",
      "Epoch 7830/10000, Training Loss: 0.6129778623580933, Validation Loss: 0.708729088306427\n",
      "Epoch 7831/10000, Training Loss: 0.644909143447876, Validation Loss: 0.9004576802253723\n",
      "Epoch 7832/10000, Training Loss: 0.62404865026474, Validation Loss: 0.7912666201591492\n",
      "Epoch 7833/10000, Training Loss: 0.6069916486740112, Validation Loss: 0.7473432421684265\n",
      "Epoch 7834/10000, Training Loss: 0.6728547215461731, Validation Loss: 0.9137794375419617\n",
      "Epoch 7835/10000, Training Loss: 0.6656358242034912, Validation Loss: 0.7524537444114685\n",
      "Epoch 7836/10000, Training Loss: 0.6439427733421326, Validation Loss: 0.7780347466468811\n",
      "Epoch 7837/10000, Training Loss: 0.63876873254776, Validation Loss: 0.7283806800842285\n",
      "Epoch 7838/10000, Training Loss: 0.6343200206756592, Validation Loss: 0.6126943230628967\n",
      "Epoch 7839/10000, Training Loss: 0.6263504028320312, Validation Loss: 0.6123730540275574\n",
      "Epoch 7840/10000, Training Loss: 0.6416196823120117, Validation Loss: 0.7196370959281921\n",
      "Epoch 7841/10000, Training Loss: 0.7343552112579346, Validation Loss: 0.7180109620094299\n",
      "Epoch 7842/10000, Training Loss: 0.5920650959014893, Validation Loss: 0.75496906042099\n",
      "Epoch 7843/10000, Training Loss: 0.6388906836509705, Validation Loss: 0.5293260812759399\n",
      "Epoch 7844/10000, Training Loss: 0.6452251672744751, Validation Loss: 0.7650067210197449\n",
      "Epoch 7845/10000, Training Loss: 0.6155680418014526, Validation Loss: 1.2010070085525513\n",
      "Epoch 7846/10000, Training Loss: 0.6644579172134399, Validation Loss: 0.664966881275177\n",
      "Epoch 7847/10000, Training Loss: 0.6496838331222534, Validation Loss: 0.7770387530326843\n",
      "Epoch 7848/10000, Training Loss: 0.654298722743988, Validation Loss: 0.717216968536377\n",
      "Epoch 7849/10000, Training Loss: 0.6773423552513123, Validation Loss: 0.6899373531341553\n",
      "Epoch 7850/10000, Training Loss: 0.6458310484886169, Validation Loss: 0.7926526665687561\n",
      "Epoch 7851/10000, Training Loss: 0.6621444225311279, Validation Loss: 0.6980339884757996\n",
      "Epoch 7852/10000, Training Loss: 0.6333246231079102, Validation Loss: 0.6308836340904236\n",
      "Epoch 7853/10000, Training Loss: 0.6367302536964417, Validation Loss: 0.6656026244163513\n",
      "Epoch 7854/10000, Training Loss: 0.5996586084365845, Validation Loss: 0.7344900965690613\n",
      "Epoch 7855/10000, Training Loss: 0.6678562760353088, Validation Loss: 0.525860071182251\n",
      "Epoch 7856/10000, Training Loss: 0.6661868691444397, Validation Loss: 0.7690686583518982\n",
      "Epoch 7857/10000, Training Loss: 0.6775785088539124, Validation Loss: 0.7105698585510254\n",
      "Epoch 7858/10000, Training Loss: 0.653480589389801, Validation Loss: 0.6308659911155701\n",
      "Epoch 7859/10000, Training Loss: 0.6278812885284424, Validation Loss: 0.6315205097198486\n",
      "Epoch 7860/10000, Training Loss: 0.6558417081832886, Validation Loss: 0.6436748504638672\n",
      "Epoch 7861/10000, Training Loss: 0.6375719308853149, Validation Loss: 0.7239037156105042\n",
      "Epoch 7862/10000, Training Loss: 0.6266855597496033, Validation Loss: 0.8131751418113708\n",
      "Epoch 7863/10000, Training Loss: 0.6293684244155884, Validation Loss: 0.7185872197151184\n",
      "Epoch 7864/10000, Training Loss: 0.6287654042243958, Validation Loss: 0.6185258626937866\n",
      "Epoch 7865/10000, Training Loss: 0.6870095133781433, Validation Loss: 0.7577334046363831\n",
      "Epoch 7866/10000, Training Loss: 0.6705743074417114, Validation Loss: 0.6776630282402039\n",
      "Epoch 7867/10000, Training Loss: 0.656680703163147, Validation Loss: 0.6827239394187927\n",
      "Epoch 7868/10000, Training Loss: 0.6546485424041748, Validation Loss: 0.6674820780754089\n",
      "Epoch 7869/10000, Training Loss: 0.6406819820404053, Validation Loss: 0.606315553188324\n",
      "Epoch 7870/10000, Training Loss: 0.6357218623161316, Validation Loss: 0.5751231908798218\n",
      "Epoch 7871/10000, Training Loss: 0.6366807222366333, Validation Loss: 0.7507882118225098\n",
      "Epoch 7872/10000, Training Loss: 0.6554911732673645, Validation Loss: 0.7643318176269531\n",
      "Epoch 7873/10000, Training Loss: 0.6526352763175964, Validation Loss: 0.8113484978675842\n",
      "Epoch 7874/10000, Training Loss: 0.6528705954551697, Validation Loss: 0.6525896191596985\n",
      "Epoch 7875/10000, Training Loss: 0.6412305235862732, Validation Loss: 0.657993495464325\n",
      "Epoch 7876/10000, Training Loss: 0.6204509139060974, Validation Loss: 0.6094584465026855\n",
      "Epoch 7877/10000, Training Loss: 0.6409380435943604, Validation Loss: 0.6616474986076355\n",
      "Epoch 7878/10000, Training Loss: 0.6831058263778687, Validation Loss: 0.9365366101264954\n",
      "Epoch 7879/10000, Training Loss: 0.6550112366676331, Validation Loss: 0.6469427347183228\n",
      "Epoch 7880/10000, Training Loss: 0.6300685405731201, Validation Loss: 0.7608776092529297\n",
      "Epoch 7881/10000, Training Loss: 0.6378141045570374, Validation Loss: 0.6902474761009216\n",
      "Epoch 7882/10000, Training Loss: 0.6262645721435547, Validation Loss: 0.6560261249542236\n",
      "Epoch 7883/10000, Training Loss: 0.6512979865074158, Validation Loss: 0.6051177382469177\n",
      "Epoch 7884/10000, Training Loss: 0.6482923030853271, Validation Loss: 0.5649595260620117\n",
      "Epoch 7885/10000, Training Loss: 0.6545171737670898, Validation Loss: 0.6414484977722168\n",
      "Epoch 7886/10000, Training Loss: 0.6980718374252319, Validation Loss: 0.6006125807762146\n",
      "Epoch 7887/10000, Training Loss: 0.6458747386932373, Validation Loss: 0.8216516375541687\n",
      "Epoch 7888/10000, Training Loss: 0.6808640360832214, Validation Loss: 0.6838319897651672\n",
      "Epoch 7889/10000, Training Loss: 0.6688922047615051, Validation Loss: 0.7371991276741028\n",
      "Epoch 7890/10000, Training Loss: 0.6024645566940308, Validation Loss: 0.7066249251365662\n",
      "Epoch 7891/10000, Training Loss: 0.6277657747268677, Validation Loss: 0.6775819659233093\n",
      "Epoch 7892/10000, Training Loss: 0.6576617956161499, Validation Loss: 0.706987202167511\n",
      "Epoch 7893/10000, Training Loss: 0.6604814529418945, Validation Loss: 0.6043211817741394\n",
      "Epoch 7894/10000, Training Loss: 0.6332769393920898, Validation Loss: 0.576093316078186\n",
      "Epoch 7895/10000, Training Loss: 0.6474272608757019, Validation Loss: 0.5973134636878967\n",
      "Epoch 7896/10000, Training Loss: 0.6548472046852112, Validation Loss: 0.9780213236808777\n",
      "Epoch 7897/10000, Training Loss: 0.6477734446525574, Validation Loss: 0.7797126770019531\n",
      "Epoch 7898/10000, Training Loss: 0.7267307639122009, Validation Loss: 0.6501597762107849\n",
      "Epoch 7899/10000, Training Loss: 0.6445998549461365, Validation Loss: 0.8312987685203552\n",
      "Epoch 7900/10000, Training Loss: 0.6439304351806641, Validation Loss: 0.602026104927063\n",
      "Epoch 7901/10000, Training Loss: 0.6423802971839905, Validation Loss: 0.7150837779045105\n",
      "Epoch 7902/10000, Training Loss: 0.6624873876571655, Validation Loss: 0.8140739798545837\n",
      "Epoch 7903/10000, Training Loss: 0.6240552663803101, Validation Loss: 0.6357484459877014\n",
      "Epoch 7904/10000, Training Loss: 0.6885757446289062, Validation Loss: 0.5943980813026428\n",
      "Epoch 7905/10000, Training Loss: 0.6235666871070862, Validation Loss: 0.9498397707939148\n",
      "Epoch 7906/10000, Training Loss: 0.6583977937698364, Validation Loss: 0.6341652274131775\n",
      "Epoch 7907/10000, Training Loss: 0.6540486812591553, Validation Loss: 0.5953567028045654\n",
      "Epoch 7908/10000, Training Loss: 0.6448558568954468, Validation Loss: 0.7081482410430908\n",
      "Epoch 7909/10000, Training Loss: 0.6548815369606018, Validation Loss: 0.64997798204422\n",
      "Epoch 7910/10000, Training Loss: 0.6522222757339478, Validation Loss: 0.7613585591316223\n",
      "Epoch 7911/10000, Training Loss: 0.605620265007019, Validation Loss: 0.8186253905296326\n",
      "Epoch 7912/10000, Training Loss: 0.630558967590332, Validation Loss: 0.5592067241668701\n",
      "Epoch 7913/10000, Training Loss: 0.6618001461029053, Validation Loss: 0.6940093040466309\n",
      "Epoch 7914/10000, Training Loss: 0.6399336457252502, Validation Loss: 0.44647374749183655\n",
      "Epoch 7915/10000, Training Loss: 0.6438160538673401, Validation Loss: 0.5787935853004456\n",
      "Epoch 7916/10000, Training Loss: 0.6396659016609192, Validation Loss: 0.7072742581367493\n",
      "Epoch 7917/10000, Training Loss: 0.6610696315765381, Validation Loss: 0.6835260391235352\n",
      "Epoch 7918/10000, Training Loss: 0.6707299947738647, Validation Loss: 0.47550228238105774\n",
      "Epoch 7919/10000, Training Loss: 0.645066499710083, Validation Loss: 0.5480597019195557\n",
      "Epoch 7920/10000, Training Loss: 0.6331446170806885, Validation Loss: 0.7714772820472717\n",
      "Epoch 7921/10000, Training Loss: 0.6246358156204224, Validation Loss: 0.8915141224861145\n",
      "Epoch 7922/10000, Training Loss: 0.601584792137146, Validation Loss: 0.5247821807861328\n",
      "Epoch 7923/10000, Training Loss: 0.6485427021980286, Validation Loss: 0.7626000046730042\n",
      "Epoch 7924/10000, Training Loss: 0.6537450551986694, Validation Loss: 0.7008922696113586\n",
      "Epoch 7925/10000, Training Loss: 0.6508004665374756, Validation Loss: 0.7499240040779114\n",
      "Epoch 7926/10000, Training Loss: 0.6563854813575745, Validation Loss: 0.6508597731590271\n",
      "Epoch 7927/10000, Training Loss: 0.6384017467498779, Validation Loss: 0.7539355754852295\n",
      "Epoch 7928/10000, Training Loss: 0.6532739400863647, Validation Loss: 0.7161667346954346\n",
      "Epoch 7929/10000, Training Loss: 0.6419432163238525, Validation Loss: 0.6430414319038391\n",
      "Epoch 7930/10000, Training Loss: 0.6510237455368042, Validation Loss: 0.5346073508262634\n",
      "Epoch 7931/10000, Training Loss: 0.6226116418838501, Validation Loss: 0.7153582572937012\n",
      "Epoch 7932/10000, Training Loss: 0.6505047082901001, Validation Loss: 0.5966253876686096\n",
      "Epoch 7933/10000, Training Loss: 0.629902184009552, Validation Loss: 0.6529513001441956\n",
      "Epoch 7934/10000, Training Loss: 0.674623429775238, Validation Loss: 0.5952914357185364\n",
      "Epoch 7935/10000, Training Loss: 0.6297988891601562, Validation Loss: 0.630660355091095\n",
      "Epoch 7936/10000, Training Loss: 0.6404422521591187, Validation Loss: 0.9080976843833923\n",
      "Epoch 7937/10000, Training Loss: 0.6203774809837341, Validation Loss: 0.6247131824493408\n",
      "Epoch 7938/10000, Training Loss: 0.616881251335144, Validation Loss: 0.5968573093414307\n",
      "Epoch 7939/10000, Training Loss: 0.6332797408103943, Validation Loss: 0.6849632263183594\n",
      "Epoch 7940/10000, Training Loss: 0.6645345687866211, Validation Loss: 0.6393842101097107\n",
      "Epoch 7941/10000, Training Loss: 0.6366602778434753, Validation Loss: 0.7049906849861145\n",
      "Epoch 7942/10000, Training Loss: 0.6695559620857239, Validation Loss: 0.8080213069915771\n",
      "Epoch 7943/10000, Training Loss: 0.661690354347229, Validation Loss: 0.793862521648407\n",
      "Epoch 7944/10000, Training Loss: 0.6521243453025818, Validation Loss: 0.9536015391349792\n",
      "Epoch 7945/10000, Training Loss: 0.6537874937057495, Validation Loss: 0.6024943590164185\n",
      "Epoch 7946/10000, Training Loss: 0.6115840673446655, Validation Loss: 0.7293808460235596\n",
      "Epoch 7947/10000, Training Loss: 0.636082112789154, Validation Loss: 0.7841947078704834\n",
      "Epoch 7948/10000, Training Loss: 0.6050603985786438, Validation Loss: 0.6652163863182068\n",
      "Epoch 7949/10000, Training Loss: 0.6448487043380737, Validation Loss: 0.7383673787117004\n",
      "Epoch 7950/10000, Training Loss: 0.6663230657577515, Validation Loss: 0.7155233025550842\n",
      "Epoch 7951/10000, Training Loss: 0.6232349872589111, Validation Loss: 0.585739254951477\n",
      "Epoch 7952/10000, Training Loss: 0.6258715987205505, Validation Loss: 0.6270318627357483\n",
      "Epoch 7953/10000, Training Loss: 0.6724570989608765, Validation Loss: 0.6985702514648438\n",
      "Epoch 7954/10000, Training Loss: 0.6364614963531494, Validation Loss: 0.5931296944618225\n",
      "Epoch 7955/10000, Training Loss: 0.6237537860870361, Validation Loss: 0.6593967080116272\n",
      "Epoch 7956/10000, Training Loss: 0.61917644739151, Validation Loss: 0.6018126606941223\n",
      "Epoch 7957/10000, Training Loss: 0.6620732545852661, Validation Loss: 0.5055341124534607\n",
      "Epoch 7958/10000, Training Loss: 0.6095937490463257, Validation Loss: 0.561036229133606\n",
      "Epoch 7959/10000, Training Loss: 0.6384535431861877, Validation Loss: 0.652058482170105\n",
      "Epoch 7960/10000, Training Loss: 0.6614264249801636, Validation Loss: 0.6473577618598938\n",
      "Epoch 7961/10000, Training Loss: 0.6662671566009521, Validation Loss: 0.7094358801841736\n",
      "Epoch 7962/10000, Training Loss: 0.6302567720413208, Validation Loss: 0.4984377920627594\n",
      "Epoch 7963/10000, Training Loss: 0.6456339359283447, Validation Loss: 0.6714188456535339\n",
      "Epoch 7964/10000, Training Loss: 0.62192302942276, Validation Loss: 0.8931052088737488\n",
      "Epoch 7965/10000, Training Loss: 0.6354707479476929, Validation Loss: 0.6283461451530457\n",
      "Epoch 7966/10000, Training Loss: 0.5988719463348389, Validation Loss: 0.720990002155304\n",
      "Epoch 7967/10000, Training Loss: 0.6405220031738281, Validation Loss: 0.8453566431999207\n",
      "Epoch 7968/10000, Training Loss: 0.65956050157547, Validation Loss: 0.601259171962738\n",
      "Epoch 7969/10000, Training Loss: 0.6424187421798706, Validation Loss: 0.650906503200531\n",
      "Epoch 7970/10000, Training Loss: 0.6224858164787292, Validation Loss: 0.6560952663421631\n",
      "Epoch 7971/10000, Training Loss: 0.6221437454223633, Validation Loss: 0.6545012593269348\n",
      "Epoch 7972/10000, Training Loss: 0.6573871970176697, Validation Loss: 0.6884810328483582\n",
      "Epoch 7973/10000, Training Loss: 0.6782618165016174, Validation Loss: 0.6429535746574402\n",
      "Epoch 7974/10000, Training Loss: 0.6262325048446655, Validation Loss: 0.6184852123260498\n",
      "Epoch 7975/10000, Training Loss: 0.6279261112213135, Validation Loss: 0.55130535364151\n",
      "Epoch 7976/10000, Training Loss: 0.6320403218269348, Validation Loss: 0.5305706858634949\n",
      "Epoch 7977/10000, Training Loss: 0.6468493938446045, Validation Loss: 0.6376499533653259\n",
      "Epoch 7978/10000, Training Loss: 0.6566872000694275, Validation Loss: 0.8328266739845276\n",
      "Epoch 7979/10000, Training Loss: 0.6230953931808472, Validation Loss: 0.6460489630699158\n",
      "Epoch 7980/10000, Training Loss: 0.6302170157432556, Validation Loss: 0.6517400145530701\n",
      "Epoch 7981/10000, Training Loss: 0.655481219291687, Validation Loss: 0.7193600535392761\n",
      "Epoch 7982/10000, Training Loss: 0.613595724105835, Validation Loss: 0.6781725287437439\n",
      "Epoch 7983/10000, Training Loss: 0.6488900780677795, Validation Loss: 0.5843418836593628\n",
      "Epoch 7984/10000, Training Loss: 0.6561672687530518, Validation Loss: 0.7260932326316833\n",
      "Epoch 7985/10000, Training Loss: 0.6483800411224365, Validation Loss: 0.7654111981391907\n",
      "Epoch 7986/10000, Training Loss: 0.6531361937522888, Validation Loss: 0.71077561378479\n",
      "Epoch 7987/10000, Training Loss: 0.6446936130523682, Validation Loss: 0.5492221713066101\n",
      "Epoch 7988/10000, Training Loss: 0.6635113954544067, Validation Loss: 0.7895113825798035\n",
      "Epoch 7989/10000, Training Loss: 0.6294488310813904, Validation Loss: 0.7256710529327393\n",
      "Epoch 7990/10000, Training Loss: 0.6654703617095947, Validation Loss: 0.6839110255241394\n",
      "Epoch 7991/10000, Training Loss: 0.6334032416343689, Validation Loss: 0.6889709830284119\n",
      "Epoch 7992/10000, Training Loss: 0.6249773502349854, Validation Loss: 0.5156898498535156\n",
      "Epoch 7993/10000, Training Loss: 0.6507537364959717, Validation Loss: 0.9291701912879944\n",
      "Epoch 7994/10000, Training Loss: 0.6102554798126221, Validation Loss: 0.6154575347900391\n",
      "Epoch 7995/10000, Training Loss: 0.6421247720718384, Validation Loss: 0.627305805683136\n",
      "Epoch 7996/10000, Training Loss: 0.6498148441314697, Validation Loss: 0.8151953220367432\n",
      "Epoch 7997/10000, Training Loss: 0.6602759957313538, Validation Loss: 0.71210116147995\n",
      "Epoch 7998/10000, Training Loss: 0.6179360151290894, Validation Loss: 0.8121686577796936\n",
      "Epoch 7999/10000, Training Loss: 0.5762484073638916, Validation Loss: 0.678082287311554\n",
      "Epoch 8000/10000, Training Loss: 0.653962254524231, Validation Loss: 0.5937779545783997\n",
      "Epoch 8001/10000, Training Loss: 0.6677194833755493, Validation Loss: 0.6004844307899475\n",
      "Epoch 8002/10000, Training Loss: 0.6608210206031799, Validation Loss: 0.6261170506477356\n",
      "Epoch 8003/10000, Training Loss: 0.6403831243515015, Validation Loss: 0.9596959948539734\n",
      "Epoch 8004/10000, Training Loss: 0.6353912949562073, Validation Loss: 0.7304210662841797\n",
      "Epoch 8005/10000, Training Loss: 0.6325401663780212, Validation Loss: 0.43726345896720886\n",
      "Epoch 8006/10000, Training Loss: 0.6833895444869995, Validation Loss: 0.6018617153167725\n",
      "Epoch 8007/10000, Training Loss: 0.7006363272666931, Validation Loss: 0.7589638233184814\n",
      "Epoch 8008/10000, Training Loss: 0.6406809687614441, Validation Loss: 0.4882293939590454\n",
      "Epoch 8009/10000, Training Loss: 0.669908881187439, Validation Loss: 0.5646817684173584\n",
      "Epoch 8010/10000, Training Loss: 0.6398766040802002, Validation Loss: 0.9475722312927246\n",
      "Epoch 8011/10000, Training Loss: 0.6456327438354492, Validation Loss: 0.7458598613739014\n",
      "Epoch 8012/10000, Training Loss: 0.602781355381012, Validation Loss: 0.615049421787262\n",
      "Epoch 8013/10000, Training Loss: 0.6177974939346313, Validation Loss: 0.5906009674072266\n",
      "Epoch 8014/10000, Training Loss: 0.6408084630966187, Validation Loss: 0.6318286061286926\n",
      "Epoch 8015/10000, Training Loss: 0.6102556586265564, Validation Loss: 0.5496336817741394\n",
      "Epoch 8016/10000, Training Loss: 0.637518048286438, Validation Loss: 0.7311566472053528\n",
      "Epoch 8017/10000, Training Loss: 0.6285110116004944, Validation Loss: 0.7517352104187012\n",
      "Epoch 8018/10000, Training Loss: 0.7042903304100037, Validation Loss: 0.7370209693908691\n",
      "Epoch 8019/10000, Training Loss: 0.6676766276359558, Validation Loss: 0.8477253317832947\n",
      "Epoch 8020/10000, Training Loss: 0.6429980993270874, Validation Loss: 0.7627081274986267\n",
      "Epoch 8021/10000, Training Loss: 0.661188542842865, Validation Loss: 0.7446963787078857\n",
      "Epoch 8022/10000, Training Loss: 0.6197440028190613, Validation Loss: 0.5213256478309631\n",
      "Epoch 8023/10000, Training Loss: 0.6102659702301025, Validation Loss: 0.7864527106285095\n",
      "Epoch 8024/10000, Training Loss: 0.6349006295204163, Validation Loss: 0.8109739422798157\n",
      "Epoch 8025/10000, Training Loss: 0.6075297594070435, Validation Loss: 0.8670070767402649\n",
      "Epoch 8026/10000, Training Loss: 0.6374956369400024, Validation Loss: 0.5498160123825073\n",
      "Epoch 8027/10000, Training Loss: 0.6768918633460999, Validation Loss: 0.5597921013832092\n",
      "Epoch 8028/10000, Training Loss: 0.6489062905311584, Validation Loss: 0.6432784795761108\n",
      "Epoch 8029/10000, Training Loss: 0.6659724712371826, Validation Loss: 0.6729868054389954\n",
      "Epoch 8030/10000, Training Loss: 0.6445218324661255, Validation Loss: 0.6568437218666077\n",
      "Epoch 8031/10000, Training Loss: 0.6532830595970154, Validation Loss: 0.6907410621643066\n",
      "Epoch 8032/10000, Training Loss: 0.6506451368331909, Validation Loss: 0.7746772766113281\n",
      "Epoch 8033/10000, Training Loss: 0.6484134197235107, Validation Loss: 0.6399348378181458\n",
      "Epoch 8034/10000, Training Loss: 0.6265938878059387, Validation Loss: 0.5265421867370605\n",
      "Epoch 8035/10000, Training Loss: 0.6434967517852783, Validation Loss: 0.7821393609046936\n",
      "Epoch 8036/10000, Training Loss: 0.6542688012123108, Validation Loss: 0.8005549311637878\n",
      "Epoch 8037/10000, Training Loss: 0.6080752611160278, Validation Loss: 0.598859965801239\n",
      "Epoch 8038/10000, Training Loss: 0.6540098190307617, Validation Loss: 0.5739233493804932\n",
      "Epoch 8039/10000, Training Loss: 0.6492767930030823, Validation Loss: 0.6124016642570496\n",
      "Epoch 8040/10000, Training Loss: 0.6368201971054077, Validation Loss: 0.6475933194160461\n",
      "Epoch 8041/10000, Training Loss: 0.6768672466278076, Validation Loss: 0.6959338188171387\n",
      "Epoch 8042/10000, Training Loss: 0.6794141530990601, Validation Loss: 0.6885603070259094\n",
      "Epoch 8043/10000, Training Loss: 0.6227693557739258, Validation Loss: 0.717207133769989\n",
      "Epoch 8044/10000, Training Loss: 0.6342697143554688, Validation Loss: 0.9585419297218323\n",
      "Epoch 8045/10000, Training Loss: 0.6447553038597107, Validation Loss: 0.6219943761825562\n",
      "Epoch 8046/10000, Training Loss: 0.6360453367233276, Validation Loss: 0.5977386832237244\n",
      "Epoch 8047/10000, Training Loss: 0.6776708960533142, Validation Loss: 0.8220270276069641\n",
      "Epoch 8048/10000, Training Loss: 0.6409581303596497, Validation Loss: 0.6581584811210632\n",
      "Epoch 8049/10000, Training Loss: 0.6214045882225037, Validation Loss: 0.6194939017295837\n",
      "Epoch 8050/10000, Training Loss: 0.6911544799804688, Validation Loss: 0.8446692824363708\n",
      "Epoch 8051/10000, Training Loss: 0.6429914832115173, Validation Loss: 0.9029988646507263\n",
      "Epoch 8052/10000, Training Loss: 0.6024618744850159, Validation Loss: 0.7850961089134216\n",
      "Epoch 8053/10000, Training Loss: 0.6522570252418518, Validation Loss: 0.5969517230987549\n",
      "Epoch 8054/10000, Training Loss: 0.6387147307395935, Validation Loss: 0.5769056677818298\n",
      "Epoch 8055/10000, Training Loss: 0.6760741472244263, Validation Loss: 0.7630873322486877\n",
      "Epoch 8056/10000, Training Loss: 0.6861012578010559, Validation Loss: 0.6727225184440613\n",
      "Epoch 8057/10000, Training Loss: 0.632876992225647, Validation Loss: 0.6766943335533142\n",
      "Epoch 8058/10000, Training Loss: 0.6267071962356567, Validation Loss: 0.8043425679206848\n",
      "Epoch 8059/10000, Training Loss: 0.6373669505119324, Validation Loss: 0.6598300933837891\n",
      "Epoch 8060/10000, Training Loss: 0.6435282230377197, Validation Loss: 0.7861177325248718\n",
      "Epoch 8061/10000, Training Loss: 0.5760186314582825, Validation Loss: 0.6845858693122864\n",
      "Epoch 8062/10000, Training Loss: 0.6371021270751953, Validation Loss: 0.6578555107116699\n",
      "Epoch 8063/10000, Training Loss: 0.6205494403839111, Validation Loss: 0.8225066661834717\n",
      "Epoch 8064/10000, Training Loss: 0.7249152660369873, Validation Loss: 0.6600677371025085\n",
      "Epoch 8065/10000, Training Loss: 0.6763781905174255, Validation Loss: 0.5709489583969116\n",
      "Epoch 8066/10000, Training Loss: 0.6677722930908203, Validation Loss: 0.6911110877990723\n",
      "Epoch 8067/10000, Training Loss: 0.6412776708602905, Validation Loss: 0.6318051218986511\n",
      "Epoch 8068/10000, Training Loss: 0.6741225123405457, Validation Loss: 0.7615455985069275\n",
      "Epoch 8069/10000, Training Loss: 0.6402872800827026, Validation Loss: 0.6112033724784851\n",
      "Epoch 8070/10000, Training Loss: 0.6181848049163818, Validation Loss: 0.6659266948699951\n",
      "Epoch 8071/10000, Training Loss: 0.6713540554046631, Validation Loss: 0.6235355138778687\n",
      "Epoch 8072/10000, Training Loss: 0.6523461937904358, Validation Loss: 0.730323851108551\n",
      "Epoch 8073/10000, Training Loss: 0.6357030868530273, Validation Loss: 0.7318760752677917\n",
      "Epoch 8074/10000, Training Loss: 0.629817545413971, Validation Loss: 0.9275183081626892\n",
      "Epoch 8075/10000, Training Loss: 0.6458795070648193, Validation Loss: 0.7246981263160706\n",
      "Epoch 8076/10000, Training Loss: 0.6394219994544983, Validation Loss: 0.6222750544548035\n",
      "Epoch 8077/10000, Training Loss: 0.6180792450904846, Validation Loss: 0.5604440569877625\n",
      "Epoch 8078/10000, Training Loss: 0.6349712014198303, Validation Loss: 0.8636205196380615\n",
      "Epoch 8079/10000, Training Loss: 0.6599394679069519, Validation Loss: 0.7009424567222595\n",
      "Epoch 8080/10000, Training Loss: 0.6611039042472839, Validation Loss: 1.1187657117843628\n",
      "Epoch 8081/10000, Training Loss: 0.6716128587722778, Validation Loss: 0.7050130367279053\n",
      "Epoch 8082/10000, Training Loss: 0.6429456472396851, Validation Loss: 0.8318914771080017\n",
      "Epoch 8083/10000, Training Loss: 0.6257807612419128, Validation Loss: 0.7055392265319824\n",
      "Epoch 8084/10000, Training Loss: 0.6707513332366943, Validation Loss: 0.5916218757629395\n",
      "Epoch 8085/10000, Training Loss: 0.6567445397377014, Validation Loss: 0.8927755355834961\n",
      "Epoch 8086/10000, Training Loss: 0.6854361891746521, Validation Loss: 0.8160731196403503\n",
      "Epoch 8087/10000, Training Loss: 0.6417970061302185, Validation Loss: 0.8611847758293152\n",
      "Epoch 8088/10000, Training Loss: 0.6533899307250977, Validation Loss: 0.7137972712516785\n",
      "Epoch 8089/10000, Training Loss: 0.6082550883293152, Validation Loss: 0.7132546901702881\n",
      "Epoch 8090/10000, Training Loss: 0.6318391561508179, Validation Loss: 0.5784643292427063\n",
      "Epoch 8091/10000, Training Loss: 0.6346322298049927, Validation Loss: 0.6643293499946594\n",
      "Epoch 8092/10000, Training Loss: 0.6608816385269165, Validation Loss: 0.7612826824188232\n",
      "Epoch 8093/10000, Training Loss: 0.6198753118515015, Validation Loss: 0.7328075766563416\n",
      "Epoch 8094/10000, Training Loss: 0.6047326326370239, Validation Loss: 0.8850035071372986\n",
      "Epoch 8095/10000, Training Loss: 0.6490485668182373, Validation Loss: 0.7015193104743958\n",
      "Epoch 8096/10000, Training Loss: 0.6821334362030029, Validation Loss: 0.6860988736152649\n",
      "Epoch 8097/10000, Training Loss: 0.644606351852417, Validation Loss: 0.958994448184967\n",
      "Epoch 8098/10000, Training Loss: 0.6701678037643433, Validation Loss: 0.6143069863319397\n",
      "Epoch 8099/10000, Training Loss: 0.6764140725135803, Validation Loss: 0.5321330428123474\n",
      "Epoch 8100/10000, Training Loss: 0.6426354050636292, Validation Loss: 0.7566137313842773\n",
      "Epoch 8101/10000, Training Loss: 0.6682504415512085, Validation Loss: 0.8543801307678223\n",
      "Epoch 8102/10000, Training Loss: 0.6710933446884155, Validation Loss: 0.7233080267906189\n",
      "Epoch 8103/10000, Training Loss: 0.6544722318649292, Validation Loss: 0.8900119662284851\n",
      "Epoch 8104/10000, Training Loss: 0.6377445459365845, Validation Loss: 0.5893141627311707\n",
      "Epoch 8105/10000, Training Loss: 0.6103792786598206, Validation Loss: 0.6884452700614929\n",
      "Epoch 8106/10000, Training Loss: 0.6398486495018005, Validation Loss: 0.6741616129875183\n",
      "Epoch 8107/10000, Training Loss: 0.6562283039093018, Validation Loss: 0.7210474610328674\n",
      "Epoch 8108/10000, Training Loss: 0.655288815498352, Validation Loss: 0.7015552520751953\n",
      "Epoch 8109/10000, Training Loss: 0.6610277891159058, Validation Loss: 0.8239307403564453\n",
      "Epoch 8110/10000, Training Loss: 0.6350289583206177, Validation Loss: 0.6090461015701294\n",
      "Epoch 8111/10000, Training Loss: 0.6964953541755676, Validation Loss: 0.7492772936820984\n",
      "Epoch 8112/10000, Training Loss: 0.6241461038589478, Validation Loss: 0.6640861630439758\n",
      "Epoch 8113/10000, Training Loss: 0.6662155985832214, Validation Loss: 0.6633331179618835\n",
      "Epoch 8114/10000, Training Loss: 0.6430750489234924, Validation Loss: 0.725776731967926\n",
      "Epoch 8115/10000, Training Loss: 0.6480233073234558, Validation Loss: 0.813286542892456\n",
      "Epoch 8116/10000, Training Loss: 0.6326630711555481, Validation Loss: 0.7287173271179199\n",
      "Epoch 8117/10000, Training Loss: 0.6514667868614197, Validation Loss: 0.7255731225013733\n",
      "Epoch 8118/10000, Training Loss: 0.6664620041847229, Validation Loss: 0.8692904114723206\n",
      "Epoch 8119/10000, Training Loss: 0.6386017203330994, Validation Loss: 0.7211452126502991\n",
      "Epoch 8120/10000, Training Loss: 0.6458329558372498, Validation Loss: 0.5476129651069641\n",
      "Epoch 8121/10000, Training Loss: 0.6656770706176758, Validation Loss: 0.7624158263206482\n",
      "Epoch 8122/10000, Training Loss: 0.6755138039588928, Validation Loss: 0.8373284339904785\n",
      "Epoch 8123/10000, Training Loss: 0.6145959496498108, Validation Loss: 0.925166130065918\n",
      "Epoch 8124/10000, Training Loss: 0.6581408381462097, Validation Loss: 0.8121457099914551\n",
      "Epoch 8125/10000, Training Loss: 0.6363030672073364, Validation Loss: 0.6406556963920593\n",
      "Epoch 8126/10000, Training Loss: 0.6239134669303894, Validation Loss: 0.691436767578125\n",
      "Epoch 8127/10000, Training Loss: 0.6486606597900391, Validation Loss: 0.6095404028892517\n",
      "Epoch 8128/10000, Training Loss: 0.6660317182540894, Validation Loss: 0.6887198090553284\n",
      "Epoch 8129/10000, Training Loss: 0.6685491800308228, Validation Loss: 0.5761968493461609\n",
      "Epoch 8130/10000, Training Loss: 0.627491295337677, Validation Loss: 0.7404911518096924\n",
      "Epoch 8131/10000, Training Loss: 0.666723370552063, Validation Loss: 0.6616141200065613\n",
      "Epoch 8132/10000, Training Loss: 0.6529139280319214, Validation Loss: 0.7091264724731445\n",
      "Epoch 8133/10000, Training Loss: 0.6547899842262268, Validation Loss: 0.895934522151947\n",
      "Epoch 8134/10000, Training Loss: 0.6668397188186646, Validation Loss: 1.2297418117523193\n",
      "Epoch 8135/10000, Training Loss: 0.6172643303871155, Validation Loss: 0.6884646415710449\n",
      "Epoch 8136/10000, Training Loss: 0.6118448972702026, Validation Loss: 0.4679074287414551\n",
      "Epoch 8137/10000, Training Loss: 0.6543033123016357, Validation Loss: 0.762593686580658\n",
      "Epoch 8138/10000, Training Loss: 0.6526569724082947, Validation Loss: 0.6507467031478882\n",
      "Epoch 8139/10000, Training Loss: 0.603117823600769, Validation Loss: 0.6326637268066406\n",
      "Epoch 8140/10000, Training Loss: 0.6512798070907593, Validation Loss: 0.589417040348053\n",
      "Epoch 8141/10000, Training Loss: 0.6257425546646118, Validation Loss: 0.49824991822242737\n",
      "Epoch 8142/10000, Training Loss: 0.6383634805679321, Validation Loss: 0.7337594032287598\n",
      "Epoch 8143/10000, Training Loss: 0.6570911407470703, Validation Loss: 0.7220590114593506\n",
      "Epoch 8144/10000, Training Loss: 0.6632795333862305, Validation Loss: 0.8494818210601807\n",
      "Epoch 8145/10000, Training Loss: 0.6547432541847229, Validation Loss: 0.8175552487373352\n",
      "Epoch 8146/10000, Training Loss: 0.645150899887085, Validation Loss: 0.6394898295402527\n",
      "Epoch 8147/10000, Training Loss: 0.6662444472312927, Validation Loss: 0.7829787731170654\n",
      "Epoch 8148/10000, Training Loss: 0.6209818720817566, Validation Loss: 0.69683438539505\n",
      "Epoch 8149/10000, Training Loss: 0.633978545665741, Validation Loss: 0.6028098464012146\n",
      "Epoch 8150/10000, Training Loss: 0.6164271831512451, Validation Loss: 0.5747644305229187\n",
      "Epoch 8151/10000, Training Loss: 0.6128588318824768, Validation Loss: 0.7339292168617249\n",
      "Epoch 8152/10000, Training Loss: 0.6190375089645386, Validation Loss: 0.6611986756324768\n",
      "Epoch 8153/10000, Training Loss: 0.6051889657974243, Validation Loss: 1.293092966079712\n",
      "Epoch 8154/10000, Training Loss: 0.6700395941734314, Validation Loss: 0.7673813700675964\n",
      "Epoch 8155/10000, Training Loss: 0.6172753572463989, Validation Loss: 0.6209368109703064\n",
      "Epoch 8156/10000, Training Loss: 0.6280266642570496, Validation Loss: 0.6490231156349182\n",
      "Epoch 8157/10000, Training Loss: 0.6392730474472046, Validation Loss: 0.6377901434898376\n",
      "Epoch 8158/10000, Training Loss: 0.6383085250854492, Validation Loss: 0.734088659286499\n",
      "Epoch 8159/10000, Training Loss: 0.6798756718635559, Validation Loss: 0.7088851928710938\n",
      "Epoch 8160/10000, Training Loss: 0.6360874176025391, Validation Loss: 0.5844313502311707\n",
      "Epoch 8161/10000, Training Loss: 0.6638111472129822, Validation Loss: 0.5839561223983765\n",
      "Epoch 8162/10000, Training Loss: 0.6547126173973083, Validation Loss: 0.7093513607978821\n",
      "Epoch 8163/10000, Training Loss: 0.6323451995849609, Validation Loss: 0.7759625911712646\n",
      "Epoch 8164/10000, Training Loss: 0.63809734582901, Validation Loss: 0.6261776089668274\n",
      "Epoch 8165/10000, Training Loss: 0.6375092267990112, Validation Loss: 0.7598132491111755\n",
      "Epoch 8166/10000, Training Loss: 0.6644185185432434, Validation Loss: 0.7656629681587219\n",
      "Epoch 8167/10000, Training Loss: 0.6736803650856018, Validation Loss: 0.7532843947410583\n",
      "Epoch 8168/10000, Training Loss: 0.6769206523895264, Validation Loss: 0.8412554860115051\n",
      "Epoch 8169/10000, Training Loss: 0.6664923429489136, Validation Loss: 0.8482441306114197\n",
      "Epoch 8170/10000, Training Loss: 0.6341199278831482, Validation Loss: 0.7669816017150879\n",
      "Epoch 8171/10000, Training Loss: 0.6942339539527893, Validation Loss: 0.7754325270652771\n",
      "Epoch 8172/10000, Training Loss: 0.5854164958000183, Validation Loss: 0.698799192905426\n",
      "Epoch 8173/10000, Training Loss: 0.6268442869186401, Validation Loss: 0.6878940463066101\n",
      "Epoch 8174/10000, Training Loss: 0.6609401702880859, Validation Loss: 0.7651602625846863\n",
      "Epoch 8175/10000, Training Loss: 0.6523268818855286, Validation Loss: 0.5939536690711975\n",
      "Epoch 8176/10000, Training Loss: 0.6299130916595459, Validation Loss: 0.7358788847923279\n",
      "Epoch 8177/10000, Training Loss: 0.6027544140815735, Validation Loss: 0.719588577747345\n",
      "Epoch 8178/10000, Training Loss: 0.6316144466400146, Validation Loss: 0.687673032283783\n",
      "Epoch 8179/10000, Training Loss: 0.642870306968689, Validation Loss: 0.7120288014411926\n",
      "Epoch 8180/10000, Training Loss: 0.6427603363990784, Validation Loss: 0.7754921913146973\n",
      "Epoch 8181/10000, Training Loss: 0.6301666498184204, Validation Loss: 0.6936292052268982\n",
      "Epoch 8182/10000, Training Loss: 0.6529567241668701, Validation Loss: 0.4618133008480072\n",
      "Epoch 8183/10000, Training Loss: 0.6643904447555542, Validation Loss: 0.8970212936401367\n",
      "Epoch 8184/10000, Training Loss: 0.6238022446632385, Validation Loss: 0.680759608745575\n",
      "Epoch 8185/10000, Training Loss: 0.6593242287635803, Validation Loss: 0.6111111640930176\n",
      "Epoch 8186/10000, Training Loss: 0.6008960008621216, Validation Loss: 0.6446929574012756\n",
      "Epoch 8187/10000, Training Loss: 0.6181482076644897, Validation Loss: 0.8049617409706116\n",
      "Epoch 8188/10000, Training Loss: 0.5864111185073853, Validation Loss: 0.8601482510566711\n",
      "Epoch 8189/10000, Training Loss: 0.654244065284729, Validation Loss: 0.6513777375221252\n",
      "Epoch 8190/10000, Training Loss: 0.6533815860748291, Validation Loss: 0.8185861110687256\n",
      "Epoch 8191/10000, Training Loss: 0.6893429160118103, Validation Loss: 0.6195690631866455\n",
      "Epoch 8192/10000, Training Loss: 0.6850937604904175, Validation Loss: 0.6483772397041321\n",
      "Epoch 8193/10000, Training Loss: 0.6504380106925964, Validation Loss: 0.7522293925285339\n",
      "Epoch 8194/10000, Training Loss: 0.6438073515892029, Validation Loss: 0.557414174079895\n",
      "Epoch 8195/10000, Training Loss: 0.6542333960533142, Validation Loss: 0.9138424396514893\n",
      "Epoch 8196/10000, Training Loss: 0.6856890320777893, Validation Loss: 0.7105506062507629\n",
      "Epoch 8197/10000, Training Loss: 0.6588131785392761, Validation Loss: 0.6143233180046082\n",
      "Epoch 8198/10000, Training Loss: 0.6461104154586792, Validation Loss: 0.6730350852012634\n",
      "Epoch 8199/10000, Training Loss: 0.6241430640220642, Validation Loss: 0.7662548422813416\n",
      "Epoch 8200/10000, Training Loss: 0.6516247987747192, Validation Loss: 0.689939022064209\n",
      "Epoch 8201/10000, Training Loss: 0.6299419403076172, Validation Loss: 0.6088294386863708\n",
      "Epoch 8202/10000, Training Loss: 0.6729596853256226, Validation Loss: 0.7576925158500671\n",
      "Epoch 8203/10000, Training Loss: 0.643668532371521, Validation Loss: 0.6538743376731873\n",
      "Epoch 8204/10000, Training Loss: 0.6416710615158081, Validation Loss: 0.9406395554542542\n",
      "Epoch 8205/10000, Training Loss: 0.669018030166626, Validation Loss: 0.8000872135162354\n",
      "Epoch 8206/10000, Training Loss: 0.6562795042991638, Validation Loss: 0.6760668158531189\n",
      "Epoch 8207/10000, Training Loss: 0.6212889552116394, Validation Loss: 0.5311104655265808\n",
      "Epoch 8208/10000, Training Loss: 0.6486552953720093, Validation Loss: 0.6167128682136536\n",
      "Epoch 8209/10000, Training Loss: 0.6328563094139099, Validation Loss: 0.6831992268562317\n",
      "Epoch 8210/10000, Training Loss: 0.6761322021484375, Validation Loss: 0.7698030471801758\n",
      "Epoch 8211/10000, Training Loss: 0.6427883505821228, Validation Loss: 0.7352957129478455\n",
      "Epoch 8212/10000, Training Loss: 0.6132209300994873, Validation Loss: 0.805278480052948\n",
      "Epoch 8213/10000, Training Loss: 0.6470502614974976, Validation Loss: 0.7200813293457031\n",
      "Epoch 8214/10000, Training Loss: 0.6304138898849487, Validation Loss: 0.9287280440330505\n",
      "Epoch 8215/10000, Training Loss: 0.6486189961433411, Validation Loss: 0.6726164221763611\n",
      "Epoch 8216/10000, Training Loss: 0.6293331980705261, Validation Loss: 0.6421434283256531\n",
      "Epoch 8217/10000, Training Loss: 0.6475951671600342, Validation Loss: 0.6905255317687988\n",
      "Epoch 8218/10000, Training Loss: 0.6175644993782043, Validation Loss: 0.6565181612968445\n",
      "Epoch 8219/10000, Training Loss: 0.6368715763092041, Validation Loss: 0.5829190611839294\n",
      "Epoch 8220/10000, Training Loss: 0.6274881362915039, Validation Loss: 0.8325254917144775\n",
      "Epoch 8221/10000, Training Loss: 0.6241902709007263, Validation Loss: 0.7186357975006104\n",
      "Epoch 8222/10000, Training Loss: 0.6855742931365967, Validation Loss: 0.7169445157051086\n",
      "Epoch 8223/10000, Training Loss: 0.6995909810066223, Validation Loss: 0.5800716876983643\n",
      "Epoch 8224/10000, Training Loss: 0.639460563659668, Validation Loss: 0.6466348171234131\n",
      "Epoch 8225/10000, Training Loss: 0.6979930400848389, Validation Loss: 0.6490180492401123\n",
      "Epoch 8226/10000, Training Loss: 0.6502813696861267, Validation Loss: 0.6329086422920227\n",
      "Epoch 8227/10000, Training Loss: 0.6312825083732605, Validation Loss: 0.7754276394844055\n",
      "Epoch 8228/10000, Training Loss: 0.667521595954895, Validation Loss: 0.7777776718139648\n",
      "Epoch 8229/10000, Training Loss: 0.6399667263031006, Validation Loss: 0.676720917224884\n",
      "Epoch 8230/10000, Training Loss: 0.6903095841407776, Validation Loss: 0.529423713684082\n",
      "Epoch 8231/10000, Training Loss: 0.6376743316650391, Validation Loss: 0.8066359162330627\n",
      "Epoch 8232/10000, Training Loss: 0.6363024115562439, Validation Loss: 0.6968550682067871\n",
      "Epoch 8233/10000, Training Loss: 0.6492182612419128, Validation Loss: 0.7823123931884766\n",
      "Epoch 8234/10000, Training Loss: 0.6255682706832886, Validation Loss: 0.8142349123954773\n",
      "Epoch 8235/10000, Training Loss: 0.6056714653968811, Validation Loss: 0.6748610138893127\n",
      "Epoch 8236/10000, Training Loss: 0.6593754291534424, Validation Loss: 0.6615033745765686\n",
      "Epoch 8237/10000, Training Loss: 0.6300599575042725, Validation Loss: 0.684028685092926\n",
      "Epoch 8238/10000, Training Loss: 0.6509661078453064, Validation Loss: 0.7292279601097107\n",
      "Epoch 8239/10000, Training Loss: 0.6692206263542175, Validation Loss: 0.7420567870140076\n",
      "Epoch 8240/10000, Training Loss: 0.6411504745483398, Validation Loss: 0.7057505249977112\n",
      "Epoch 8241/10000, Training Loss: 0.6327528357505798, Validation Loss: 0.6461560726165771\n",
      "Epoch 8242/10000, Training Loss: 0.6261513233184814, Validation Loss: 0.6546946167945862\n",
      "Epoch 8243/10000, Training Loss: 0.658359169960022, Validation Loss: 0.7505914568901062\n",
      "Epoch 8244/10000, Training Loss: 0.6481392979621887, Validation Loss: 0.5774241089820862\n",
      "Epoch 8245/10000, Training Loss: 0.6351416707038879, Validation Loss: 0.6894577145576477\n",
      "Epoch 8246/10000, Training Loss: 0.6612691879272461, Validation Loss: 0.6158769726753235\n",
      "Epoch 8247/10000, Training Loss: 0.6210839152336121, Validation Loss: 0.6736040115356445\n",
      "Epoch 8248/10000, Training Loss: 0.6087002158164978, Validation Loss: 0.5787191987037659\n",
      "Epoch 8249/10000, Training Loss: 0.656764566898346, Validation Loss: 0.6664355993270874\n",
      "Epoch 8250/10000, Training Loss: 0.6358860731124878, Validation Loss: 0.666231095790863\n",
      "Epoch 8251/10000, Training Loss: 0.6553351879119873, Validation Loss: 0.6544970870018005\n",
      "Epoch 8252/10000, Training Loss: 0.6487491130828857, Validation Loss: 0.7557573914527893\n",
      "Epoch 8253/10000, Training Loss: 0.6724108457565308, Validation Loss: 0.704029381275177\n",
      "Epoch 8254/10000, Training Loss: 0.6729526519775391, Validation Loss: 0.6787293553352356\n",
      "Epoch 8255/10000, Training Loss: 0.6592191457748413, Validation Loss: 0.6048600673675537\n",
      "Epoch 8256/10000, Training Loss: 0.6424117684364319, Validation Loss: 0.7587015628814697\n",
      "Epoch 8257/10000, Training Loss: 0.6217581033706665, Validation Loss: 0.7307047843933105\n",
      "Epoch 8258/10000, Training Loss: 0.6253693699836731, Validation Loss: 0.8419422507286072\n",
      "Epoch 8259/10000, Training Loss: 0.6470792293548584, Validation Loss: 0.7053096294403076\n",
      "Epoch 8260/10000, Training Loss: 0.6252208352088928, Validation Loss: 0.6414102911949158\n",
      "Epoch 8261/10000, Training Loss: 0.635273277759552, Validation Loss: 0.6304802894592285\n",
      "Epoch 8262/10000, Training Loss: 0.6374753713607788, Validation Loss: 0.7238033413887024\n",
      "Epoch 8263/10000, Training Loss: 0.6344899535179138, Validation Loss: 0.7031151652336121\n",
      "Epoch 8264/10000, Training Loss: 0.6699076294898987, Validation Loss: 0.8137806057929993\n",
      "Epoch 8265/10000, Training Loss: 0.6427732706069946, Validation Loss: 0.7141357064247131\n",
      "Epoch 8266/10000, Training Loss: 0.6558876037597656, Validation Loss: 0.5910945534706116\n",
      "Epoch 8267/10000, Training Loss: 0.6458446979522705, Validation Loss: 0.7124633193016052\n",
      "Epoch 8268/10000, Training Loss: 0.611061155796051, Validation Loss: 0.5405839681625366\n",
      "Epoch 8269/10000, Training Loss: 0.6436873078346252, Validation Loss: 0.6685709357261658\n",
      "Epoch 8270/10000, Training Loss: 0.6441141963005066, Validation Loss: 0.7897611260414124\n",
      "Epoch 8271/10000, Training Loss: 0.6855006814002991, Validation Loss: 0.8620546460151672\n",
      "Epoch 8272/10000, Training Loss: 0.6453990936279297, Validation Loss: 0.680300235748291\n",
      "Epoch 8273/10000, Training Loss: 0.5903907418251038, Validation Loss: 0.6159689426422119\n",
      "Epoch 8274/10000, Training Loss: 0.6392729878425598, Validation Loss: 0.7597478032112122\n",
      "Epoch 8275/10000, Training Loss: 0.6192436814308167, Validation Loss: 0.6326940059661865\n",
      "Epoch 8276/10000, Training Loss: 0.6654775142669678, Validation Loss: 0.8036854863166809\n",
      "Epoch 8277/10000, Training Loss: 0.695665717124939, Validation Loss: 0.7418538928031921\n",
      "Epoch 8278/10000, Training Loss: 0.6465316414833069, Validation Loss: 0.7667139172554016\n",
      "Epoch 8279/10000, Training Loss: 0.6493582129478455, Validation Loss: 0.6688933968544006\n",
      "Epoch 8280/10000, Training Loss: 0.6368497014045715, Validation Loss: 0.8417474627494812\n",
      "Epoch 8281/10000, Training Loss: 0.6579834222793579, Validation Loss: 0.7590136528015137\n",
      "Epoch 8282/10000, Training Loss: 0.6599172353744507, Validation Loss: 0.693211555480957\n",
      "Epoch 8283/10000, Training Loss: 0.6806482672691345, Validation Loss: 0.7988734841346741\n",
      "Epoch 8284/10000, Training Loss: 0.6365870237350464, Validation Loss: 0.6225106120109558\n",
      "Epoch 8285/10000, Training Loss: 0.6164877414703369, Validation Loss: 0.6434476375579834\n",
      "Epoch 8286/10000, Training Loss: 0.6324905753135681, Validation Loss: 0.9117262959480286\n",
      "Epoch 8287/10000, Training Loss: 0.6630207896232605, Validation Loss: 0.5931991934776306\n",
      "Epoch 8288/10000, Training Loss: 0.6304787397384644, Validation Loss: 0.728897750377655\n",
      "Epoch 8289/10000, Training Loss: 0.6520381569862366, Validation Loss: 0.7026169896125793\n",
      "Epoch 8290/10000, Training Loss: 0.6857125759124756, Validation Loss: 0.9055255055427551\n",
      "Epoch 8291/10000, Training Loss: 0.6675500273704529, Validation Loss: 1.1017276048660278\n",
      "Epoch 8292/10000, Training Loss: 0.652062714099884, Validation Loss: 0.619851291179657\n",
      "Epoch 8293/10000, Training Loss: 0.6100091934204102, Validation Loss: 0.7507419586181641\n",
      "Epoch 8294/10000, Training Loss: 0.6507142782211304, Validation Loss: 0.5033032894134521\n",
      "Epoch 8295/10000, Training Loss: 0.6449337005615234, Validation Loss: 0.5691125392913818\n",
      "Epoch 8296/10000, Training Loss: 0.6185595989227295, Validation Loss: 0.6288378238677979\n",
      "Epoch 8297/10000, Training Loss: 0.6162754893302917, Validation Loss: 0.6704495549201965\n",
      "Epoch 8298/10000, Training Loss: 0.6356900334358215, Validation Loss: 0.6314725279808044\n",
      "Epoch 8299/10000, Training Loss: 0.624958336353302, Validation Loss: 0.7701530456542969\n",
      "Epoch 8300/10000, Training Loss: 0.625721275806427, Validation Loss: 0.7178890705108643\n",
      "Epoch 8301/10000, Training Loss: 0.6179768443107605, Validation Loss: 0.6813520789146423\n",
      "Epoch 8302/10000, Training Loss: 0.6388064026832581, Validation Loss: 0.8312620520591736\n",
      "Epoch 8303/10000, Training Loss: 0.6537126898765564, Validation Loss: 0.6042271852493286\n",
      "Epoch 8304/10000, Training Loss: 0.6602107286453247, Validation Loss: 0.6162914037704468\n",
      "Epoch 8305/10000, Training Loss: 0.6552292704582214, Validation Loss: 0.6286251544952393\n",
      "Epoch 8306/10000, Training Loss: 0.6200836896896362, Validation Loss: 0.6447497010231018\n",
      "Epoch 8307/10000, Training Loss: 0.6389232277870178, Validation Loss: 0.899053156375885\n",
      "Epoch 8308/10000, Training Loss: 0.6412426233291626, Validation Loss: 0.6742167472839355\n",
      "Epoch 8309/10000, Training Loss: 0.6762173175811768, Validation Loss: 0.5773741602897644\n",
      "Epoch 8310/10000, Training Loss: 0.6451209783554077, Validation Loss: 0.9097502827644348\n",
      "Epoch 8311/10000, Training Loss: 0.6462230682373047, Validation Loss: 0.5970805287361145\n",
      "Epoch 8312/10000, Training Loss: 0.6048875451087952, Validation Loss: 0.7176091074943542\n",
      "Epoch 8313/10000, Training Loss: 0.6635359525680542, Validation Loss: 0.8420424461364746\n",
      "Epoch 8314/10000, Training Loss: 0.6690328121185303, Validation Loss: 0.6587021946907043\n",
      "Epoch 8315/10000, Training Loss: 0.6113932132720947, Validation Loss: 0.5850128531455994\n",
      "Epoch 8316/10000, Training Loss: 0.6398406624794006, Validation Loss: 0.6457864046096802\n",
      "Epoch 8317/10000, Training Loss: 0.6254343390464783, Validation Loss: 0.5886021852493286\n",
      "Epoch 8318/10000, Training Loss: 0.6667710542678833, Validation Loss: 0.6646670699119568\n",
      "Epoch 8319/10000, Training Loss: 0.6213104724884033, Validation Loss: 0.663057267665863\n",
      "Epoch 8320/10000, Training Loss: 0.6128088235855103, Validation Loss: 0.5766137838363647\n",
      "Epoch 8321/10000, Training Loss: 0.6145854592323303, Validation Loss: 0.6139836311340332\n",
      "Epoch 8322/10000, Training Loss: 0.6374136209487915, Validation Loss: 0.6058580875396729\n",
      "Epoch 8323/10000, Training Loss: 0.6629490256309509, Validation Loss: 0.7641788125038147\n",
      "Epoch 8324/10000, Training Loss: 0.7086479663848877, Validation Loss: 0.6011210680007935\n",
      "Epoch 8325/10000, Training Loss: 0.660873293876648, Validation Loss: 0.680480420589447\n",
      "Epoch 8326/10000, Training Loss: 0.6563820242881775, Validation Loss: 0.6345276832580566\n",
      "Epoch 8327/10000, Training Loss: 0.6192219257354736, Validation Loss: 0.6377235054969788\n",
      "Epoch 8328/10000, Training Loss: 0.6721134185791016, Validation Loss: 0.6273175477981567\n",
      "Epoch 8329/10000, Training Loss: 0.6422604918479919, Validation Loss: 0.8010923266410828\n",
      "Epoch 8330/10000, Training Loss: 0.6625826358795166, Validation Loss: 0.6286279559135437\n",
      "Epoch 8331/10000, Training Loss: 0.6130015850067139, Validation Loss: 0.6064085364341736\n",
      "Epoch 8332/10000, Training Loss: 0.6570676565170288, Validation Loss: 0.5593135356903076\n",
      "Epoch 8333/10000, Training Loss: 0.6636329293251038, Validation Loss: 0.8089379668235779\n",
      "Epoch 8334/10000, Training Loss: 0.6442550420761108, Validation Loss: 0.7459476590156555\n",
      "Epoch 8335/10000, Training Loss: 0.6682671904563904, Validation Loss: 0.7019946575164795\n",
      "Epoch 8336/10000, Training Loss: 0.6276684999465942, Validation Loss: 0.5825362801551819\n",
      "Epoch 8337/10000, Training Loss: 0.6638803482055664, Validation Loss: 0.5947790741920471\n",
      "Epoch 8338/10000, Training Loss: 0.6479408740997314, Validation Loss: 0.4808870553970337\n",
      "Epoch 8339/10000, Training Loss: 0.6493899822235107, Validation Loss: 0.6876270771026611\n",
      "Epoch 8340/10000, Training Loss: 0.6482639312744141, Validation Loss: 0.5598393082618713\n",
      "Epoch 8341/10000, Training Loss: 0.6628008484840393, Validation Loss: 0.7896676659584045\n",
      "Epoch 8342/10000, Training Loss: 0.6314703226089478, Validation Loss: 0.7288334369659424\n",
      "Epoch 8343/10000, Training Loss: 0.6154625415802002, Validation Loss: 0.6325786709785461\n",
      "Epoch 8344/10000, Training Loss: 0.6705726981163025, Validation Loss: 0.787142276763916\n",
      "Epoch 8345/10000, Training Loss: 0.6638184785842896, Validation Loss: 0.7078836560249329\n",
      "Epoch 8346/10000, Training Loss: 0.662316083908081, Validation Loss: 0.658566951751709\n",
      "Epoch 8347/10000, Training Loss: 0.6357520818710327, Validation Loss: 0.816042423248291\n",
      "Epoch 8348/10000, Training Loss: 0.6823819875717163, Validation Loss: 0.7755631804466248\n",
      "Epoch 8349/10000, Training Loss: 0.661001443862915, Validation Loss: 0.6372207403182983\n",
      "Epoch 8350/10000, Training Loss: 0.6220257878303528, Validation Loss: 0.5993813276290894\n",
      "Epoch 8351/10000, Training Loss: 0.6567445993423462, Validation Loss: 0.5143006443977356\n",
      "Epoch 8352/10000, Training Loss: 0.630186915397644, Validation Loss: 0.5134626626968384\n",
      "Epoch 8353/10000, Training Loss: 0.6400734186172485, Validation Loss: 0.6037612557411194\n",
      "Epoch 8354/10000, Training Loss: 0.6148732304573059, Validation Loss: 0.770489513874054\n",
      "Epoch 8355/10000, Training Loss: 0.6315844655036926, Validation Loss: 0.502476155757904\n",
      "Epoch 8356/10000, Training Loss: 0.6628510355949402, Validation Loss: 0.56939297914505\n",
      "Epoch 8357/10000, Training Loss: 0.6774759888648987, Validation Loss: 0.5746273398399353\n",
      "Epoch 8358/10000, Training Loss: 0.6026408672332764, Validation Loss: 0.7696310877799988\n",
      "Epoch 8359/10000, Training Loss: 0.6446530222892761, Validation Loss: 0.6021658182144165\n",
      "Epoch 8360/10000, Training Loss: 0.680971086025238, Validation Loss: 0.5870164632797241\n",
      "Epoch 8361/10000, Training Loss: 0.692295491695404, Validation Loss: 0.7339805960655212\n",
      "Epoch 8362/10000, Training Loss: 0.6227588057518005, Validation Loss: 0.5970703959465027\n",
      "Epoch 8363/10000, Training Loss: 0.6824347972869873, Validation Loss: 0.7071592807769775\n",
      "Epoch 8364/10000, Training Loss: 0.6371351480484009, Validation Loss: 0.6302723288536072\n",
      "Epoch 8365/10000, Training Loss: 0.6648566126823425, Validation Loss: 0.802173376083374\n",
      "Epoch 8366/10000, Training Loss: 0.6886825561523438, Validation Loss: 0.8477514386177063\n",
      "Epoch 8367/10000, Training Loss: 0.6058120131492615, Validation Loss: 0.7190559506416321\n",
      "Epoch 8368/10000, Training Loss: 0.6484693884849548, Validation Loss: 0.5622262954711914\n",
      "Epoch 8369/10000, Training Loss: 0.6621633768081665, Validation Loss: 0.6958863139152527\n",
      "Epoch 8370/10000, Training Loss: 0.6249732971191406, Validation Loss: 0.6499530673027039\n",
      "Epoch 8371/10000, Training Loss: 0.700212836265564, Validation Loss: 0.7234728336334229\n",
      "Epoch 8372/10000, Training Loss: 0.6459484696388245, Validation Loss: 0.6559975147247314\n",
      "Epoch 8373/10000, Training Loss: 0.6388728022575378, Validation Loss: 0.6063061356544495\n",
      "Epoch 8374/10000, Training Loss: 0.6219222545623779, Validation Loss: 0.6172613501548767\n",
      "Epoch 8375/10000, Training Loss: 0.6628319025039673, Validation Loss: 0.6622666716575623\n",
      "Epoch 8376/10000, Training Loss: 0.6367879509925842, Validation Loss: 0.6314110159873962\n",
      "Epoch 8377/10000, Training Loss: 0.6547737717628479, Validation Loss: 0.792400062084198\n",
      "Epoch 8378/10000, Training Loss: 0.6274273991584778, Validation Loss: 0.7410896420478821\n",
      "Epoch 8379/10000, Training Loss: 0.6545400619506836, Validation Loss: 0.6734967827796936\n",
      "Epoch 8380/10000, Training Loss: 0.6846304535865784, Validation Loss: 0.8583788275718689\n",
      "Epoch 8381/10000, Training Loss: 0.6567043662071228, Validation Loss: 0.8003304600715637\n",
      "Epoch 8382/10000, Training Loss: 0.5968711972236633, Validation Loss: 0.7067767977714539\n",
      "Epoch 8383/10000, Training Loss: 0.6281735301017761, Validation Loss: 0.670950710773468\n",
      "Epoch 8384/10000, Training Loss: 0.6106558442115784, Validation Loss: 0.6845504641532898\n",
      "Epoch 8385/10000, Training Loss: 0.6435332894325256, Validation Loss: 0.6939933896064758\n",
      "Epoch 8386/10000, Training Loss: 0.6473740339279175, Validation Loss: 0.7086806297302246\n",
      "Epoch 8387/10000, Training Loss: 0.6455640196800232, Validation Loss: 0.8155946135520935\n",
      "Epoch 8388/10000, Training Loss: 0.6373742818832397, Validation Loss: 0.7000503540039062\n",
      "Epoch 8389/10000, Training Loss: 0.6771441102027893, Validation Loss: 0.5776435732841492\n",
      "Epoch 8390/10000, Training Loss: 0.6537757515907288, Validation Loss: 0.5975788235664368\n",
      "Epoch 8391/10000, Training Loss: 0.631018340587616, Validation Loss: 0.7136836051940918\n",
      "Epoch 8392/10000, Training Loss: 0.6262223720550537, Validation Loss: 0.561238706111908\n",
      "Epoch 8393/10000, Training Loss: 0.6413619518280029, Validation Loss: 0.6224070191383362\n",
      "Epoch 8394/10000, Training Loss: 0.6400899887084961, Validation Loss: 0.7899045944213867\n",
      "Epoch 8395/10000, Training Loss: 0.6376793384552002, Validation Loss: 0.7397434115409851\n",
      "Epoch 8396/10000, Training Loss: 0.6694647669792175, Validation Loss: 0.7466259598731995\n",
      "Epoch 8397/10000, Training Loss: 0.6597460508346558, Validation Loss: 0.7739348411560059\n",
      "Epoch 8398/10000, Training Loss: 0.6374728679656982, Validation Loss: 0.8807013630867004\n",
      "Epoch 8399/10000, Training Loss: 0.6288989186286926, Validation Loss: 0.5900924801826477\n",
      "Epoch 8400/10000, Training Loss: 0.6472550630569458, Validation Loss: 1.00937020778656\n",
      "Epoch 8401/10000, Training Loss: 0.6636696457862854, Validation Loss: 0.6204603910446167\n",
      "Epoch 8402/10000, Training Loss: 0.6470534801483154, Validation Loss: 0.6399884223937988\n",
      "Epoch 8403/10000, Training Loss: 0.6252070069313049, Validation Loss: 0.7725846171379089\n",
      "Epoch 8404/10000, Training Loss: 0.6306518316268921, Validation Loss: 0.7206732630729675\n",
      "Epoch 8405/10000, Training Loss: 0.6413544416427612, Validation Loss: 0.6164294481277466\n",
      "Epoch 8406/10000, Training Loss: 0.671846330165863, Validation Loss: 0.5705768465995789\n",
      "Epoch 8407/10000, Training Loss: 0.6708714962005615, Validation Loss: 0.7876066565513611\n",
      "Epoch 8408/10000, Training Loss: 0.6275336146354675, Validation Loss: 0.7229864001274109\n",
      "Epoch 8409/10000, Training Loss: 0.6475132703781128, Validation Loss: 0.8175004124641418\n",
      "Epoch 8410/10000, Training Loss: 0.6421562433242798, Validation Loss: 0.7036389708518982\n",
      "Epoch 8411/10000, Training Loss: 0.6499449014663696, Validation Loss: 0.8980641961097717\n",
      "Epoch 8412/10000, Training Loss: 0.6358689665794373, Validation Loss: 0.7800511717796326\n",
      "Epoch 8413/10000, Training Loss: 0.6574404835700989, Validation Loss: 1.0190941095352173\n",
      "Epoch 8414/10000, Training Loss: 0.6549361944198608, Validation Loss: 0.6601524353027344\n",
      "Epoch 8415/10000, Training Loss: 0.6449594497680664, Validation Loss: 0.5858864188194275\n",
      "Epoch 8416/10000, Training Loss: 0.6204529404640198, Validation Loss: 0.6284058094024658\n",
      "Epoch 8417/10000, Training Loss: 0.6418293714523315, Validation Loss: 0.5974987745285034\n",
      "Epoch 8418/10000, Training Loss: 0.6372501850128174, Validation Loss: 0.7134778499603271\n",
      "Epoch 8419/10000, Training Loss: 0.6680296659469604, Validation Loss: 1.4815818071365356\n",
      "Epoch 8420/10000, Training Loss: 0.6338350772857666, Validation Loss: 0.6032130122184753\n",
      "Epoch 8421/10000, Training Loss: 0.6441044211387634, Validation Loss: 0.6352097392082214\n",
      "Epoch 8422/10000, Training Loss: 0.6327799558639526, Validation Loss: 0.6397257447242737\n",
      "Epoch 8423/10000, Training Loss: 0.6355105638504028, Validation Loss: 0.6282461285591125\n",
      "Epoch 8424/10000, Training Loss: 0.6279574632644653, Validation Loss: 0.7139062285423279\n",
      "Epoch 8425/10000, Training Loss: 0.6372588872909546, Validation Loss: 0.7347913384437561\n",
      "Epoch 8426/10000, Training Loss: 0.6615254282951355, Validation Loss: 0.6518234610557556\n",
      "Epoch 8427/10000, Training Loss: 0.6455398201942444, Validation Loss: 0.6761351227760315\n",
      "Epoch 8428/10000, Training Loss: 0.6885116100311279, Validation Loss: 0.649317741394043\n",
      "Epoch 8429/10000, Training Loss: 0.6379318833351135, Validation Loss: 0.63785320520401\n",
      "Epoch 8430/10000, Training Loss: 0.701804518699646, Validation Loss: 0.5145348906517029\n",
      "Epoch 8431/10000, Training Loss: 0.606681227684021, Validation Loss: 0.8639331459999084\n",
      "Epoch 8432/10000, Training Loss: 0.6618942618370056, Validation Loss: 0.834216833114624\n",
      "Epoch 8433/10000, Training Loss: 0.6136338710784912, Validation Loss: 0.5687084794044495\n",
      "Epoch 8434/10000, Training Loss: 0.6225897073745728, Validation Loss: 0.7154666781425476\n",
      "Epoch 8435/10000, Training Loss: 0.6307451725006104, Validation Loss: 0.8094058632850647\n",
      "Epoch 8436/10000, Training Loss: 0.5880886316299438, Validation Loss: 0.9322783350944519\n",
      "Epoch 8437/10000, Training Loss: 0.601033627986908, Validation Loss: 0.8094800114631653\n",
      "Epoch 8438/10000, Training Loss: 0.6428444385528564, Validation Loss: 0.6058092713356018\n",
      "Epoch 8439/10000, Training Loss: 0.6097621321678162, Validation Loss: 0.8522966504096985\n",
      "Epoch 8440/10000, Training Loss: 0.635371208190918, Validation Loss: 0.7195134162902832\n",
      "Epoch 8441/10000, Training Loss: 0.6203728318214417, Validation Loss: 0.7236542105674744\n",
      "Epoch 8442/10000, Training Loss: 0.6848163604736328, Validation Loss: 0.643069326877594\n",
      "Epoch 8443/10000, Training Loss: 0.6124334335327148, Validation Loss: 0.8304648995399475\n",
      "Epoch 8444/10000, Training Loss: 0.6613585352897644, Validation Loss: 0.48726633191108704\n",
      "Epoch 8445/10000, Training Loss: 0.6372295022010803, Validation Loss: 0.8807532787322998\n",
      "Epoch 8446/10000, Training Loss: 0.6296776533126831, Validation Loss: 0.7754920125007629\n",
      "Epoch 8447/10000, Training Loss: 0.6230310797691345, Validation Loss: 0.6312970519065857\n",
      "Epoch 8448/10000, Training Loss: 0.6514142751693726, Validation Loss: 0.45252907276153564\n",
      "Epoch 8449/10000, Training Loss: 0.6482678651809692, Validation Loss: 0.813991367816925\n",
      "Epoch 8450/10000, Training Loss: 0.6277720928192139, Validation Loss: 0.7541499733924866\n",
      "Epoch 8451/10000, Training Loss: 0.6581379771232605, Validation Loss: 0.7737419009208679\n",
      "Epoch 8452/10000, Training Loss: 0.6308010816574097, Validation Loss: 0.611443042755127\n",
      "Epoch 8453/10000, Training Loss: 0.6363461017608643, Validation Loss: 0.7166711688041687\n",
      "Epoch 8454/10000, Training Loss: 0.6392955183982849, Validation Loss: 0.8681135773658752\n",
      "Epoch 8455/10000, Training Loss: 0.630058228969574, Validation Loss: 0.6285132765769958\n",
      "Epoch 8456/10000, Training Loss: 0.6152241230010986, Validation Loss: 0.6958670616149902\n",
      "Epoch 8457/10000, Training Loss: 0.642417848110199, Validation Loss: 0.6789181232452393\n",
      "Epoch 8458/10000, Training Loss: 0.6941931247711182, Validation Loss: 0.6490437388420105\n",
      "Epoch 8459/10000, Training Loss: 0.6162499189376831, Validation Loss: 0.5972135663032532\n",
      "Epoch 8460/10000, Training Loss: 0.6379113793373108, Validation Loss: 0.6328578591346741\n",
      "Epoch 8461/10000, Training Loss: 0.6778427958488464, Validation Loss: 0.9987004399299622\n",
      "Epoch 8462/10000, Training Loss: 0.6292946338653564, Validation Loss: 0.8268201947212219\n",
      "Epoch 8463/10000, Training Loss: 0.6856316924095154, Validation Loss: 0.6274864077568054\n",
      "Epoch 8464/10000, Training Loss: 0.6287241578102112, Validation Loss: 0.5938053727149963\n",
      "Epoch 8465/10000, Training Loss: 0.6334985494613647, Validation Loss: 0.7080392241477966\n",
      "Epoch 8466/10000, Training Loss: 0.6457312703132629, Validation Loss: 0.9397470355033875\n",
      "Epoch 8467/10000, Training Loss: 0.6309579610824585, Validation Loss: 0.8247663378715515\n",
      "Epoch 8468/10000, Training Loss: 0.6770431995391846, Validation Loss: 0.6839621663093567\n",
      "Epoch 8469/10000, Training Loss: 0.6388286352157593, Validation Loss: 0.6192635893821716\n",
      "Epoch 8470/10000, Training Loss: 0.6554428935050964, Validation Loss: 0.7694136500358582\n",
      "Epoch 8471/10000, Training Loss: 0.6642621159553528, Validation Loss: 0.5394533276557922\n",
      "Epoch 8472/10000, Training Loss: 0.6277791261672974, Validation Loss: 0.6941515803337097\n",
      "Epoch 8473/10000, Training Loss: 0.6514403223991394, Validation Loss: 0.6746169924736023\n",
      "Epoch 8474/10000, Training Loss: 0.6677332520484924, Validation Loss: 0.9717580676078796\n",
      "Epoch 8475/10000, Training Loss: 0.6560330986976624, Validation Loss: 0.6550657749176025\n",
      "Epoch 8476/10000, Training Loss: 0.6538561582565308, Validation Loss: 0.6380679607391357\n",
      "Epoch 8477/10000, Training Loss: 0.677405595779419, Validation Loss: 0.7515951991081238\n",
      "Epoch 8478/10000, Training Loss: 0.6505717635154724, Validation Loss: 0.5851389765739441\n",
      "Epoch 8479/10000, Training Loss: 0.6620518565177917, Validation Loss: 0.7139580845832825\n",
      "Epoch 8480/10000, Training Loss: 0.6416116952896118, Validation Loss: 0.7772162556648254\n",
      "Epoch 8481/10000, Training Loss: 0.6407182216644287, Validation Loss: 0.7354178428649902\n",
      "Epoch 8482/10000, Training Loss: 0.6325167417526245, Validation Loss: 0.7149076461791992\n",
      "Epoch 8483/10000, Training Loss: 0.639274001121521, Validation Loss: 0.6902737021446228\n",
      "Epoch 8484/10000, Training Loss: 0.6065908670425415, Validation Loss: 0.6706452369689941\n",
      "Epoch 8485/10000, Training Loss: 0.679417073726654, Validation Loss: 0.660353422164917\n",
      "Epoch 8486/10000, Training Loss: 0.6442526578903198, Validation Loss: 0.4640680253505707\n",
      "Epoch 8487/10000, Training Loss: 0.6415023803710938, Validation Loss: 0.6718485355377197\n",
      "Epoch 8488/10000, Training Loss: 0.6106393933296204, Validation Loss: 0.6836483478546143\n",
      "Epoch 8489/10000, Training Loss: 0.6518229246139526, Validation Loss: 0.64486163854599\n",
      "Epoch 8490/10000, Training Loss: 0.6378085017204285, Validation Loss: 0.6756748557090759\n",
      "Epoch 8491/10000, Training Loss: 0.6613511443138123, Validation Loss: 0.6283156871795654\n",
      "Epoch 8492/10000, Training Loss: 0.6611815690994263, Validation Loss: 0.7292981147766113\n",
      "Epoch 8493/10000, Training Loss: 0.6606795191764832, Validation Loss: 0.7480747103691101\n",
      "Epoch 8494/10000, Training Loss: 0.6152689456939697, Validation Loss: 0.7576651573181152\n",
      "Epoch 8495/10000, Training Loss: 0.6624663472175598, Validation Loss: 0.6464338898658752\n",
      "Epoch 8496/10000, Training Loss: 0.6318044662475586, Validation Loss: 1.0276082754135132\n",
      "Epoch 8497/10000, Training Loss: 0.6514520645141602, Validation Loss: 0.9166852831840515\n",
      "Epoch 8498/10000, Training Loss: 0.6648843288421631, Validation Loss: 0.8198170065879822\n",
      "Epoch 8499/10000, Training Loss: 0.6202110052108765, Validation Loss: 0.6408594846725464\n",
      "Epoch 8500/10000, Training Loss: 0.6202341318130493, Validation Loss: 0.6171286702156067\n",
      "Epoch 8501/10000, Training Loss: 0.6847565174102783, Validation Loss: 0.6850420832633972\n",
      "Epoch 8502/10000, Training Loss: 0.6427225470542908, Validation Loss: 0.7428486347198486\n",
      "Epoch 8503/10000, Training Loss: 0.6446820497512817, Validation Loss: 0.6739926338195801\n",
      "Epoch 8504/10000, Training Loss: 0.6718927025794983, Validation Loss: 0.5942850708961487\n",
      "Epoch 8505/10000, Training Loss: 0.6585718989372253, Validation Loss: 0.8020266890525818\n",
      "Epoch 8506/10000, Training Loss: 0.6463387608528137, Validation Loss: 0.7251942157745361\n",
      "Epoch 8507/10000, Training Loss: 0.7005135416984558, Validation Loss: 0.7419171333312988\n",
      "Epoch 8508/10000, Training Loss: 0.6395405530929565, Validation Loss: 0.6786491274833679\n",
      "Epoch 8509/10000, Training Loss: 0.6378206014633179, Validation Loss: 0.7266865372657776\n",
      "Epoch 8510/10000, Training Loss: 0.6308028101921082, Validation Loss: 0.7017462849617004\n",
      "Epoch 8511/10000, Training Loss: 0.6894190907478333, Validation Loss: 0.7498335242271423\n",
      "Epoch 8512/10000, Training Loss: 0.665197491645813, Validation Loss: 0.6534046530723572\n",
      "Epoch 8513/10000, Training Loss: 0.6170110702514648, Validation Loss: 0.7784061431884766\n",
      "Epoch 8514/10000, Training Loss: 0.65351402759552, Validation Loss: 0.6221941709518433\n",
      "Epoch 8515/10000, Training Loss: 0.7001867890357971, Validation Loss: 0.9269738793373108\n",
      "Epoch 8516/10000, Training Loss: 0.649921178817749, Validation Loss: 0.9145415425300598\n",
      "Epoch 8517/10000, Training Loss: 0.6389102339744568, Validation Loss: 0.7042286396026611\n",
      "Epoch 8518/10000, Training Loss: 0.6337642073631287, Validation Loss: 0.6096723079681396\n",
      "Epoch 8519/10000, Training Loss: 0.6312090158462524, Validation Loss: 0.5882969498634338\n",
      "Epoch 8520/10000, Training Loss: 0.5882619619369507, Validation Loss: 0.5030031800270081\n",
      "Epoch 8521/10000, Training Loss: 0.6228709816932678, Validation Loss: 0.73148113489151\n",
      "Epoch 8522/10000, Training Loss: 0.6721571683883667, Validation Loss: 0.5989570021629333\n",
      "Epoch 8523/10000, Training Loss: 0.64005047082901, Validation Loss: 0.7479894161224365\n",
      "Epoch 8524/10000, Training Loss: 0.6467037796974182, Validation Loss: 0.6900562644004822\n",
      "Epoch 8525/10000, Training Loss: 0.6109646558761597, Validation Loss: 0.5401949286460876\n",
      "Epoch 8526/10000, Training Loss: 0.6394701600074768, Validation Loss: 0.625253438949585\n",
      "Epoch 8527/10000, Training Loss: 0.6703701019287109, Validation Loss: 0.6815123558044434\n",
      "Epoch 8528/10000, Training Loss: 0.6794232726097107, Validation Loss: 0.6785168647766113\n",
      "Epoch 8529/10000, Training Loss: 0.6459211111068726, Validation Loss: 0.6433131694793701\n",
      "Epoch 8530/10000, Training Loss: 0.6330103278160095, Validation Loss: 0.6337180733680725\n",
      "Epoch 8531/10000, Training Loss: 0.6634617447853088, Validation Loss: 0.6425716876983643\n",
      "Epoch 8532/10000, Training Loss: 0.6295690536499023, Validation Loss: 0.5692419409751892\n",
      "Epoch 8533/10000, Training Loss: 0.6594556570053101, Validation Loss: 0.5985454320907593\n",
      "Epoch 8534/10000, Training Loss: 0.6504966020584106, Validation Loss: 0.8399367332458496\n",
      "Epoch 8535/10000, Training Loss: 0.6657686233520508, Validation Loss: 0.7799370884895325\n",
      "Epoch 8536/10000, Training Loss: 0.6272745728492737, Validation Loss: 0.5602084398269653\n",
      "Epoch 8537/10000, Training Loss: 0.6702073812484741, Validation Loss: 0.6174213290214539\n",
      "Epoch 8538/10000, Training Loss: 0.631330132484436, Validation Loss: 0.7862675189971924\n",
      "Epoch 8539/10000, Training Loss: 0.6244906187057495, Validation Loss: 0.6980721354484558\n",
      "Epoch 8540/10000, Training Loss: 0.6503718495368958, Validation Loss: 0.5708245635032654\n",
      "Epoch 8541/10000, Training Loss: 0.641132652759552, Validation Loss: 0.6310632228851318\n",
      "Epoch 8542/10000, Training Loss: 0.635794997215271, Validation Loss: 0.6749250888824463\n",
      "Epoch 8543/10000, Training Loss: 0.6401493549346924, Validation Loss: 0.950139582157135\n",
      "Epoch 8544/10000, Training Loss: 0.6340404152870178, Validation Loss: 0.8218002319335938\n",
      "Epoch 8545/10000, Training Loss: 0.7045708298683167, Validation Loss: 0.48768869042396545\n",
      "Epoch 8546/10000, Training Loss: 0.6399037837982178, Validation Loss: 0.7744255661964417\n",
      "Epoch 8547/10000, Training Loss: 0.6180273294448853, Validation Loss: 0.6136669516563416\n",
      "Epoch 8548/10000, Training Loss: 0.6295496821403503, Validation Loss: 0.6265293955802917\n",
      "Epoch 8549/10000, Training Loss: 0.6553220748901367, Validation Loss: 0.7667012810707092\n",
      "Epoch 8550/10000, Training Loss: 0.6309580206871033, Validation Loss: 0.5756808519363403\n",
      "Epoch 8551/10000, Training Loss: 0.6550692319869995, Validation Loss: 0.6894292235374451\n",
      "Epoch 8552/10000, Training Loss: 0.6653311848640442, Validation Loss: 0.6542107462882996\n",
      "Epoch 8553/10000, Training Loss: 0.5990680456161499, Validation Loss: 0.6708628535270691\n",
      "Epoch 8554/10000, Training Loss: 0.6482973694801331, Validation Loss: 0.697002649307251\n",
      "Epoch 8555/10000, Training Loss: 0.6774540543556213, Validation Loss: 0.708608865737915\n",
      "Epoch 8556/10000, Training Loss: 0.659334123134613, Validation Loss: 0.7560051083564758\n",
      "Epoch 8557/10000, Training Loss: 0.6843078136444092, Validation Loss: 0.5422462821006775\n",
      "Epoch 8558/10000, Training Loss: 0.6280666589736938, Validation Loss: 0.7617622017860413\n",
      "Epoch 8559/10000, Training Loss: 0.6583918333053589, Validation Loss: 0.7347431182861328\n",
      "Epoch 8560/10000, Training Loss: 0.6302734017372131, Validation Loss: 0.5907706618309021\n",
      "Epoch 8561/10000, Training Loss: 0.6643410921096802, Validation Loss: 0.6311042308807373\n",
      "Epoch 8562/10000, Training Loss: 0.6854524612426758, Validation Loss: 0.9263216853141785\n",
      "Epoch 8563/10000, Training Loss: 0.619346022605896, Validation Loss: 0.7192932963371277\n",
      "Epoch 8564/10000, Training Loss: 0.6527532339096069, Validation Loss: 0.6862422823905945\n",
      "Epoch 8565/10000, Training Loss: 0.6220963001251221, Validation Loss: 0.6324437856674194\n",
      "Epoch 8566/10000, Training Loss: 0.6756134033203125, Validation Loss: 0.6423878073692322\n",
      "Epoch 8567/10000, Training Loss: 0.5987001061439514, Validation Loss: 0.8179115653038025\n",
      "Epoch 8568/10000, Training Loss: 0.607058584690094, Validation Loss: 0.5757516622543335\n",
      "Epoch 8569/10000, Training Loss: 0.6363494396209717, Validation Loss: 0.8193261027336121\n",
      "Epoch 8570/10000, Training Loss: 0.6534205079078674, Validation Loss: 0.6640450358390808\n",
      "Epoch 8571/10000, Training Loss: 0.6791325211524963, Validation Loss: 0.6225992441177368\n",
      "Epoch 8572/10000, Training Loss: 0.6723172664642334, Validation Loss: 0.7485297322273254\n",
      "Epoch 8573/10000, Training Loss: 0.6471090912818909, Validation Loss: 0.5924458503723145\n",
      "Epoch 8574/10000, Training Loss: 0.6271457672119141, Validation Loss: 0.6046448349952698\n",
      "Epoch 8575/10000, Training Loss: 0.6648442149162292, Validation Loss: 0.9023206830024719\n",
      "Epoch 8576/10000, Training Loss: 0.6191967725753784, Validation Loss: 0.6688957214355469\n",
      "Epoch 8577/10000, Training Loss: 0.6172082424163818, Validation Loss: 0.652101457118988\n",
      "Epoch 8578/10000, Training Loss: 0.6398414969444275, Validation Loss: 0.5934073328971863\n",
      "Epoch 8579/10000, Training Loss: 0.6427893042564392, Validation Loss: 1.357912540435791\n",
      "Epoch 8580/10000, Training Loss: 0.6621042490005493, Validation Loss: 0.6963160634040833\n",
      "Epoch 8581/10000, Training Loss: 0.6425853967666626, Validation Loss: 0.9301550388336182\n",
      "Epoch 8582/10000, Training Loss: 0.6016112565994263, Validation Loss: 0.528322160243988\n",
      "Epoch 8583/10000, Training Loss: 0.6899235248565674, Validation Loss: 0.6827932000160217\n",
      "Epoch 8584/10000, Training Loss: 0.5941749215126038, Validation Loss: 0.6855859756469727\n",
      "Epoch 8585/10000, Training Loss: 0.6051532626152039, Validation Loss: 0.6169711947441101\n",
      "Epoch 8586/10000, Training Loss: 0.6562889814376831, Validation Loss: 0.7188161015510559\n",
      "Epoch 8587/10000, Training Loss: 0.6546666026115417, Validation Loss: 0.7569590210914612\n",
      "Epoch 8588/10000, Training Loss: 0.6613848805427551, Validation Loss: 0.5557911992073059\n",
      "Epoch 8589/10000, Training Loss: 0.6445855498313904, Validation Loss: 0.6962929368019104\n",
      "Epoch 8590/10000, Training Loss: 0.6822202205657959, Validation Loss: 0.7321028113365173\n",
      "Epoch 8591/10000, Training Loss: 0.6498517990112305, Validation Loss: 0.7644786238670349\n",
      "Epoch 8592/10000, Training Loss: 0.6352325677871704, Validation Loss: 0.795089066028595\n",
      "Epoch 8593/10000, Training Loss: 0.6333391666412354, Validation Loss: 0.6627830862998962\n",
      "Epoch 8594/10000, Training Loss: 0.6529947519302368, Validation Loss: 0.6774862408638\n",
      "Epoch 8595/10000, Training Loss: 0.6161817908287048, Validation Loss: 0.7819013595581055\n",
      "Epoch 8596/10000, Training Loss: 0.6370563507080078, Validation Loss: 0.8872483372688293\n",
      "Epoch 8597/10000, Training Loss: 0.6310996413230896, Validation Loss: 0.6459863781929016\n",
      "Epoch 8598/10000, Training Loss: 0.6325334310531616, Validation Loss: 0.5743311047554016\n",
      "Epoch 8599/10000, Training Loss: 0.6279577612876892, Validation Loss: 0.7439741492271423\n",
      "Epoch 8600/10000, Training Loss: 0.620436429977417, Validation Loss: 0.6570397615432739\n",
      "Epoch 8601/10000, Training Loss: 0.6318985819816589, Validation Loss: 0.7076902985572815\n",
      "Epoch 8602/10000, Training Loss: 0.6793097257614136, Validation Loss: 0.8071227073669434\n",
      "Epoch 8603/10000, Training Loss: 0.6714479923248291, Validation Loss: 0.7429284453392029\n",
      "Epoch 8604/10000, Training Loss: 0.6264383792877197, Validation Loss: 0.6414456963539124\n",
      "Epoch 8605/10000, Training Loss: 0.6208966970443726, Validation Loss: 0.7524399161338806\n",
      "Epoch 8606/10000, Training Loss: 0.6690971851348877, Validation Loss: 0.7587361335754395\n",
      "Epoch 8607/10000, Training Loss: 0.6547146439552307, Validation Loss: 0.6951138973236084\n",
      "Epoch 8608/10000, Training Loss: 0.6853915452957153, Validation Loss: 0.644836962223053\n",
      "Epoch 8609/10000, Training Loss: 0.6107351779937744, Validation Loss: 0.5699048638343811\n",
      "Epoch 8610/10000, Training Loss: 0.6541277766227722, Validation Loss: 0.8087370991706848\n",
      "Epoch 8611/10000, Training Loss: 0.6703712940216064, Validation Loss: 0.7941563725471497\n",
      "Epoch 8612/10000, Training Loss: 0.6586417555809021, Validation Loss: 0.9142541289329529\n",
      "Epoch 8613/10000, Training Loss: 0.6551424860954285, Validation Loss: 0.7424690127372742\n",
      "Epoch 8614/10000, Training Loss: 0.6173709630966187, Validation Loss: 0.6433512568473816\n",
      "Epoch 8615/10000, Training Loss: 0.6123850345611572, Validation Loss: 0.6326837539672852\n",
      "Epoch 8616/10000, Training Loss: 0.5987128615379333, Validation Loss: 0.673520565032959\n",
      "Epoch 8617/10000, Training Loss: 0.6165422797203064, Validation Loss: 0.6872885823249817\n",
      "Epoch 8618/10000, Training Loss: 0.6930668354034424, Validation Loss: 0.6597889065742493\n",
      "Epoch 8619/10000, Training Loss: 0.6345665454864502, Validation Loss: 0.8023288249969482\n",
      "Epoch 8620/10000, Training Loss: 0.6491183042526245, Validation Loss: 0.641681969165802\n",
      "Epoch 8621/10000, Training Loss: 0.6438312530517578, Validation Loss: 0.5882681012153625\n",
      "Epoch 8622/10000, Training Loss: 0.614035427570343, Validation Loss: 0.5025437474250793\n",
      "Epoch 8623/10000, Training Loss: 0.6292555928230286, Validation Loss: 0.6337394118309021\n",
      "Epoch 8624/10000, Training Loss: 0.6543131470680237, Validation Loss: 0.5415979027748108\n",
      "Epoch 8625/10000, Training Loss: 0.6083396077156067, Validation Loss: 0.6694745421409607\n",
      "Epoch 8626/10000, Training Loss: 0.5712525248527527, Validation Loss: 0.6586828827857971\n",
      "Epoch 8627/10000, Training Loss: 0.6600014567375183, Validation Loss: 0.564971387386322\n",
      "Epoch 8628/10000, Training Loss: 0.6305569410324097, Validation Loss: 0.6380649209022522\n",
      "Epoch 8629/10000, Training Loss: 0.6642760038375854, Validation Loss: 0.6686367988586426\n",
      "Epoch 8630/10000, Training Loss: 0.6310025453567505, Validation Loss: 0.6829783916473389\n",
      "Epoch 8631/10000, Training Loss: 0.633112370967865, Validation Loss: 0.825782299041748\n",
      "Epoch 8632/10000, Training Loss: 0.6096929311752319, Validation Loss: 0.7548437714576721\n",
      "Epoch 8633/10000, Training Loss: 0.6356748938560486, Validation Loss: 0.5906813144683838\n",
      "Epoch 8634/10000, Training Loss: 0.6519274115562439, Validation Loss: 0.7029004693031311\n",
      "Epoch 8635/10000, Training Loss: 0.6442942023277283, Validation Loss: 0.6627242565155029\n",
      "Epoch 8636/10000, Training Loss: 0.6059328317642212, Validation Loss: 0.5181159973144531\n",
      "Epoch 8637/10000, Training Loss: 0.6560803651809692, Validation Loss: 0.6689736247062683\n",
      "Epoch 8638/10000, Training Loss: 0.6189936995506287, Validation Loss: 0.5641752481460571\n",
      "Epoch 8639/10000, Training Loss: 0.6226609349250793, Validation Loss: 0.6941378116607666\n",
      "Epoch 8640/10000, Training Loss: 0.6327599287033081, Validation Loss: 0.7450941205024719\n",
      "Epoch 8641/10000, Training Loss: 0.6589813232421875, Validation Loss: 0.6478175520896912\n",
      "Epoch 8642/10000, Training Loss: 0.6331130266189575, Validation Loss: 0.6639890074729919\n",
      "Epoch 8643/10000, Training Loss: 0.6686707735061646, Validation Loss: 0.6172593832015991\n",
      "Epoch 8644/10000, Training Loss: 0.6384563446044922, Validation Loss: 0.7628226280212402\n",
      "Epoch 8645/10000, Training Loss: 0.6785142421722412, Validation Loss: 0.5970657467842102\n",
      "Epoch 8646/10000, Training Loss: 0.6328257322311401, Validation Loss: 0.9001578688621521\n",
      "Epoch 8647/10000, Training Loss: 0.6614226698875427, Validation Loss: 0.630255401134491\n",
      "Epoch 8648/10000, Training Loss: 0.6356131434440613, Validation Loss: 0.6355165839195251\n",
      "Epoch 8649/10000, Training Loss: 0.6605529189109802, Validation Loss: 0.7582523226737976\n",
      "Epoch 8650/10000, Training Loss: 0.6093182563781738, Validation Loss: 0.6217992901802063\n",
      "Epoch 8651/10000, Training Loss: 0.6403035521507263, Validation Loss: 0.7239232659339905\n",
      "Epoch 8652/10000, Training Loss: 0.6423807740211487, Validation Loss: 0.6949396729469299\n",
      "Epoch 8653/10000, Training Loss: 0.6360675096511841, Validation Loss: 0.7822887301445007\n",
      "Epoch 8654/10000, Training Loss: 0.623382568359375, Validation Loss: 0.8137753009796143\n",
      "Epoch 8655/10000, Training Loss: 0.6186109781265259, Validation Loss: 0.7448727488517761\n",
      "Epoch 8656/10000, Training Loss: 0.6962683200836182, Validation Loss: 0.6417420506477356\n",
      "Epoch 8657/10000, Training Loss: 0.6510452628135681, Validation Loss: 0.7269983887672424\n",
      "Epoch 8658/10000, Training Loss: 0.6130198240280151, Validation Loss: 0.6721346974372864\n",
      "Epoch 8659/10000, Training Loss: 0.6267166137695312, Validation Loss: 0.6649559736251831\n",
      "Epoch 8660/10000, Training Loss: 0.6443368196487427, Validation Loss: 0.7985957264900208\n",
      "Epoch 8661/10000, Training Loss: 0.6288241147994995, Validation Loss: 0.6809484362602234\n",
      "Epoch 8662/10000, Training Loss: 0.6463900804519653, Validation Loss: 0.6261467337608337\n",
      "Epoch 8663/10000, Training Loss: 0.6582527160644531, Validation Loss: 0.6737150549888611\n",
      "Epoch 8664/10000, Training Loss: 0.6506659388542175, Validation Loss: 0.6952633857727051\n",
      "Epoch 8665/10000, Training Loss: 0.6465559601783752, Validation Loss: 0.7371004223823547\n",
      "Epoch 8666/10000, Training Loss: 0.6247758269309998, Validation Loss: 0.719770610332489\n",
      "Epoch 8667/10000, Training Loss: 0.6302512884140015, Validation Loss: 0.5468634963035583\n",
      "Epoch 8668/10000, Training Loss: 0.6299813985824585, Validation Loss: 0.6525733470916748\n",
      "Epoch 8669/10000, Training Loss: 0.6495048999786377, Validation Loss: 0.7631099820137024\n",
      "Epoch 8670/10000, Training Loss: 0.6081264019012451, Validation Loss: 0.5723694562911987\n",
      "Epoch 8671/10000, Training Loss: 0.6479036808013916, Validation Loss: 0.7860106825828552\n",
      "Epoch 8672/10000, Training Loss: 0.6594828367233276, Validation Loss: 0.9983047842979431\n",
      "Epoch 8673/10000, Training Loss: 0.668754518032074, Validation Loss: 0.4352027177810669\n",
      "Epoch 8674/10000, Training Loss: 0.6144572496414185, Validation Loss: 0.7045230269432068\n",
      "Epoch 8675/10000, Training Loss: 0.7300280928611755, Validation Loss: 0.8339352607727051\n",
      "Epoch 8676/10000, Training Loss: 0.6566741466522217, Validation Loss: 0.699547529220581\n",
      "Epoch 8677/10000, Training Loss: 0.6332705616950989, Validation Loss: 0.646312952041626\n",
      "Epoch 8678/10000, Training Loss: 0.639866292476654, Validation Loss: 0.6643490195274353\n",
      "Epoch 8679/10000, Training Loss: 0.6484934091567993, Validation Loss: 0.7052800059318542\n",
      "Epoch 8680/10000, Training Loss: 0.655515730381012, Validation Loss: 0.7474825978279114\n",
      "Epoch 8681/10000, Training Loss: 0.5970011353492737, Validation Loss: 0.5676483511924744\n",
      "Epoch 8682/10000, Training Loss: 0.6547164916992188, Validation Loss: 0.9066210389137268\n",
      "Epoch 8683/10000, Training Loss: 0.6043986678123474, Validation Loss: 0.6764548420906067\n",
      "Epoch 8684/10000, Training Loss: 0.6185775995254517, Validation Loss: 0.5739262104034424\n",
      "Epoch 8685/10000, Training Loss: 0.615077793598175, Validation Loss: 0.7807672023773193\n",
      "Epoch 8686/10000, Training Loss: 0.6484078168869019, Validation Loss: 0.5916320085525513\n",
      "Epoch 8687/10000, Training Loss: 0.6551489233970642, Validation Loss: 0.460064560174942\n",
      "Epoch 8688/10000, Training Loss: 0.6512150168418884, Validation Loss: 0.8247038722038269\n",
      "Epoch 8689/10000, Training Loss: 0.6765924096107483, Validation Loss: 0.6516044735908508\n",
      "Epoch 8690/10000, Training Loss: 0.6524105072021484, Validation Loss: 0.6721522212028503\n",
      "Epoch 8691/10000, Training Loss: 0.6635786890983582, Validation Loss: 0.8579115867614746\n",
      "Epoch 8692/10000, Training Loss: 0.6680765151977539, Validation Loss: 0.5626462697982788\n",
      "Epoch 8693/10000, Training Loss: 0.6190515160560608, Validation Loss: 0.595650315284729\n",
      "Epoch 8694/10000, Training Loss: 0.5893374085426331, Validation Loss: 0.6718614101409912\n",
      "Epoch 8695/10000, Training Loss: 0.6700258255004883, Validation Loss: 0.7382972836494446\n",
      "Epoch 8696/10000, Training Loss: 0.6455391645431519, Validation Loss: 0.7359239459037781\n",
      "Epoch 8697/10000, Training Loss: 0.6062383055686951, Validation Loss: 0.6139711737632751\n",
      "Epoch 8698/10000, Training Loss: 0.6508381962776184, Validation Loss: 0.6502966284751892\n",
      "Epoch 8699/10000, Training Loss: 0.64482581615448, Validation Loss: 0.5950360894203186\n",
      "Epoch 8700/10000, Training Loss: 0.6564257144927979, Validation Loss: 0.8725056648254395\n",
      "Epoch 8701/10000, Training Loss: 0.6504777669906616, Validation Loss: 0.7131140828132629\n",
      "Epoch 8702/10000, Training Loss: 0.6639060974121094, Validation Loss: 0.693012535572052\n",
      "Epoch 8703/10000, Training Loss: 0.617577314376831, Validation Loss: 0.5281916260719299\n",
      "Epoch 8704/10000, Training Loss: 0.6184558868408203, Validation Loss: 0.9518364071846008\n",
      "Epoch 8705/10000, Training Loss: 0.6314910054206848, Validation Loss: 0.8475909233093262\n",
      "Epoch 8706/10000, Training Loss: 0.6603263020515442, Validation Loss: 0.7411773800849915\n",
      "Epoch 8707/10000, Training Loss: 0.6639872789382935, Validation Loss: 0.7504052519798279\n",
      "Epoch 8708/10000, Training Loss: 0.6678669452667236, Validation Loss: 0.7772300839424133\n",
      "Epoch 8709/10000, Training Loss: 0.6376596093177795, Validation Loss: 0.8756468892097473\n",
      "Epoch 8710/10000, Training Loss: 0.650516152381897, Validation Loss: 0.8532508015632629\n",
      "Epoch 8711/10000, Training Loss: 0.6185808777809143, Validation Loss: 0.6470312476158142\n",
      "Epoch 8712/10000, Training Loss: 0.6144298315048218, Validation Loss: 0.5829974412918091\n",
      "Epoch 8713/10000, Training Loss: 0.6409703493118286, Validation Loss: 0.4903463125228882\n",
      "Epoch 8714/10000, Training Loss: 0.6563783884048462, Validation Loss: 0.7228362560272217\n",
      "Epoch 8715/10000, Training Loss: 0.6572710275650024, Validation Loss: 0.7052979469299316\n",
      "Epoch 8716/10000, Training Loss: 0.6153120398521423, Validation Loss: 0.9514971375465393\n",
      "Epoch 8717/10000, Training Loss: 0.6692046523094177, Validation Loss: 0.6045342683792114\n",
      "Epoch 8718/10000, Training Loss: 0.653329610824585, Validation Loss: 0.7041937708854675\n",
      "Epoch 8719/10000, Training Loss: 0.6663309335708618, Validation Loss: 0.6525275111198425\n",
      "Epoch 8720/10000, Training Loss: 0.6434328556060791, Validation Loss: 0.6782613396644592\n",
      "Epoch 8721/10000, Training Loss: 0.6084147691726685, Validation Loss: 0.552193820476532\n",
      "Epoch 8722/10000, Training Loss: 0.6637492775917053, Validation Loss: 0.7858898043632507\n",
      "Epoch 8723/10000, Training Loss: 0.6732016205787659, Validation Loss: 0.5344585180282593\n",
      "Epoch 8724/10000, Training Loss: 0.6628819704055786, Validation Loss: 0.9299837946891785\n",
      "Epoch 8725/10000, Training Loss: 0.6107234358787537, Validation Loss: 0.6507964134216309\n",
      "Epoch 8726/10000, Training Loss: 0.622123122215271, Validation Loss: 0.6560496091842651\n",
      "Epoch 8727/10000, Training Loss: 0.6321843862533569, Validation Loss: 0.5828948616981506\n",
      "Epoch 8728/10000, Training Loss: 0.6788367033004761, Validation Loss: 0.7792961597442627\n",
      "Epoch 8729/10000, Training Loss: 0.621256947517395, Validation Loss: 0.5800657272338867\n",
      "Epoch 8730/10000, Training Loss: 0.6098586320877075, Validation Loss: 0.6412623524665833\n",
      "Epoch 8731/10000, Training Loss: 0.6031354665756226, Validation Loss: 0.6228156685829163\n",
      "Epoch 8732/10000, Training Loss: 0.6017764806747437, Validation Loss: 0.721247136592865\n",
      "Epoch 8733/10000, Training Loss: 0.6413280963897705, Validation Loss: 0.7756910920143127\n",
      "Epoch 8734/10000, Training Loss: 0.6107897162437439, Validation Loss: 0.6516689658164978\n",
      "Epoch 8735/10000, Training Loss: 0.610554575920105, Validation Loss: 0.6368826031684875\n",
      "Epoch 8736/10000, Training Loss: 0.6468404531478882, Validation Loss: 0.7942516803741455\n",
      "Epoch 8737/10000, Training Loss: 0.6163256764411926, Validation Loss: 0.6218321919441223\n",
      "Epoch 8738/10000, Training Loss: 0.643003523349762, Validation Loss: 0.6210123300552368\n",
      "Epoch 8739/10000, Training Loss: 0.6115979552268982, Validation Loss: 0.7618715763092041\n",
      "Epoch 8740/10000, Training Loss: 0.6111739873886108, Validation Loss: 0.7188388705253601\n",
      "Epoch 8741/10000, Training Loss: 0.6357066631317139, Validation Loss: 0.6452457904815674\n",
      "Epoch 8742/10000, Training Loss: 0.6176204085350037, Validation Loss: 0.5885249972343445\n",
      "Epoch 8743/10000, Training Loss: 0.6387666463851929, Validation Loss: 0.6111136078834534\n",
      "Epoch 8744/10000, Training Loss: 0.6293747425079346, Validation Loss: 0.6233764290809631\n",
      "Epoch 8745/10000, Training Loss: 0.6212096810340881, Validation Loss: 0.7358667254447937\n",
      "Epoch 8746/10000, Training Loss: 0.6567797064781189, Validation Loss: 0.7311385273933411\n",
      "Epoch 8747/10000, Training Loss: 0.6191471815109253, Validation Loss: 0.7106919884681702\n",
      "Epoch 8748/10000, Training Loss: 0.6400029063224792, Validation Loss: 0.8429169058799744\n",
      "Epoch 8749/10000, Training Loss: 0.6256091594696045, Validation Loss: 0.9555750489234924\n",
      "Epoch 8750/10000, Training Loss: 0.6666567921638489, Validation Loss: 0.6341606974601746\n",
      "Epoch 8751/10000, Training Loss: 0.6016875505447388, Validation Loss: 0.5475988388061523\n",
      "Epoch 8752/10000, Training Loss: 0.6680752635002136, Validation Loss: 0.9359946250915527\n",
      "Epoch 8753/10000, Training Loss: 0.6724632978439331, Validation Loss: 0.7501196265220642\n",
      "Epoch 8754/10000, Training Loss: 0.6446983814239502, Validation Loss: 0.6478672027587891\n",
      "Epoch 8755/10000, Training Loss: 0.6194241642951965, Validation Loss: 0.49476373195648193\n",
      "Epoch 8756/10000, Training Loss: 0.6401095986366272, Validation Loss: 0.7082538604736328\n",
      "Epoch 8757/10000, Training Loss: 0.6491981148719788, Validation Loss: 0.6474281549453735\n",
      "Epoch 8758/10000, Training Loss: 0.614149808883667, Validation Loss: 0.5806789994239807\n",
      "Epoch 8759/10000, Training Loss: 0.6324307918548584, Validation Loss: 0.6735745072364807\n",
      "Epoch 8760/10000, Training Loss: 0.6914975643157959, Validation Loss: 0.5446777939796448\n",
      "Epoch 8761/10000, Training Loss: 0.6155813932418823, Validation Loss: 0.62498939037323\n",
      "Epoch 8762/10000, Training Loss: 0.6313667893409729, Validation Loss: 0.5934672951698303\n",
      "Epoch 8763/10000, Training Loss: 0.6361472606658936, Validation Loss: 0.9799385666847229\n",
      "Epoch 8764/10000, Training Loss: 0.6518858671188354, Validation Loss: 0.6551636457443237\n",
      "Epoch 8765/10000, Training Loss: 0.566680371761322, Validation Loss: 0.6314556002616882\n",
      "Epoch 8766/10000, Training Loss: 0.6500453948974609, Validation Loss: 0.5449236631393433\n",
      "Epoch 8767/10000, Training Loss: 0.6575992107391357, Validation Loss: 0.7740383148193359\n",
      "Epoch 8768/10000, Training Loss: 0.61527019739151, Validation Loss: 0.945264995098114\n",
      "Epoch 8769/10000, Training Loss: 0.6840162873268127, Validation Loss: 0.6760361194610596\n",
      "Epoch 8770/10000, Training Loss: 0.6347389817237854, Validation Loss: 0.8460662961006165\n",
      "Epoch 8771/10000, Training Loss: 0.6120498776435852, Validation Loss: 0.7347100377082825\n",
      "Epoch 8772/10000, Training Loss: 0.6881398558616638, Validation Loss: 0.7296572327613831\n",
      "Epoch 8773/10000, Training Loss: 0.6602075099945068, Validation Loss: 0.6761038303375244\n",
      "Epoch 8774/10000, Training Loss: 0.655155599117279, Validation Loss: 0.7419736981391907\n",
      "Epoch 8775/10000, Training Loss: 0.6175038814544678, Validation Loss: 0.6437687277793884\n",
      "Epoch 8776/10000, Training Loss: 0.6275240182876587, Validation Loss: 0.8915215134620667\n",
      "Epoch 8777/10000, Training Loss: 0.6065202951431274, Validation Loss: 0.7311069965362549\n",
      "Epoch 8778/10000, Training Loss: 0.5943590402603149, Validation Loss: 0.8325229287147522\n",
      "Epoch 8779/10000, Training Loss: 0.6127485632896423, Validation Loss: 0.9524479508399963\n",
      "Epoch 8780/10000, Training Loss: 0.6520945429801941, Validation Loss: 0.5283797979354858\n",
      "Epoch 8781/10000, Training Loss: 0.6345364451408386, Validation Loss: 0.7218690514564514\n",
      "Epoch 8782/10000, Training Loss: 0.674416184425354, Validation Loss: 0.7035709023475647\n",
      "Epoch 8783/10000, Training Loss: 0.668121874332428, Validation Loss: 0.7371805310249329\n",
      "Epoch 8784/10000, Training Loss: 0.6937618851661682, Validation Loss: 0.5967646241188049\n",
      "Epoch 8785/10000, Training Loss: 0.6589351296424866, Validation Loss: 0.6973832249641418\n",
      "Epoch 8786/10000, Training Loss: 0.6764539480209351, Validation Loss: 0.7112358212471008\n",
      "Epoch 8787/10000, Training Loss: 0.5860185623168945, Validation Loss: 0.6863808035850525\n",
      "Epoch 8788/10000, Training Loss: 0.6190780401229858, Validation Loss: 0.6769793629646301\n",
      "Epoch 8789/10000, Training Loss: 0.6113128662109375, Validation Loss: 0.6838298439979553\n",
      "Epoch 8790/10000, Training Loss: 0.6183649897575378, Validation Loss: 0.5945372581481934\n",
      "Epoch 8791/10000, Training Loss: 0.6404955983161926, Validation Loss: 0.628692090511322\n",
      "Epoch 8792/10000, Training Loss: 0.6323760151863098, Validation Loss: 0.6216046214103699\n",
      "Epoch 8793/10000, Training Loss: 0.6442989706993103, Validation Loss: 0.5925731062889099\n",
      "Epoch 8794/10000, Training Loss: 0.626872718334198, Validation Loss: 0.6300842761993408\n",
      "Epoch 8795/10000, Training Loss: 0.674475371837616, Validation Loss: 0.7304784655570984\n",
      "Epoch 8796/10000, Training Loss: 0.6329548954963684, Validation Loss: 0.8173879981040955\n",
      "Epoch 8797/10000, Training Loss: 0.6249768733978271, Validation Loss: 0.7510806918144226\n",
      "Epoch 8798/10000, Training Loss: 0.6803073883056641, Validation Loss: 0.8373498320579529\n",
      "Epoch 8799/10000, Training Loss: 0.6596717834472656, Validation Loss: 0.6102682948112488\n",
      "Epoch 8800/10000, Training Loss: 0.6474984884262085, Validation Loss: 0.7681156992912292\n",
      "Epoch 8801/10000, Training Loss: 0.6425610184669495, Validation Loss: 0.7959676384925842\n",
      "Epoch 8802/10000, Training Loss: 0.641383707523346, Validation Loss: 0.6632447838783264\n",
      "Epoch 8803/10000, Training Loss: 0.6015486121177673, Validation Loss: 0.5761198401451111\n",
      "Epoch 8804/10000, Training Loss: 0.6639727354049683, Validation Loss: 0.652866780757904\n",
      "Epoch 8805/10000, Training Loss: 0.620635449886322, Validation Loss: 0.640222430229187\n",
      "Epoch 8806/10000, Training Loss: 0.625090479850769, Validation Loss: 0.638705313205719\n",
      "Epoch 8807/10000, Training Loss: 0.6524425745010376, Validation Loss: 0.7253623604774475\n",
      "Epoch 8808/10000, Training Loss: 0.6415665149688721, Validation Loss: 0.5847696661949158\n",
      "Epoch 8809/10000, Training Loss: 0.6083068251609802, Validation Loss: 0.4700964391231537\n",
      "Epoch 8810/10000, Training Loss: 0.6427271962165833, Validation Loss: 0.8388491272926331\n",
      "Epoch 8811/10000, Training Loss: 0.6114088296890259, Validation Loss: 0.5546193718910217\n",
      "Epoch 8812/10000, Training Loss: 0.6071149110794067, Validation Loss: 0.5155468583106995\n",
      "Epoch 8813/10000, Training Loss: 0.6276522278785706, Validation Loss: 0.8990289568901062\n",
      "Epoch 8814/10000, Training Loss: 0.6364877223968506, Validation Loss: 0.6419317126274109\n",
      "Epoch 8815/10000, Training Loss: 0.6066111326217651, Validation Loss: 0.6961567401885986\n",
      "Epoch 8816/10000, Training Loss: 0.6346115469932556, Validation Loss: 0.5638737678527832\n",
      "Epoch 8817/10000, Training Loss: 0.6430809497833252, Validation Loss: 0.6817881464958191\n",
      "Epoch 8818/10000, Training Loss: 0.6219069957733154, Validation Loss: 0.6932856440544128\n",
      "Epoch 8819/10000, Training Loss: 0.6406431794166565, Validation Loss: 0.5675583481788635\n",
      "Epoch 8820/10000, Training Loss: 0.6977971196174622, Validation Loss: 1.0512627363204956\n",
      "Epoch 8821/10000, Training Loss: 0.6381822824478149, Validation Loss: 0.6021708846092224\n",
      "Epoch 8822/10000, Training Loss: 0.6911574006080627, Validation Loss: 0.7006141543388367\n",
      "Epoch 8823/10000, Training Loss: 0.6499446034431458, Validation Loss: 0.7896638512611389\n",
      "Epoch 8824/10000, Training Loss: 0.6572794318199158, Validation Loss: 0.5850508809089661\n",
      "Epoch 8825/10000, Training Loss: 0.6205148696899414, Validation Loss: 0.6767752170562744\n",
      "Epoch 8826/10000, Training Loss: 0.6372982263565063, Validation Loss: 0.6730074882507324\n",
      "Epoch 8827/10000, Training Loss: 0.6606913805007935, Validation Loss: 0.7937970757484436\n",
      "Epoch 8828/10000, Training Loss: 0.6580036878585815, Validation Loss: 0.6293366551399231\n",
      "Epoch 8829/10000, Training Loss: 0.6234747767448425, Validation Loss: 0.6837547421455383\n",
      "Epoch 8830/10000, Training Loss: 0.6173566579818726, Validation Loss: 0.7234193682670593\n",
      "Epoch 8831/10000, Training Loss: 0.649835467338562, Validation Loss: 0.8228957653045654\n",
      "Epoch 8832/10000, Training Loss: 0.6346683502197266, Validation Loss: 0.6794630885124207\n",
      "Epoch 8833/10000, Training Loss: 0.6358984708786011, Validation Loss: 0.7519180178642273\n",
      "Epoch 8834/10000, Training Loss: 0.6347348093986511, Validation Loss: 0.6912403106689453\n",
      "Epoch 8835/10000, Training Loss: 0.6234965920448303, Validation Loss: 0.738001823425293\n",
      "Epoch 8836/10000, Training Loss: 0.6595834493637085, Validation Loss: 0.6981760859489441\n",
      "Epoch 8837/10000, Training Loss: 0.6360032558441162, Validation Loss: 0.7081120014190674\n",
      "Epoch 8838/10000, Training Loss: 0.6453577280044556, Validation Loss: 0.7017969489097595\n",
      "Epoch 8839/10000, Training Loss: 0.65852290391922, Validation Loss: 0.4901149272918701\n",
      "Epoch 8840/10000, Training Loss: 0.6721166968345642, Validation Loss: 0.6244222521781921\n",
      "Epoch 8841/10000, Training Loss: 0.6421589255332947, Validation Loss: 0.7418134212493896\n",
      "Epoch 8842/10000, Training Loss: 0.6262229084968567, Validation Loss: 0.5848811268806458\n",
      "Epoch 8843/10000, Training Loss: 0.6838399171829224, Validation Loss: 0.7163756489753723\n",
      "Epoch 8844/10000, Training Loss: 0.6295033097267151, Validation Loss: 0.7636826634407043\n",
      "Epoch 8845/10000, Training Loss: 0.639098584651947, Validation Loss: 0.7306123375892639\n",
      "Epoch 8846/10000, Training Loss: 0.6456813812255859, Validation Loss: 0.5571114420890808\n",
      "Epoch 8847/10000, Training Loss: 0.6532027721405029, Validation Loss: 0.7541906237602234\n",
      "Epoch 8848/10000, Training Loss: 0.6507217884063721, Validation Loss: 0.7697529792785645\n",
      "Epoch 8849/10000, Training Loss: 0.6745465993881226, Validation Loss: 0.6519232392311096\n",
      "Epoch 8850/10000, Training Loss: 0.6521285176277161, Validation Loss: 0.8595818877220154\n",
      "Epoch 8851/10000, Training Loss: 0.6542491912841797, Validation Loss: 0.7420404553413391\n",
      "Epoch 8852/10000, Training Loss: 0.6676703691482544, Validation Loss: 0.5920844674110413\n",
      "Epoch 8853/10000, Training Loss: 0.6214650869369507, Validation Loss: 0.7577075958251953\n",
      "Epoch 8854/10000, Training Loss: 0.6350783109664917, Validation Loss: 0.7512819766998291\n",
      "Epoch 8855/10000, Training Loss: 0.6336522698402405, Validation Loss: 0.6016361117362976\n",
      "Epoch 8856/10000, Training Loss: 0.6586981415748596, Validation Loss: 0.6225175857543945\n",
      "Epoch 8857/10000, Training Loss: 0.6489296555519104, Validation Loss: 0.6613809466362\n",
      "Epoch 8858/10000, Training Loss: 0.689469039440155, Validation Loss: 0.6166989207267761\n",
      "Epoch 8859/10000, Training Loss: 0.6409844756126404, Validation Loss: 0.6197575926780701\n",
      "Epoch 8860/10000, Training Loss: 0.6310614347457886, Validation Loss: 0.5845092535018921\n",
      "Epoch 8861/10000, Training Loss: 0.6722135543823242, Validation Loss: 0.6803498268127441\n",
      "Epoch 8862/10000, Training Loss: 0.6621159315109253, Validation Loss: 0.6443011164665222\n",
      "Epoch 8863/10000, Training Loss: 0.6706510186195374, Validation Loss: 0.7419073581695557\n",
      "Epoch 8864/10000, Training Loss: 0.6487348079681396, Validation Loss: 0.6477468609809875\n",
      "Epoch 8865/10000, Training Loss: 0.6585276126861572, Validation Loss: 0.6351975202560425\n",
      "Epoch 8866/10000, Training Loss: 0.6239993572235107, Validation Loss: 0.6506104469299316\n",
      "Epoch 8867/10000, Training Loss: 0.6721379160881042, Validation Loss: 0.5942174792289734\n",
      "Epoch 8868/10000, Training Loss: 0.6558921337127686, Validation Loss: 0.6876502633094788\n",
      "Epoch 8869/10000, Training Loss: 0.6517268419265747, Validation Loss: 0.722166121006012\n",
      "Epoch 8870/10000, Training Loss: 0.6257401704788208, Validation Loss: 0.7586462497711182\n",
      "Epoch 8871/10000, Training Loss: 0.6745210289955139, Validation Loss: 0.6134747266769409\n",
      "Epoch 8872/10000, Training Loss: 0.6547054648399353, Validation Loss: 0.65097975730896\n",
      "Epoch 8873/10000, Training Loss: 0.5971270203590393, Validation Loss: 0.6481468081474304\n",
      "Epoch 8874/10000, Training Loss: 0.6521686315536499, Validation Loss: 0.6794893741607666\n",
      "Epoch 8875/10000, Training Loss: 0.6705974340438843, Validation Loss: 0.8194203972816467\n",
      "Epoch 8876/10000, Training Loss: 0.6545432209968567, Validation Loss: 0.7678387761116028\n",
      "Epoch 8877/10000, Training Loss: 0.6343870162963867, Validation Loss: 0.515440821647644\n",
      "Epoch 8878/10000, Training Loss: 0.6386957764625549, Validation Loss: 0.7218806147575378\n",
      "Epoch 8879/10000, Training Loss: 0.6222586035728455, Validation Loss: 0.8883592486381531\n",
      "Epoch 8880/10000, Training Loss: 0.6233128309249878, Validation Loss: 0.7273411750793457\n",
      "Epoch 8881/10000, Training Loss: 0.6656017899513245, Validation Loss: 1.203450322151184\n",
      "Epoch 8882/10000, Training Loss: 0.6808837652206421, Validation Loss: 0.6497600078582764\n",
      "Epoch 8883/10000, Training Loss: 0.6308786869049072, Validation Loss: 0.6464371681213379\n",
      "Epoch 8884/10000, Training Loss: 0.6273394227027893, Validation Loss: 1.0636180639266968\n",
      "Epoch 8885/10000, Training Loss: 0.6507176756858826, Validation Loss: 0.9637842774391174\n",
      "Epoch 8886/10000, Training Loss: 0.6482831239700317, Validation Loss: 0.5942886471748352\n",
      "Epoch 8887/10000, Training Loss: 0.6578152775764465, Validation Loss: 0.788357675075531\n",
      "Epoch 8888/10000, Training Loss: 0.6383097171783447, Validation Loss: 0.6992611289024353\n",
      "Epoch 8889/10000, Training Loss: 0.6552219390869141, Validation Loss: 0.6422519087791443\n",
      "Epoch 8890/10000, Training Loss: 0.6486988663673401, Validation Loss: 0.5963741540908813\n",
      "Epoch 8891/10000, Training Loss: 0.6456377506256104, Validation Loss: 0.7211935520172119\n",
      "Epoch 8892/10000, Training Loss: 0.6082192063331604, Validation Loss: 0.8243134617805481\n",
      "Epoch 8893/10000, Training Loss: 0.6466959714889526, Validation Loss: 0.6366081833839417\n",
      "Epoch 8894/10000, Training Loss: 0.6396305561065674, Validation Loss: 0.7535392642021179\n",
      "Epoch 8895/10000, Training Loss: 0.6341237425804138, Validation Loss: 0.7088562846183777\n",
      "Epoch 8896/10000, Training Loss: 0.628015398979187, Validation Loss: 0.962299108505249\n",
      "Epoch 8897/10000, Training Loss: 0.6026819944381714, Validation Loss: 0.5526553988456726\n",
      "Epoch 8898/10000, Training Loss: 0.6309925317764282, Validation Loss: 0.5607224106788635\n",
      "Epoch 8899/10000, Training Loss: 0.6369199752807617, Validation Loss: 0.8334957957267761\n",
      "Epoch 8900/10000, Training Loss: 0.6255404949188232, Validation Loss: 0.7592566609382629\n",
      "Epoch 8901/10000, Training Loss: 0.6283503770828247, Validation Loss: 0.5823290944099426\n",
      "Epoch 8902/10000, Training Loss: 0.640699028968811, Validation Loss: 0.6264329552650452\n",
      "Epoch 8903/10000, Training Loss: 0.6365360617637634, Validation Loss: 0.6451367735862732\n",
      "Epoch 8904/10000, Training Loss: 0.6152920126914978, Validation Loss: 0.5286820530891418\n",
      "Epoch 8905/10000, Training Loss: 0.6322780251502991, Validation Loss: 0.6909433007240295\n",
      "Epoch 8906/10000, Training Loss: 0.6334421634674072, Validation Loss: 0.9854035973548889\n",
      "Epoch 8907/10000, Training Loss: 0.5867189168930054, Validation Loss: 0.5326150059700012\n",
      "Epoch 8908/10000, Training Loss: 0.6306542754173279, Validation Loss: 0.6309554576873779\n",
      "Epoch 8909/10000, Training Loss: 0.6103114485740662, Validation Loss: 0.815028727054596\n",
      "Epoch 8910/10000, Training Loss: 0.6392372846603394, Validation Loss: 0.7903170585632324\n",
      "Epoch 8911/10000, Training Loss: 0.6266819834709167, Validation Loss: 0.7304162979125977\n",
      "Epoch 8912/10000, Training Loss: 0.6151743531227112, Validation Loss: 0.863399088382721\n",
      "Epoch 8913/10000, Training Loss: 0.6485806703567505, Validation Loss: 0.7914294600486755\n",
      "Epoch 8914/10000, Training Loss: 0.7195978760719299, Validation Loss: 0.8820967674255371\n",
      "Epoch 8915/10000, Training Loss: 0.6147730946540833, Validation Loss: 0.7082173824310303\n",
      "Epoch 8916/10000, Training Loss: 0.6133837699890137, Validation Loss: 0.6071876883506775\n",
      "Epoch 8917/10000, Training Loss: 0.6177918910980225, Validation Loss: 1.0793367624282837\n",
      "Epoch 8918/10000, Training Loss: 0.6653175354003906, Validation Loss: 0.7413315176963806\n",
      "Epoch 8919/10000, Training Loss: 0.6606325507164001, Validation Loss: 0.9239092469215393\n",
      "Epoch 8920/10000, Training Loss: 0.6864379048347473, Validation Loss: 0.970574676990509\n",
      "Epoch 8921/10000, Training Loss: 0.65164715051651, Validation Loss: 0.8427493572235107\n",
      "Epoch 8922/10000, Training Loss: 0.6732720136642456, Validation Loss: 0.47964489459991455\n",
      "Epoch 8923/10000, Training Loss: 0.6018754243850708, Validation Loss: 0.6114685535430908\n",
      "Epoch 8924/10000, Training Loss: 0.6212854385375977, Validation Loss: 0.6923372745513916\n",
      "Epoch 8925/10000, Training Loss: 0.6391367316246033, Validation Loss: 0.8349289894104004\n",
      "Epoch 8926/10000, Training Loss: 0.6505337357521057, Validation Loss: 0.772728681564331\n",
      "Epoch 8927/10000, Training Loss: 0.6139764785766602, Validation Loss: 0.6400113105773926\n",
      "Epoch 8928/10000, Training Loss: 0.6673722863197327, Validation Loss: 0.6592057347297668\n",
      "Epoch 8929/10000, Training Loss: 0.6245489716529846, Validation Loss: 0.5936502814292908\n",
      "Epoch 8930/10000, Training Loss: 0.647144615650177, Validation Loss: 0.651155412197113\n",
      "Epoch 8931/10000, Training Loss: 0.6632078886032104, Validation Loss: 0.7220349311828613\n",
      "Epoch 8932/10000, Training Loss: 0.6363645792007446, Validation Loss: 0.6724510192871094\n",
      "Epoch 8933/10000, Training Loss: 0.6726101636886597, Validation Loss: 0.6264647841453552\n",
      "Epoch 8934/10000, Training Loss: 0.6598559617996216, Validation Loss: 0.8040303587913513\n",
      "Epoch 8935/10000, Training Loss: 0.6073840856552124, Validation Loss: 0.5446954369544983\n",
      "Epoch 8936/10000, Training Loss: 0.6252723932266235, Validation Loss: 0.48467954993247986\n",
      "Epoch 8937/10000, Training Loss: 0.6215143799781799, Validation Loss: 0.7318789958953857\n",
      "Epoch 8938/10000, Training Loss: 0.6294957399368286, Validation Loss: 0.6362660527229309\n",
      "Epoch 8939/10000, Training Loss: 0.6317091584205627, Validation Loss: 0.6280909180641174\n",
      "Epoch 8940/10000, Training Loss: 0.6203381419181824, Validation Loss: 0.7305945754051208\n",
      "Epoch 8941/10000, Training Loss: 0.6336403489112854, Validation Loss: 0.6152796745300293\n",
      "Epoch 8942/10000, Training Loss: 0.6319758296012878, Validation Loss: 0.6069101691246033\n",
      "Epoch 8943/10000, Training Loss: 0.6304821372032166, Validation Loss: 0.672385036945343\n",
      "Epoch 8944/10000, Training Loss: 0.6515607237815857, Validation Loss: 0.8118330836296082\n",
      "Epoch 8945/10000, Training Loss: 0.6495954990386963, Validation Loss: 0.7163805961608887\n",
      "Epoch 8946/10000, Training Loss: 0.6300821304321289, Validation Loss: 0.7431715130805969\n",
      "Epoch 8947/10000, Training Loss: 0.6553623676300049, Validation Loss: 1.0133715867996216\n",
      "Epoch 8948/10000, Training Loss: 0.623097836971283, Validation Loss: 0.5876408815383911\n",
      "Epoch 8949/10000, Training Loss: 0.6660267114639282, Validation Loss: 0.5890262126922607\n",
      "Epoch 8950/10000, Training Loss: 0.6429287195205688, Validation Loss: 0.7769100069999695\n",
      "Epoch 8951/10000, Training Loss: 0.6328522562980652, Validation Loss: 0.7590644359588623\n",
      "Epoch 8952/10000, Training Loss: 0.5911124348640442, Validation Loss: 0.7717282772064209\n",
      "Epoch 8953/10000, Training Loss: 0.6676139235496521, Validation Loss: 0.6969214081764221\n",
      "Epoch 8954/10000, Training Loss: 0.6335287690162659, Validation Loss: 0.6324918270111084\n",
      "Epoch 8955/10000, Training Loss: 0.6251792311668396, Validation Loss: 0.8918591141700745\n",
      "Epoch 8956/10000, Training Loss: 0.6617522835731506, Validation Loss: 0.8819935321807861\n",
      "Epoch 8957/10000, Training Loss: 0.6217358112335205, Validation Loss: 0.5842177271842957\n",
      "Epoch 8958/10000, Training Loss: 0.6534252762794495, Validation Loss: 0.6950747966766357\n",
      "Epoch 8959/10000, Training Loss: 0.6372074484825134, Validation Loss: 0.9234285354614258\n",
      "Epoch 8960/10000, Training Loss: 0.6561869382858276, Validation Loss: 0.6305281519889832\n",
      "Epoch 8961/10000, Training Loss: 0.6136799454689026, Validation Loss: 0.5097123980522156\n",
      "Epoch 8962/10000, Training Loss: 0.6229889392852783, Validation Loss: 0.48848044872283936\n",
      "Epoch 8963/10000, Training Loss: 0.6296581029891968, Validation Loss: 0.781460702419281\n",
      "Epoch 8964/10000, Training Loss: 0.6691719889640808, Validation Loss: 0.5641579627990723\n",
      "Epoch 8965/10000, Training Loss: 0.6257814168930054, Validation Loss: 0.6852472424507141\n",
      "Epoch 8966/10000, Training Loss: 0.6639995574951172, Validation Loss: 0.8196955323219299\n",
      "Epoch 8967/10000, Training Loss: 0.6181463003158569, Validation Loss: 1.008135437965393\n",
      "Epoch 8968/10000, Training Loss: 0.6551761627197266, Validation Loss: 0.754906177520752\n",
      "Epoch 8969/10000, Training Loss: 0.6801574230194092, Validation Loss: 0.6579882502555847\n",
      "Epoch 8970/10000, Training Loss: 0.6381973028182983, Validation Loss: 0.9529678225517273\n",
      "Epoch 8971/10000, Training Loss: 0.6090397238731384, Validation Loss: 0.8567127585411072\n",
      "Epoch 8972/10000, Training Loss: 0.665027379989624, Validation Loss: 0.6122234463691711\n",
      "Epoch 8973/10000, Training Loss: 0.6865957975387573, Validation Loss: 0.5936116576194763\n",
      "Epoch 8974/10000, Training Loss: 0.6543356776237488, Validation Loss: 0.6513331532478333\n",
      "Epoch 8975/10000, Training Loss: 0.6497828960418701, Validation Loss: 0.8385303616523743\n",
      "Epoch 8976/10000, Training Loss: 0.642221987247467, Validation Loss: 0.6961533427238464\n",
      "Epoch 8977/10000, Training Loss: 0.7098305821418762, Validation Loss: 0.7930260300636292\n",
      "Epoch 8978/10000, Training Loss: 0.6799681782722473, Validation Loss: 0.7977342009544373\n",
      "Epoch 8979/10000, Training Loss: 0.6166402101516724, Validation Loss: 0.7261276841163635\n",
      "Epoch 8980/10000, Training Loss: 0.6402164101600647, Validation Loss: 0.6883280873298645\n",
      "Epoch 8981/10000, Training Loss: 0.6691996455192566, Validation Loss: 0.636231005191803\n",
      "Epoch 8982/10000, Training Loss: 0.6193278431892395, Validation Loss: 0.6989486813545227\n",
      "Epoch 8983/10000, Training Loss: 0.6491652727127075, Validation Loss: 0.576304018497467\n",
      "Epoch 8984/10000, Training Loss: 0.647593080997467, Validation Loss: 0.7390125393867493\n",
      "Epoch 8985/10000, Training Loss: 0.6349904537200928, Validation Loss: 0.9786229133605957\n",
      "Epoch 8986/10000, Training Loss: 0.6649499535560608, Validation Loss: 0.7796711325645447\n",
      "Epoch 8987/10000, Training Loss: 0.6863101720809937, Validation Loss: 0.7483925819396973\n",
      "Epoch 8988/10000, Training Loss: 0.6266041994094849, Validation Loss: 0.6055569052696228\n",
      "Epoch 8989/10000, Training Loss: 0.6723088026046753, Validation Loss: 0.6897348761558533\n",
      "Epoch 8990/10000, Training Loss: 0.6376033425331116, Validation Loss: 0.8370761275291443\n",
      "Epoch 8991/10000, Training Loss: 0.6324142813682556, Validation Loss: 0.6356614828109741\n",
      "Epoch 8992/10000, Training Loss: 0.6437505483627319, Validation Loss: 0.7025566697120667\n",
      "Epoch 8993/10000, Training Loss: 0.6417361497879028, Validation Loss: 0.9946168065071106\n",
      "Epoch 8994/10000, Training Loss: 0.6406142711639404, Validation Loss: 0.611158013343811\n",
      "Epoch 8995/10000, Training Loss: 0.6322576999664307, Validation Loss: 0.6216549277305603\n",
      "Epoch 8996/10000, Training Loss: 0.8085473775863647, Validation Loss: 0.7095540165901184\n",
      "Epoch 8997/10000, Training Loss: 0.6286982297897339, Validation Loss: 0.6443966031074524\n",
      "Epoch 8998/10000, Training Loss: 0.688267171382904, Validation Loss: 0.646363377571106\n",
      "Epoch 8999/10000, Training Loss: 0.6228724718093872, Validation Loss: 0.6621549129486084\n",
      "Epoch 9000/10000, Training Loss: 0.6116533875465393, Validation Loss: 0.70669025182724\n",
      "Epoch 9001/10000, Training Loss: 0.6173689961433411, Validation Loss: 0.7137734889984131\n",
      "Epoch 9002/10000, Training Loss: 0.6530002355575562, Validation Loss: 0.583829402923584\n",
      "Epoch 9003/10000, Training Loss: 0.638472318649292, Validation Loss: 0.7057353854179382\n",
      "Epoch 9004/10000, Training Loss: 0.6437506079673767, Validation Loss: 0.7416897416114807\n",
      "Epoch 9005/10000, Training Loss: 0.6246267557144165, Validation Loss: 0.6258094906806946\n",
      "Epoch 9006/10000, Training Loss: 0.6414718627929688, Validation Loss: 0.5808047652244568\n",
      "Epoch 9007/10000, Training Loss: 0.6559157967567444, Validation Loss: 0.5137738585472107\n",
      "Epoch 9008/10000, Training Loss: 0.6207443475723267, Validation Loss: 0.7630329728126526\n",
      "Epoch 9009/10000, Training Loss: 0.6483120322227478, Validation Loss: 0.8436477780342102\n",
      "Epoch 9010/10000, Training Loss: 0.6729586720466614, Validation Loss: 0.7497861385345459\n",
      "Epoch 9011/10000, Training Loss: 0.6482058763504028, Validation Loss: 0.7034806609153748\n",
      "Epoch 9012/10000, Training Loss: 0.6629973649978638, Validation Loss: 0.5871486067771912\n",
      "Epoch 9013/10000, Training Loss: 0.6871176958084106, Validation Loss: 0.7016861438751221\n",
      "Epoch 9014/10000, Training Loss: 0.6305729746818542, Validation Loss: 1.057313323020935\n",
      "Epoch 9015/10000, Training Loss: 0.6427931189537048, Validation Loss: 0.8709762692451477\n",
      "Epoch 9016/10000, Training Loss: 0.5964006781578064, Validation Loss: 0.6854324340820312\n",
      "Epoch 9017/10000, Training Loss: 0.6463042497634888, Validation Loss: 0.7589234709739685\n",
      "Epoch 9018/10000, Training Loss: 0.5889288187026978, Validation Loss: 0.5291209816932678\n",
      "Epoch 9019/10000, Training Loss: 0.5785059332847595, Validation Loss: 0.7975361943244934\n",
      "Epoch 9020/10000, Training Loss: 0.632334291934967, Validation Loss: 0.6827208399772644\n",
      "Epoch 9021/10000, Training Loss: 0.6495246291160583, Validation Loss: 0.6746432781219482\n",
      "Epoch 9022/10000, Training Loss: 0.6027320027351379, Validation Loss: 0.6357597708702087\n",
      "Epoch 9023/10000, Training Loss: 0.6372615098953247, Validation Loss: 0.6665794253349304\n",
      "Epoch 9024/10000, Training Loss: 0.6588438749313354, Validation Loss: 0.6653872728347778\n",
      "Epoch 9025/10000, Training Loss: 0.6205670833587646, Validation Loss: 0.5494489073753357\n",
      "Epoch 9026/10000, Training Loss: 0.6115244626998901, Validation Loss: 0.6104691624641418\n",
      "Epoch 9027/10000, Training Loss: 0.6410808563232422, Validation Loss: 0.4593713581562042\n",
      "Epoch 9028/10000, Training Loss: 0.6674718260765076, Validation Loss: 0.6135200262069702\n",
      "Epoch 9029/10000, Training Loss: 0.6705116629600525, Validation Loss: 0.6484811902046204\n",
      "Epoch 9030/10000, Training Loss: 0.6203160881996155, Validation Loss: 0.7099976539611816\n",
      "Epoch 9031/10000, Training Loss: 0.6485096216201782, Validation Loss: 0.7692436575889587\n",
      "Epoch 9032/10000, Training Loss: 0.6645026803016663, Validation Loss: 0.5399331450462341\n",
      "Epoch 9033/10000, Training Loss: 0.6235209107398987, Validation Loss: 0.7149975299835205\n",
      "Epoch 9034/10000, Training Loss: 0.6349087953567505, Validation Loss: 0.6100073456764221\n",
      "Epoch 9035/10000, Training Loss: 0.665501594543457, Validation Loss: 0.5097309947013855\n",
      "Epoch 9036/10000, Training Loss: 0.6291876435279846, Validation Loss: 0.6378123164176941\n",
      "Epoch 9037/10000, Training Loss: 0.6476898789405823, Validation Loss: 0.6576903462409973\n",
      "Epoch 9038/10000, Training Loss: 0.6582323312759399, Validation Loss: 0.8982861638069153\n",
      "Epoch 9039/10000, Training Loss: 0.7059016823768616, Validation Loss: 0.781886637210846\n",
      "Epoch 9040/10000, Training Loss: 0.6139291524887085, Validation Loss: 0.6789371371269226\n",
      "Epoch 9041/10000, Training Loss: 0.6762812733650208, Validation Loss: 0.6974816918373108\n",
      "Epoch 9042/10000, Training Loss: 0.6526692509651184, Validation Loss: 0.6763036251068115\n",
      "Epoch 9043/10000, Training Loss: 0.5767151117324829, Validation Loss: 0.6940904259681702\n",
      "Epoch 9044/10000, Training Loss: 0.6490399241447449, Validation Loss: 0.7168892025947571\n",
      "Epoch 9045/10000, Training Loss: 0.6692162156105042, Validation Loss: 0.6579553484916687\n",
      "Epoch 9046/10000, Training Loss: 0.6347073912620544, Validation Loss: 0.724575936794281\n",
      "Epoch 9047/10000, Training Loss: 0.6047096252441406, Validation Loss: 0.6528624296188354\n",
      "Epoch 9048/10000, Training Loss: 0.6516058444976807, Validation Loss: 0.675409734249115\n",
      "Epoch 9049/10000, Training Loss: 0.6388808488845825, Validation Loss: 0.5917035937309265\n",
      "Epoch 9050/10000, Training Loss: 0.6021256446838379, Validation Loss: 0.5543636679649353\n",
      "Epoch 9051/10000, Training Loss: 0.6273533701896667, Validation Loss: 0.7083973288536072\n",
      "Epoch 9052/10000, Training Loss: 0.6192902326583862, Validation Loss: 0.6184726357460022\n",
      "Epoch 9053/10000, Training Loss: 0.6054747104644775, Validation Loss: 0.6160430312156677\n",
      "Epoch 9054/10000, Training Loss: 0.6787862181663513, Validation Loss: 1.1143983602523804\n",
      "Epoch 9055/10000, Training Loss: 0.684526801109314, Validation Loss: 0.46183010935783386\n",
      "Epoch 9056/10000, Training Loss: 0.6389057636260986, Validation Loss: 0.7909362316131592\n",
      "Epoch 9057/10000, Training Loss: 0.710765540599823, Validation Loss: 0.8517945408821106\n",
      "Epoch 9058/10000, Training Loss: 0.6386042833328247, Validation Loss: 0.7972236275672913\n",
      "Epoch 9059/10000, Training Loss: 0.6108623147010803, Validation Loss: 0.7599214911460876\n",
      "Epoch 9060/10000, Training Loss: 0.6121174097061157, Validation Loss: 0.6779411435127258\n",
      "Epoch 9061/10000, Training Loss: 0.6392519474029541, Validation Loss: 0.7458669543266296\n",
      "Epoch 9062/10000, Training Loss: 0.6214096546173096, Validation Loss: 0.5955759882926941\n",
      "Epoch 9063/10000, Training Loss: 0.6465703248977661, Validation Loss: 0.7192387580871582\n",
      "Epoch 9064/10000, Training Loss: 0.6376955509185791, Validation Loss: 0.8254157900810242\n",
      "Epoch 9065/10000, Training Loss: 0.6784804463386536, Validation Loss: 0.5738487839698792\n",
      "Epoch 9066/10000, Training Loss: 0.6247259378433228, Validation Loss: 0.6759131550788879\n",
      "Epoch 9067/10000, Training Loss: 0.6191233396530151, Validation Loss: 0.5945547223091125\n",
      "Epoch 9068/10000, Training Loss: 0.6096433401107788, Validation Loss: 0.5008381605148315\n",
      "Epoch 9069/10000, Training Loss: 0.6881276369094849, Validation Loss: 0.8041872382164001\n",
      "Epoch 9070/10000, Training Loss: 0.65263831615448, Validation Loss: 0.6579299569129944\n",
      "Epoch 9071/10000, Training Loss: 0.6603354811668396, Validation Loss: 0.4126352369785309\n",
      "Epoch 9072/10000, Training Loss: 0.6616526246070862, Validation Loss: 0.6885478496551514\n",
      "Epoch 9073/10000, Training Loss: 0.6005527973175049, Validation Loss: 0.7967252135276794\n",
      "Epoch 9074/10000, Training Loss: 0.6471201777458191, Validation Loss: 0.7096209526062012\n",
      "Epoch 9075/10000, Training Loss: 0.645139753818512, Validation Loss: 0.7402490973472595\n",
      "Epoch 9076/10000, Training Loss: 0.6674359440803528, Validation Loss: 0.5828152298927307\n",
      "Epoch 9077/10000, Training Loss: 0.6111513376235962, Validation Loss: 0.5642402768135071\n",
      "Epoch 9078/10000, Training Loss: 0.6418529152870178, Validation Loss: 0.7479932904243469\n",
      "Epoch 9079/10000, Training Loss: 0.6767049431800842, Validation Loss: 0.8734908103942871\n",
      "Epoch 9080/10000, Training Loss: 0.6129862666130066, Validation Loss: 0.5418788194656372\n",
      "Epoch 9081/10000, Training Loss: 0.6301149725914001, Validation Loss: 0.733454167842865\n",
      "Epoch 9082/10000, Training Loss: 0.6265252828598022, Validation Loss: 0.5676625370979309\n",
      "Epoch 9083/10000, Training Loss: 0.6287053823471069, Validation Loss: 0.39036402106285095\n",
      "Epoch 9084/10000, Training Loss: 0.6398409605026245, Validation Loss: 0.7860265374183655\n",
      "Epoch 9085/10000, Training Loss: 0.6299359202384949, Validation Loss: 0.5854600071907043\n",
      "Epoch 9086/10000, Training Loss: 0.6460808515548706, Validation Loss: 0.9280363917350769\n",
      "Epoch 9087/10000, Training Loss: 0.6529904007911682, Validation Loss: 0.687384307384491\n",
      "Epoch 9088/10000, Training Loss: 0.6216862797737122, Validation Loss: 0.5272414088249207\n",
      "Epoch 9089/10000, Training Loss: 0.6351599097251892, Validation Loss: 0.7775607705116272\n",
      "Epoch 9090/10000, Training Loss: 0.6284980773925781, Validation Loss: 0.6846233010292053\n",
      "Epoch 9091/10000, Training Loss: 0.647487998008728, Validation Loss: 0.7135848999023438\n",
      "Epoch 9092/10000, Training Loss: 0.6558055281639099, Validation Loss: 0.562213122844696\n",
      "Epoch 9093/10000, Training Loss: 0.5889630913734436, Validation Loss: 0.6519555449485779\n",
      "Epoch 9094/10000, Training Loss: 0.6183192729949951, Validation Loss: 0.6492152810096741\n",
      "Epoch 9095/10000, Training Loss: 0.6068761348724365, Validation Loss: 0.6881494522094727\n",
      "Epoch 9096/10000, Training Loss: 0.6399231553077698, Validation Loss: 0.62076735496521\n",
      "Epoch 9097/10000, Training Loss: 0.6591861248016357, Validation Loss: 0.7895386219024658\n",
      "Epoch 9098/10000, Training Loss: 0.6387897729873657, Validation Loss: 0.9615482687950134\n",
      "Epoch 9099/10000, Training Loss: 0.6766590476036072, Validation Loss: 0.6439982056617737\n",
      "Epoch 9100/10000, Training Loss: 0.6281512379646301, Validation Loss: 0.898400068283081\n",
      "Epoch 9101/10000, Training Loss: 0.6512444615364075, Validation Loss: 0.7865856289863586\n",
      "Epoch 9102/10000, Training Loss: 0.6470181345939636, Validation Loss: 0.673608124256134\n",
      "Epoch 9103/10000, Training Loss: 0.6484949588775635, Validation Loss: 0.681093692779541\n",
      "Epoch 9104/10000, Training Loss: 0.64327073097229, Validation Loss: 0.6628267765045166\n",
      "Epoch 9105/10000, Training Loss: 0.660447359085083, Validation Loss: 0.7038031220436096\n",
      "Epoch 9106/10000, Training Loss: 0.6445667743682861, Validation Loss: 0.7671956419944763\n",
      "Epoch 9107/10000, Training Loss: 0.6684466600418091, Validation Loss: 0.7474794387817383\n",
      "Epoch 9108/10000, Training Loss: 0.6224752068519592, Validation Loss: 0.6891480088233948\n",
      "Epoch 9109/10000, Training Loss: 0.6316349506378174, Validation Loss: 0.8264610767364502\n",
      "Epoch 9110/10000, Training Loss: 0.6351231336593628, Validation Loss: 0.640717625617981\n",
      "Epoch 9111/10000, Training Loss: 0.6913643479347229, Validation Loss: 0.6110388040542603\n",
      "Epoch 9112/10000, Training Loss: 0.6211461424827576, Validation Loss: 0.6745616793632507\n",
      "Epoch 9113/10000, Training Loss: 0.6668539643287659, Validation Loss: 0.6248261332511902\n",
      "Epoch 9114/10000, Training Loss: 0.6199976801872253, Validation Loss: 0.9192988276481628\n",
      "Epoch 9115/10000, Training Loss: 0.6376158595085144, Validation Loss: 0.713085412979126\n",
      "Epoch 9116/10000, Training Loss: 0.6221402883529663, Validation Loss: 0.8425965905189514\n",
      "Epoch 9117/10000, Training Loss: 0.6250864863395691, Validation Loss: 0.8161267638206482\n",
      "Epoch 9118/10000, Training Loss: 0.635041356086731, Validation Loss: 0.5899190306663513\n",
      "Epoch 9119/10000, Training Loss: 0.6216338276863098, Validation Loss: 0.5702051520347595\n",
      "Epoch 9120/10000, Training Loss: 0.6312078833580017, Validation Loss: 0.5827059149742126\n",
      "Epoch 9121/10000, Training Loss: 0.6292527914047241, Validation Loss: 0.8842608332633972\n",
      "Epoch 9122/10000, Training Loss: 0.6339921355247498, Validation Loss: 0.608182430267334\n",
      "Epoch 9123/10000, Training Loss: 0.6059330105781555, Validation Loss: 0.6442225575447083\n",
      "Epoch 9124/10000, Training Loss: 0.6520049571990967, Validation Loss: 0.7759742736816406\n",
      "Epoch 9125/10000, Training Loss: 0.6175782680511475, Validation Loss: 0.5344862937927246\n",
      "Epoch 9126/10000, Training Loss: 0.6325793862342834, Validation Loss: 0.6964921951293945\n",
      "Epoch 9127/10000, Training Loss: 0.6187105774879456, Validation Loss: 0.8395578861236572\n",
      "Epoch 9128/10000, Training Loss: 0.6690475940704346, Validation Loss: 0.5313138365745544\n",
      "Epoch 9129/10000, Training Loss: 0.6577918529510498, Validation Loss: 0.8794856667518616\n",
      "Epoch 9130/10000, Training Loss: 0.6699424982070923, Validation Loss: 0.7203665375709534\n",
      "Epoch 9131/10000, Training Loss: 0.6183658242225647, Validation Loss: 0.6199691295623779\n",
      "Epoch 9132/10000, Training Loss: 0.6171332597732544, Validation Loss: 0.727250874042511\n",
      "Epoch 9133/10000, Training Loss: 0.635617196559906, Validation Loss: 0.8880205154418945\n",
      "Epoch 9134/10000, Training Loss: 0.6759232878684998, Validation Loss: 0.650077760219574\n",
      "Epoch 9135/10000, Training Loss: 0.6424373388290405, Validation Loss: 0.8064277768135071\n",
      "Epoch 9136/10000, Training Loss: 0.6578964591026306, Validation Loss: 0.7474460005760193\n",
      "Epoch 9137/10000, Training Loss: 0.6354808807373047, Validation Loss: 0.881298303604126\n",
      "Epoch 9138/10000, Training Loss: 0.644677460193634, Validation Loss: 0.6307012438774109\n",
      "Epoch 9139/10000, Training Loss: 0.6449613571166992, Validation Loss: 0.7722660899162292\n",
      "Epoch 9140/10000, Training Loss: 0.6490662097930908, Validation Loss: 0.596921443939209\n",
      "Epoch 9141/10000, Training Loss: 0.6659044623374939, Validation Loss: 0.7602308392524719\n",
      "Epoch 9142/10000, Training Loss: 0.614041268825531, Validation Loss: 0.7999539375305176\n",
      "Epoch 9143/10000, Training Loss: 0.6378538608551025, Validation Loss: 0.6804711222648621\n",
      "Epoch 9144/10000, Training Loss: 0.6197060942649841, Validation Loss: 0.6142621040344238\n",
      "Epoch 9145/10000, Training Loss: 0.6500136852264404, Validation Loss: 0.6802499294281006\n",
      "Epoch 9146/10000, Training Loss: 0.5952411890029907, Validation Loss: 0.4673589766025543\n",
      "Epoch 9147/10000, Training Loss: 0.666122555732727, Validation Loss: 0.6661478281021118\n",
      "Epoch 9148/10000, Training Loss: 0.6534407138824463, Validation Loss: 0.7496179938316345\n",
      "Epoch 9149/10000, Training Loss: 0.6478621959686279, Validation Loss: 0.49123141169548035\n",
      "Epoch 9150/10000, Training Loss: 0.6635637283325195, Validation Loss: 0.4860254228115082\n",
      "Epoch 9151/10000, Training Loss: 0.6381903886795044, Validation Loss: 0.6492587327957153\n",
      "Epoch 9152/10000, Training Loss: 0.6286625266075134, Validation Loss: 0.6855769753456116\n",
      "Epoch 9153/10000, Training Loss: 0.6171665191650391, Validation Loss: 0.6125221848487854\n",
      "Epoch 9154/10000, Training Loss: 0.679937481880188, Validation Loss: 0.5820706486701965\n",
      "Epoch 9155/10000, Training Loss: 0.6069667339324951, Validation Loss: 0.7122251987457275\n",
      "Epoch 9156/10000, Training Loss: 0.6554460525512695, Validation Loss: 0.762538492679596\n",
      "Epoch 9157/10000, Training Loss: 0.6538255214691162, Validation Loss: 0.7842860817909241\n",
      "Epoch 9158/10000, Training Loss: 0.6390053033828735, Validation Loss: 0.5746235847473145\n",
      "Epoch 9159/10000, Training Loss: 0.6519845724105835, Validation Loss: 0.946010172367096\n",
      "Epoch 9160/10000, Training Loss: 0.6349395513534546, Validation Loss: 0.6763358116149902\n",
      "Epoch 9161/10000, Training Loss: 0.6238054633140564, Validation Loss: 0.6710454821586609\n",
      "Epoch 9162/10000, Training Loss: 0.6424996256828308, Validation Loss: 0.7497517466545105\n",
      "Epoch 9163/10000, Training Loss: 0.6541642546653748, Validation Loss: 0.6739141941070557\n",
      "Epoch 9164/10000, Training Loss: 0.658977746963501, Validation Loss: 0.7307228446006775\n",
      "Epoch 9165/10000, Training Loss: 0.6192667484283447, Validation Loss: 0.6113170385360718\n",
      "Epoch 9166/10000, Training Loss: 0.6625053882598877, Validation Loss: 0.6065279841423035\n",
      "Epoch 9167/10000, Training Loss: 0.6216727495193481, Validation Loss: 0.8299501538276672\n",
      "Epoch 9168/10000, Training Loss: 0.6365792155265808, Validation Loss: 0.54819256067276\n",
      "Epoch 9169/10000, Training Loss: 0.7145674228668213, Validation Loss: 0.7887162566184998\n",
      "Epoch 9170/10000, Training Loss: 0.5932241678237915, Validation Loss: 0.7686188817024231\n",
      "Epoch 9171/10000, Training Loss: 0.6199851632118225, Validation Loss: 0.5503479838371277\n",
      "Epoch 9172/10000, Training Loss: 0.6281141638755798, Validation Loss: 0.8618484139442444\n",
      "Epoch 9173/10000, Training Loss: 0.6664139628410339, Validation Loss: 0.7057302594184875\n",
      "Epoch 9174/10000, Training Loss: 0.6490691900253296, Validation Loss: 0.6609771847724915\n",
      "Epoch 9175/10000, Training Loss: 0.6295144557952881, Validation Loss: 0.4928584396839142\n",
      "Epoch 9176/10000, Training Loss: 0.6143146753311157, Validation Loss: 0.6237717270851135\n",
      "Epoch 9177/10000, Training Loss: 0.6478959321975708, Validation Loss: 0.7623689770698547\n",
      "Epoch 9178/10000, Training Loss: 0.6150686144828796, Validation Loss: 0.5702289342880249\n",
      "Epoch 9179/10000, Training Loss: 0.5921940207481384, Validation Loss: 0.6855373978614807\n",
      "Epoch 9180/10000, Training Loss: 0.6529947519302368, Validation Loss: 0.7207552790641785\n",
      "Epoch 9181/10000, Training Loss: 0.6462501883506775, Validation Loss: 0.7403329014778137\n",
      "Epoch 9182/10000, Training Loss: 0.6867311000823975, Validation Loss: 0.6143493056297302\n",
      "Epoch 9183/10000, Training Loss: 0.6243336200714111, Validation Loss: 0.6613970398902893\n",
      "Epoch 9184/10000, Training Loss: 0.627682626247406, Validation Loss: 0.5009353756904602\n",
      "Epoch 9185/10000, Training Loss: 0.624190628528595, Validation Loss: 0.5899213552474976\n",
      "Epoch 9186/10000, Training Loss: 0.640787661075592, Validation Loss: 0.6658807396888733\n",
      "Epoch 9187/10000, Training Loss: 0.6274976134300232, Validation Loss: 0.7414355278015137\n",
      "Epoch 9188/10000, Training Loss: 0.588681161403656, Validation Loss: 0.7877929210662842\n",
      "Epoch 9189/10000, Training Loss: 0.6722792387008667, Validation Loss: 0.6603338718414307\n",
      "Epoch 9190/10000, Training Loss: 0.6161098480224609, Validation Loss: 0.7632059454917908\n",
      "Epoch 9191/10000, Training Loss: 0.6111761331558228, Validation Loss: 0.8479955792427063\n",
      "Epoch 9192/10000, Training Loss: 0.6504145264625549, Validation Loss: 0.6806408762931824\n",
      "Epoch 9193/10000, Training Loss: 0.662960410118103, Validation Loss: 0.581527054309845\n",
      "Epoch 9194/10000, Training Loss: 0.6680330634117126, Validation Loss: 0.5975920557975769\n",
      "Epoch 9195/10000, Training Loss: 0.6521359086036682, Validation Loss: 0.4531097710132599\n",
      "Epoch 9196/10000, Training Loss: 0.597679615020752, Validation Loss: 0.6119201183319092\n",
      "Epoch 9197/10000, Training Loss: 0.618427574634552, Validation Loss: 0.6136575937271118\n",
      "Epoch 9198/10000, Training Loss: 0.6148510575294495, Validation Loss: 0.5449442267417908\n",
      "Epoch 9199/10000, Training Loss: 0.5923449397087097, Validation Loss: 0.6543421149253845\n",
      "Epoch 9200/10000, Training Loss: 0.6047491431236267, Validation Loss: 0.9595982432365417\n",
      "Epoch 9201/10000, Training Loss: 0.6046701073646545, Validation Loss: 0.7011825442314148\n",
      "Epoch 9202/10000, Training Loss: 0.7343078255653381, Validation Loss: 0.4305579960346222\n",
      "Epoch 9203/10000, Training Loss: 0.6241405010223389, Validation Loss: 0.9250264763832092\n",
      "Epoch 9204/10000, Training Loss: 0.6517860889434814, Validation Loss: 0.7479770183563232\n",
      "Epoch 9205/10000, Training Loss: 0.6474170684814453, Validation Loss: 0.7310207486152649\n",
      "Epoch 9206/10000, Training Loss: 0.6347478628158569, Validation Loss: 0.6738592982292175\n",
      "Epoch 9207/10000, Training Loss: 0.6507503986358643, Validation Loss: 0.6859800219535828\n",
      "Epoch 9208/10000, Training Loss: 0.6203391551971436, Validation Loss: 0.7568826079368591\n",
      "Epoch 9209/10000, Training Loss: 0.6451561450958252, Validation Loss: 0.6057767271995544\n",
      "Epoch 9210/10000, Training Loss: 0.6455308198928833, Validation Loss: 0.6967785358428955\n",
      "Epoch 9211/10000, Training Loss: 0.6320000290870667, Validation Loss: 0.7629296183586121\n",
      "Epoch 9212/10000, Training Loss: 0.6414119005203247, Validation Loss: 0.7325144410133362\n",
      "Epoch 9213/10000, Training Loss: 0.6406771540641785, Validation Loss: 0.6458510756492615\n",
      "Epoch 9214/10000, Training Loss: 0.5980640053749084, Validation Loss: 0.6472315192222595\n",
      "Epoch 9215/10000, Training Loss: 0.6228423714637756, Validation Loss: 0.5706920027732849\n",
      "Epoch 9216/10000, Training Loss: 0.6062833070755005, Validation Loss: 0.6999163031578064\n",
      "Epoch 9217/10000, Training Loss: 0.66202712059021, Validation Loss: 0.47241029143333435\n",
      "Epoch 9218/10000, Training Loss: 0.6882542967796326, Validation Loss: 0.527761697769165\n",
      "Epoch 9219/10000, Training Loss: 0.6062290072441101, Validation Loss: 0.5553346276283264\n",
      "Epoch 9220/10000, Training Loss: 0.6454887390136719, Validation Loss: 0.7526714205741882\n",
      "Epoch 9221/10000, Training Loss: 0.6413001418113708, Validation Loss: 0.6196593642234802\n",
      "Epoch 9222/10000, Training Loss: 0.6216233372688293, Validation Loss: 0.7934473156929016\n",
      "Epoch 9223/10000, Training Loss: 0.6242700219154358, Validation Loss: 0.753140389919281\n",
      "Epoch 9224/10000, Training Loss: 0.5981308221817017, Validation Loss: 0.5589070916175842\n",
      "Epoch 9225/10000, Training Loss: 0.6277183890342712, Validation Loss: 0.7137525677680969\n",
      "Epoch 9226/10000, Training Loss: 0.6342868804931641, Validation Loss: 0.7617072463035583\n",
      "Epoch 9227/10000, Training Loss: 0.607483983039856, Validation Loss: 0.6803359985351562\n",
      "Epoch 9228/10000, Training Loss: 0.6396206617355347, Validation Loss: 0.6916629672050476\n",
      "Epoch 9229/10000, Training Loss: 0.6337120532989502, Validation Loss: 0.8291196823120117\n",
      "Epoch 9230/10000, Training Loss: 0.6231288313865662, Validation Loss: 1.0761016607284546\n",
      "Epoch 9231/10000, Training Loss: 0.5896758437156677, Validation Loss: 0.7224259972572327\n",
      "Epoch 9232/10000, Training Loss: 0.6335371732711792, Validation Loss: 0.6782209873199463\n",
      "Epoch 9233/10000, Training Loss: 0.6648004055023193, Validation Loss: 0.7080841064453125\n",
      "Epoch 9234/10000, Training Loss: 0.6267556548118591, Validation Loss: 0.5842800736427307\n",
      "Epoch 9235/10000, Training Loss: 0.6467562913894653, Validation Loss: 0.594547688961029\n",
      "Epoch 9236/10000, Training Loss: 0.6241337060928345, Validation Loss: 0.6909677386283875\n",
      "Epoch 9237/10000, Training Loss: 0.6633679270744324, Validation Loss: 0.5882176756858826\n",
      "Epoch 9238/10000, Training Loss: 0.6604903340339661, Validation Loss: 0.47147810459136963\n",
      "Epoch 9239/10000, Training Loss: 0.6322213411331177, Validation Loss: 0.7036919593811035\n",
      "Epoch 9240/10000, Training Loss: 0.6548802256584167, Validation Loss: 0.7451065182685852\n",
      "Epoch 9241/10000, Training Loss: 0.6502299904823303, Validation Loss: 0.6195407509803772\n",
      "Epoch 9242/10000, Training Loss: 0.6215356588363647, Validation Loss: 0.7768364548683167\n",
      "Epoch 9243/10000, Training Loss: 0.6086283922195435, Validation Loss: 0.7429163455963135\n",
      "Epoch 9244/10000, Training Loss: 0.6667103171348572, Validation Loss: 0.6380109190940857\n",
      "Epoch 9245/10000, Training Loss: 0.6388142704963684, Validation Loss: 0.6302282214164734\n",
      "Epoch 9246/10000, Training Loss: 0.6420058012008667, Validation Loss: 0.8905279636383057\n",
      "Epoch 9247/10000, Training Loss: 0.601733922958374, Validation Loss: 0.6616760492324829\n",
      "Epoch 9248/10000, Training Loss: 0.6228776574134827, Validation Loss: 0.6832337975502014\n",
      "Epoch 9249/10000, Training Loss: 0.67611163854599, Validation Loss: 0.6712729930877686\n",
      "Epoch 9250/10000, Training Loss: 0.632450520992279, Validation Loss: 0.712981641292572\n",
      "Epoch 9251/10000, Training Loss: 0.6141314506530762, Validation Loss: 0.6173527240753174\n",
      "Epoch 9252/10000, Training Loss: 0.6620255708694458, Validation Loss: 0.9283004403114319\n",
      "Epoch 9253/10000, Training Loss: 0.6455989480018616, Validation Loss: 0.6149611473083496\n",
      "Epoch 9254/10000, Training Loss: 0.66131591796875, Validation Loss: 0.7141683101654053\n",
      "Epoch 9255/10000, Training Loss: 0.6441748142242432, Validation Loss: 0.8558876514434814\n",
      "Epoch 9256/10000, Training Loss: 0.6256888508796692, Validation Loss: 0.4698624908924103\n",
      "Epoch 9257/10000, Training Loss: 0.6118024587631226, Validation Loss: 0.7970161437988281\n",
      "Epoch 9258/10000, Training Loss: 0.6116663217544556, Validation Loss: 0.6370919346809387\n",
      "Epoch 9259/10000, Training Loss: 0.6622664928436279, Validation Loss: 0.7397754788398743\n",
      "Epoch 9260/10000, Training Loss: 0.6275371313095093, Validation Loss: 0.6714381575584412\n",
      "Epoch 9261/10000, Training Loss: 0.6560249924659729, Validation Loss: 0.5434064269065857\n",
      "Epoch 9262/10000, Training Loss: 0.6132934093475342, Validation Loss: 0.6348133087158203\n",
      "Epoch 9263/10000, Training Loss: 0.6700673699378967, Validation Loss: 0.7581045031547546\n",
      "Epoch 9264/10000, Training Loss: 0.6500273942947388, Validation Loss: 0.923559844493866\n",
      "Epoch 9265/10000, Training Loss: 0.6562929153442383, Validation Loss: 0.5122068524360657\n",
      "Epoch 9266/10000, Training Loss: 0.6132758855819702, Validation Loss: 0.635719895362854\n",
      "Epoch 9267/10000, Training Loss: 0.6336698532104492, Validation Loss: 0.6451544761657715\n",
      "Epoch 9268/10000, Training Loss: 0.6264548897743225, Validation Loss: 0.8093042373657227\n",
      "Epoch 9269/10000, Training Loss: 0.653598427772522, Validation Loss: 0.8199722170829773\n",
      "Epoch 9270/10000, Training Loss: 0.6345080137252808, Validation Loss: 0.7603911757469177\n",
      "Epoch 9271/10000, Training Loss: 0.6531803607940674, Validation Loss: 0.7501625418663025\n",
      "Epoch 9272/10000, Training Loss: 0.6573801636695862, Validation Loss: 0.6262943148612976\n",
      "Epoch 9273/10000, Training Loss: 0.7127180695533752, Validation Loss: 0.6619325280189514\n",
      "Epoch 9274/10000, Training Loss: 0.6908840537071228, Validation Loss: 0.6420333385467529\n",
      "Epoch 9275/10000, Training Loss: 0.6255489587783813, Validation Loss: 0.7833082675933838\n",
      "Epoch 9276/10000, Training Loss: 0.6414628624916077, Validation Loss: 0.6604782938957214\n",
      "Epoch 9277/10000, Training Loss: 0.6010043621063232, Validation Loss: 0.7489458918571472\n",
      "Epoch 9278/10000, Training Loss: 0.6397331953048706, Validation Loss: 0.6307300925254822\n",
      "Epoch 9279/10000, Training Loss: 0.6535306572914124, Validation Loss: 0.5378674864768982\n",
      "Epoch 9280/10000, Training Loss: 0.6216078400611877, Validation Loss: 0.9252689480781555\n",
      "Epoch 9281/10000, Training Loss: 0.6233166456222534, Validation Loss: 0.7602830529212952\n",
      "Epoch 9282/10000, Training Loss: 0.6136890053749084, Validation Loss: 0.9711148738861084\n",
      "Epoch 9283/10000, Training Loss: 0.6507890224456787, Validation Loss: 0.6308258771896362\n",
      "Epoch 9284/10000, Training Loss: 0.6210492849349976, Validation Loss: 0.5324638485908508\n",
      "Epoch 9285/10000, Training Loss: 0.656375527381897, Validation Loss: 0.71968013048172\n",
      "Epoch 9286/10000, Training Loss: 0.6657795310020447, Validation Loss: 0.7220709919929504\n",
      "Epoch 9287/10000, Training Loss: 0.6458615064620972, Validation Loss: 0.666090190410614\n",
      "Epoch 9288/10000, Training Loss: 0.6436437368392944, Validation Loss: 0.6090928912162781\n",
      "Epoch 9289/10000, Training Loss: 0.6226963400840759, Validation Loss: 0.7280860543251038\n",
      "Epoch 9290/10000, Training Loss: 0.671033501625061, Validation Loss: 0.5934785008430481\n",
      "Epoch 9291/10000, Training Loss: 0.6572462916374207, Validation Loss: 0.5925652384757996\n",
      "Epoch 9292/10000, Training Loss: 0.6618627309799194, Validation Loss: 0.6375576257705688\n",
      "Epoch 9293/10000, Training Loss: 0.6135281324386597, Validation Loss: 0.928209125995636\n",
      "Epoch 9294/10000, Training Loss: 0.6915862560272217, Validation Loss: 0.7006978392601013\n",
      "Epoch 9295/10000, Training Loss: 0.67831951379776, Validation Loss: 0.6760755181312561\n",
      "Epoch 9296/10000, Training Loss: 0.6569340825080872, Validation Loss: 0.7587661743164062\n",
      "Epoch 9297/10000, Training Loss: 0.6277641654014587, Validation Loss: 0.6454623341560364\n",
      "Epoch 9298/10000, Training Loss: 0.6182231307029724, Validation Loss: 0.7409060597419739\n",
      "Epoch 9299/10000, Training Loss: 0.6142435073852539, Validation Loss: 0.5832405686378479\n",
      "Epoch 9300/10000, Training Loss: 0.6194226145744324, Validation Loss: 0.6378046274185181\n",
      "Epoch 9301/10000, Training Loss: 0.631860613822937, Validation Loss: 0.809783935546875\n",
      "Epoch 9302/10000, Training Loss: 0.5945178270339966, Validation Loss: 0.6573549509048462\n",
      "Epoch 9303/10000, Training Loss: 0.5999530553817749, Validation Loss: 0.6835808753967285\n",
      "Epoch 9304/10000, Training Loss: 0.5945708751678467, Validation Loss: 0.6143842935562134\n",
      "Epoch 9305/10000, Training Loss: 0.6155523061752319, Validation Loss: 0.7519579529762268\n",
      "Epoch 9306/10000, Training Loss: 0.6506813764572144, Validation Loss: 0.6711638569831848\n",
      "Epoch 9307/10000, Training Loss: 0.628634512424469, Validation Loss: 0.683443546295166\n",
      "Epoch 9308/10000, Training Loss: 0.6033324003219604, Validation Loss: 0.8232137560844421\n",
      "Epoch 9309/10000, Training Loss: 0.625053346157074, Validation Loss: 0.4914034903049469\n",
      "Epoch 9310/10000, Training Loss: 0.6270450949668884, Validation Loss: 0.7030048370361328\n",
      "Epoch 9311/10000, Training Loss: 0.6292434930801392, Validation Loss: 0.766735851764679\n",
      "Epoch 9312/10000, Training Loss: 0.6726394891738892, Validation Loss: 0.5906929969787598\n",
      "Epoch 9313/10000, Training Loss: 0.6417822241783142, Validation Loss: 0.724214494228363\n",
      "Epoch 9314/10000, Training Loss: 0.636480987071991, Validation Loss: 0.7269184589385986\n",
      "Epoch 9315/10000, Training Loss: 0.6281046271324158, Validation Loss: 0.6597772240638733\n",
      "Epoch 9316/10000, Training Loss: 0.6612297892570496, Validation Loss: 0.706491231918335\n",
      "Epoch 9317/10000, Training Loss: 0.6528609395027161, Validation Loss: 0.4548463821411133\n",
      "Epoch 9318/10000, Training Loss: 0.5960880517959595, Validation Loss: 0.7260169386863708\n",
      "Epoch 9319/10000, Training Loss: 0.6795514822006226, Validation Loss: 0.41128888726234436\n",
      "Epoch 9320/10000, Training Loss: 0.6729841232299805, Validation Loss: 0.8672166466712952\n",
      "Epoch 9321/10000, Training Loss: 0.6177539825439453, Validation Loss: 0.7256374359130859\n",
      "Epoch 9322/10000, Training Loss: 0.6620177030563354, Validation Loss: 0.9022229313850403\n",
      "Epoch 9323/10000, Training Loss: 0.6407386064529419, Validation Loss: 0.7577871680259705\n",
      "Epoch 9324/10000, Training Loss: 0.664124071598053, Validation Loss: 0.7027372717857361\n",
      "Epoch 9325/10000, Training Loss: 0.6412448883056641, Validation Loss: 0.7691386342048645\n",
      "Epoch 9326/10000, Training Loss: 0.6475826501846313, Validation Loss: 1.2444671392440796\n",
      "Epoch 9327/10000, Training Loss: 0.6391432881355286, Validation Loss: 0.8176897168159485\n",
      "Epoch 9328/10000, Training Loss: 0.6767255067825317, Validation Loss: 0.5843197703361511\n",
      "Epoch 9329/10000, Training Loss: 0.6449650526046753, Validation Loss: 0.6673514246940613\n",
      "Epoch 9330/10000, Training Loss: 0.6486565470695496, Validation Loss: 0.6143297553062439\n",
      "Epoch 9331/10000, Training Loss: 0.6557307839393616, Validation Loss: 0.7579329013824463\n",
      "Epoch 9332/10000, Training Loss: 0.6419007182121277, Validation Loss: 0.74286288022995\n",
      "Epoch 9333/10000, Training Loss: 0.6984135508537292, Validation Loss: 0.6166682839393616\n",
      "Epoch 9334/10000, Training Loss: 0.6751359105110168, Validation Loss: 0.6939992904663086\n",
      "Epoch 9335/10000, Training Loss: 0.6229476928710938, Validation Loss: 0.8106892704963684\n",
      "Epoch 9336/10000, Training Loss: 0.6351786255836487, Validation Loss: 0.7499141097068787\n",
      "Epoch 9337/10000, Training Loss: 0.6664167046546936, Validation Loss: 0.5987101197242737\n",
      "Epoch 9338/10000, Training Loss: 0.6324547529220581, Validation Loss: 0.7709929347038269\n",
      "Epoch 9339/10000, Training Loss: 0.6744916439056396, Validation Loss: 0.8542348742485046\n",
      "Epoch 9340/10000, Training Loss: 0.6656166911125183, Validation Loss: 0.861509382724762\n",
      "Epoch 9341/10000, Training Loss: 0.6421575546264648, Validation Loss: 0.7043394446372986\n",
      "Epoch 9342/10000, Training Loss: 0.6441130042076111, Validation Loss: 0.6625946164131165\n",
      "Epoch 9343/10000, Training Loss: 0.6488044261932373, Validation Loss: 0.8077855110168457\n",
      "Epoch 9344/10000, Training Loss: 0.6303303241729736, Validation Loss: 0.5599234104156494\n",
      "Epoch 9345/10000, Training Loss: 0.6376224160194397, Validation Loss: 0.6465532183647156\n",
      "Epoch 9346/10000, Training Loss: 0.6472449898719788, Validation Loss: 0.7041906714439392\n",
      "Epoch 9347/10000, Training Loss: 0.6455869674682617, Validation Loss: 0.7291561961174011\n",
      "Epoch 9348/10000, Training Loss: 0.6438499093055725, Validation Loss: 0.5750947594642639\n",
      "Epoch 9349/10000, Training Loss: 0.637607991695404, Validation Loss: 0.729168176651001\n",
      "Epoch 9350/10000, Training Loss: 0.6473580002784729, Validation Loss: 0.8529356122016907\n",
      "Epoch 9351/10000, Training Loss: 0.6293262243270874, Validation Loss: 0.8108603954315186\n",
      "Epoch 9352/10000, Training Loss: 0.6021072864532471, Validation Loss: 0.5343120098114014\n",
      "Epoch 9353/10000, Training Loss: 0.6157602667808533, Validation Loss: 0.5138514637947083\n",
      "Epoch 9354/10000, Training Loss: 0.646959662437439, Validation Loss: 0.6046583652496338\n",
      "Epoch 9355/10000, Training Loss: 0.6188539862632751, Validation Loss: 0.7025351524353027\n",
      "Epoch 9356/10000, Training Loss: 0.6520916819572449, Validation Loss: 0.5229708552360535\n",
      "Epoch 9357/10000, Training Loss: 0.6558777689933777, Validation Loss: 0.796692430973053\n",
      "Epoch 9358/10000, Training Loss: 0.6271442174911499, Validation Loss: 0.48219287395477295\n",
      "Epoch 9359/10000, Training Loss: 0.6401875019073486, Validation Loss: 0.5270198583602905\n",
      "Epoch 9360/10000, Training Loss: 0.7064245939254761, Validation Loss: 0.5715657472610474\n",
      "Epoch 9361/10000, Training Loss: 0.6436893939971924, Validation Loss: 0.5973723530769348\n",
      "Epoch 9362/10000, Training Loss: 0.6543706059455872, Validation Loss: 0.7203977704048157\n",
      "Epoch 9363/10000, Training Loss: 0.6131594777107239, Validation Loss: 0.7728466391563416\n",
      "Epoch 9364/10000, Training Loss: 0.6306353807449341, Validation Loss: 0.7037160992622375\n",
      "Epoch 9365/10000, Training Loss: 0.6300280094146729, Validation Loss: 0.6807919144630432\n",
      "Epoch 9366/10000, Training Loss: 0.6663822531700134, Validation Loss: 0.7030501961708069\n",
      "Epoch 9367/10000, Training Loss: 0.6807210445404053, Validation Loss: 0.5592376589775085\n",
      "Epoch 9368/10000, Training Loss: 0.619205892086029, Validation Loss: 0.6834881901741028\n",
      "Epoch 9369/10000, Training Loss: 0.6283658742904663, Validation Loss: 0.8381455540657043\n",
      "Epoch 9370/10000, Training Loss: 0.6545653939247131, Validation Loss: 0.6541923880577087\n",
      "Epoch 9371/10000, Training Loss: 0.6415924429893494, Validation Loss: 0.6416978240013123\n",
      "Epoch 9372/10000, Training Loss: 0.6704981327056885, Validation Loss: 0.8544783592224121\n",
      "Epoch 9373/10000, Training Loss: 0.6542742848396301, Validation Loss: 0.882343590259552\n",
      "Epoch 9374/10000, Training Loss: 0.6549691557884216, Validation Loss: 0.6537503004074097\n",
      "Epoch 9375/10000, Training Loss: 0.6786105632781982, Validation Loss: 0.6394174695014954\n",
      "Epoch 9376/10000, Training Loss: 0.6242678761482239, Validation Loss: 0.8607692718505859\n",
      "Epoch 9377/10000, Training Loss: 0.6272405385971069, Validation Loss: 0.7118489146232605\n",
      "Epoch 9378/10000, Training Loss: 0.6173712015151978, Validation Loss: 0.7063257694244385\n",
      "Epoch 9379/10000, Training Loss: 0.6135954260826111, Validation Loss: 0.7269485592842102\n",
      "Epoch 9380/10000, Training Loss: 0.6741511225700378, Validation Loss: 0.6951839327812195\n",
      "Epoch 9381/10000, Training Loss: 0.6640047430992126, Validation Loss: 0.7280396819114685\n",
      "Epoch 9382/10000, Training Loss: 0.6105864644050598, Validation Loss: 0.6283594965934753\n",
      "Epoch 9383/10000, Training Loss: 0.6443843245506287, Validation Loss: 0.6480384469032288\n",
      "Epoch 9384/10000, Training Loss: 0.6517976522445679, Validation Loss: 0.7303688526153564\n",
      "Epoch 9385/10000, Training Loss: 0.6161285638809204, Validation Loss: 0.5539000630378723\n",
      "Epoch 9386/10000, Training Loss: 0.6916088461875916, Validation Loss: 0.6544340252876282\n",
      "Epoch 9387/10000, Training Loss: 0.6375561356544495, Validation Loss: 0.6451756358146667\n",
      "Epoch 9388/10000, Training Loss: 0.6082568764686584, Validation Loss: 0.7803873419761658\n",
      "Epoch 9389/10000, Training Loss: 0.6772480607032776, Validation Loss: 0.6783377528190613\n",
      "Epoch 9390/10000, Training Loss: 0.6431114673614502, Validation Loss: 0.6839766502380371\n",
      "Epoch 9391/10000, Training Loss: 0.6546226143836975, Validation Loss: 0.6582028865814209\n",
      "Epoch 9392/10000, Training Loss: 0.6243447065353394, Validation Loss: 0.5490201711654663\n",
      "Epoch 9393/10000, Training Loss: 0.6306057572364807, Validation Loss: 0.8817971348762512\n",
      "Epoch 9394/10000, Training Loss: 0.6574437022209167, Validation Loss: 0.7588467597961426\n",
      "Epoch 9395/10000, Training Loss: 0.653008759021759, Validation Loss: 0.640077531337738\n",
      "Epoch 9396/10000, Training Loss: 0.6536522507667542, Validation Loss: 0.6062073111534119\n",
      "Epoch 9397/10000, Training Loss: 0.6410142183303833, Validation Loss: 0.7097086906433105\n",
      "Epoch 9398/10000, Training Loss: 0.6636017560958862, Validation Loss: 0.6975903511047363\n",
      "Epoch 9399/10000, Training Loss: 0.6452808976173401, Validation Loss: 0.6728354096412659\n",
      "Epoch 9400/10000, Training Loss: 0.6592142581939697, Validation Loss: 0.879793107509613\n",
      "Epoch 9401/10000, Training Loss: 0.6566209197044373, Validation Loss: 0.8423389792442322\n",
      "Epoch 9402/10000, Training Loss: 0.6283268332481384, Validation Loss: 0.4874303340911865\n",
      "Epoch 9403/10000, Training Loss: 0.6403762102127075, Validation Loss: 0.6869745254516602\n",
      "Epoch 9404/10000, Training Loss: 0.6632197499275208, Validation Loss: 0.66404789686203\n",
      "Epoch 9405/10000, Training Loss: 0.617859423160553, Validation Loss: 0.625178337097168\n",
      "Epoch 9406/10000, Training Loss: 0.6726084351539612, Validation Loss: 0.5847442746162415\n",
      "Epoch 9407/10000, Training Loss: 0.6458286046981812, Validation Loss: 0.8503880500793457\n",
      "Epoch 9408/10000, Training Loss: 0.6431742310523987, Validation Loss: 0.49468016624450684\n",
      "Epoch 9409/10000, Training Loss: 0.6535266637802124, Validation Loss: 0.7754588723182678\n",
      "Epoch 9410/10000, Training Loss: 0.6395622491836548, Validation Loss: 0.8576417565345764\n",
      "Epoch 9411/10000, Training Loss: 0.64283686876297, Validation Loss: 0.7504186034202576\n",
      "Epoch 9412/10000, Training Loss: 0.6320686936378479, Validation Loss: 0.6964752078056335\n",
      "Epoch 9413/10000, Training Loss: 0.6368551850318909, Validation Loss: 0.541450560092926\n",
      "Epoch 9414/10000, Training Loss: 0.6254240274429321, Validation Loss: 0.8018441796302795\n",
      "Epoch 9415/10000, Training Loss: 0.6387099623680115, Validation Loss: 0.7699540257453918\n",
      "Epoch 9416/10000, Training Loss: 0.611726701259613, Validation Loss: 0.7088658213615417\n",
      "Epoch 9417/10000, Training Loss: 0.6349630951881409, Validation Loss: 0.49978673458099365\n",
      "Epoch 9418/10000, Training Loss: 0.6419059038162231, Validation Loss: 0.4814123809337616\n",
      "Epoch 9419/10000, Training Loss: 0.6343364715576172, Validation Loss: 0.5681591629981995\n",
      "Epoch 9420/10000, Training Loss: 0.6558409333229065, Validation Loss: 0.6613858938217163\n",
      "Epoch 9421/10000, Training Loss: 0.6276330351829529, Validation Loss: 0.5067604184150696\n",
      "Epoch 9422/10000, Training Loss: 0.6446899771690369, Validation Loss: 0.7774932384490967\n",
      "Epoch 9423/10000, Training Loss: 0.6428276300430298, Validation Loss: 0.7008756995201111\n",
      "Epoch 9424/10000, Training Loss: 0.6614231467247009, Validation Loss: 0.7878446578979492\n",
      "Epoch 9425/10000, Training Loss: 0.6477051973342896, Validation Loss: 0.6309897303581238\n",
      "Epoch 9426/10000, Training Loss: 0.6878232955932617, Validation Loss: 0.6423718333244324\n",
      "Epoch 9427/10000, Training Loss: 0.6829647421836853, Validation Loss: 0.8822101950645447\n",
      "Epoch 9428/10000, Training Loss: 0.6479047536849976, Validation Loss: 0.4730111360549927\n",
      "Epoch 9429/10000, Training Loss: 0.6334094405174255, Validation Loss: 0.674842894077301\n",
      "Epoch 9430/10000, Training Loss: 0.6415093541145325, Validation Loss: 0.5773058533668518\n",
      "Epoch 9431/10000, Training Loss: 0.6257018446922302, Validation Loss: 0.8198544979095459\n",
      "Epoch 9432/10000, Training Loss: 0.6609417200088501, Validation Loss: 0.7456989288330078\n",
      "Epoch 9433/10000, Training Loss: 0.6406685709953308, Validation Loss: 0.7097894549369812\n",
      "Epoch 9434/10000, Training Loss: 0.6086467504501343, Validation Loss: 0.6428112387657166\n",
      "Epoch 9435/10000, Training Loss: 0.6568742990493774, Validation Loss: 0.658658504486084\n",
      "Epoch 9436/10000, Training Loss: 0.6869131326675415, Validation Loss: 0.8536034226417542\n",
      "Epoch 9437/10000, Training Loss: 0.6361950635910034, Validation Loss: 0.531440019607544\n",
      "Epoch 9438/10000, Training Loss: 0.6379642486572266, Validation Loss: 0.7210378050804138\n",
      "Epoch 9439/10000, Training Loss: 0.6647524237632751, Validation Loss: 0.7288128733634949\n",
      "Epoch 9440/10000, Training Loss: 0.6016913652420044, Validation Loss: 0.793138325214386\n",
      "Epoch 9441/10000, Training Loss: 0.6268974542617798, Validation Loss: 0.5838901996612549\n",
      "Epoch 9442/10000, Training Loss: 0.6609815359115601, Validation Loss: 0.6859119534492493\n",
      "Epoch 9443/10000, Training Loss: 0.634916365146637, Validation Loss: 0.5544212460517883\n",
      "Epoch 9444/10000, Training Loss: 0.6395395994186401, Validation Loss: 0.6937429904937744\n",
      "Epoch 9445/10000, Training Loss: 0.6116268038749695, Validation Loss: 0.7634719014167786\n",
      "Epoch 9446/10000, Training Loss: 0.6398849487304688, Validation Loss: 0.7181561589241028\n",
      "Epoch 9447/10000, Training Loss: 0.6465724110603333, Validation Loss: 0.57884281873703\n",
      "Epoch 9448/10000, Training Loss: 0.6456458568572998, Validation Loss: 0.6498673558235168\n",
      "Epoch 9449/10000, Training Loss: 0.6286609172821045, Validation Loss: 0.6320489645004272\n",
      "Epoch 9450/10000, Training Loss: 0.6303913593292236, Validation Loss: 0.8609402775764465\n",
      "Epoch 9451/10000, Training Loss: 0.6378676891326904, Validation Loss: 0.7147727012634277\n",
      "Epoch 9452/10000, Training Loss: 0.6695967316627502, Validation Loss: 0.6809210181236267\n",
      "Epoch 9453/10000, Training Loss: 0.702439546585083, Validation Loss: 0.5881299376487732\n",
      "Epoch 9454/10000, Training Loss: 0.6061518788337708, Validation Loss: 0.6929218769073486\n",
      "Epoch 9455/10000, Training Loss: 0.6492019295692444, Validation Loss: 0.7193009257316589\n",
      "Epoch 9456/10000, Training Loss: 0.6759364008903503, Validation Loss: 1.0013023614883423\n",
      "Epoch 9457/10000, Training Loss: 0.6424440145492554, Validation Loss: 0.8460696339607239\n",
      "Epoch 9458/10000, Training Loss: 0.6392236948013306, Validation Loss: 0.5823116898536682\n",
      "Epoch 9459/10000, Training Loss: 0.6685190200805664, Validation Loss: 0.7003545165061951\n",
      "Epoch 9460/10000, Training Loss: 0.6494226455688477, Validation Loss: 0.6581372022628784\n",
      "Epoch 9461/10000, Training Loss: 0.5968037843704224, Validation Loss: 0.626391589641571\n",
      "Epoch 9462/10000, Training Loss: 0.652787446975708, Validation Loss: 0.6312584280967712\n",
      "Epoch 9463/10000, Training Loss: 0.6149942874908447, Validation Loss: 0.6241834759712219\n",
      "Epoch 9464/10000, Training Loss: 0.6512749195098877, Validation Loss: 0.6939945220947266\n",
      "Epoch 9465/10000, Training Loss: 0.6596205234527588, Validation Loss: 0.7588708996772766\n",
      "Epoch 9466/10000, Training Loss: 0.6145099997520447, Validation Loss: 0.7550554871559143\n",
      "Epoch 9467/10000, Training Loss: 0.6456406116485596, Validation Loss: 0.7376654744148254\n",
      "Epoch 9468/10000, Training Loss: 0.6462635397911072, Validation Loss: 0.6668738722801208\n",
      "Epoch 9469/10000, Training Loss: 0.657710075378418, Validation Loss: 0.5968373417854309\n",
      "Epoch 9470/10000, Training Loss: 0.6764415502548218, Validation Loss: 0.6990887522697449\n",
      "Epoch 9471/10000, Training Loss: 0.669086754322052, Validation Loss: 0.5017310380935669\n",
      "Epoch 9472/10000, Training Loss: 0.6148073673248291, Validation Loss: 0.714765727519989\n",
      "Epoch 9473/10000, Training Loss: 0.6382896900177002, Validation Loss: 0.7023383975028992\n",
      "Epoch 9474/10000, Training Loss: 0.6449167728424072, Validation Loss: 0.8633949160575867\n",
      "Epoch 9475/10000, Training Loss: 0.6617618203163147, Validation Loss: 0.7797648310661316\n",
      "Epoch 9476/10000, Training Loss: 0.6294313073158264, Validation Loss: 0.7177298665046692\n",
      "Epoch 9477/10000, Training Loss: 0.6279592514038086, Validation Loss: 0.5265738368034363\n",
      "Epoch 9478/10000, Training Loss: 0.6028102040290833, Validation Loss: 0.7318317890167236\n",
      "Epoch 9479/10000, Training Loss: 0.6385732889175415, Validation Loss: 0.6047741174697876\n",
      "Epoch 9480/10000, Training Loss: 0.6154689788818359, Validation Loss: 0.8918434977531433\n",
      "Epoch 9481/10000, Training Loss: 0.608116626739502, Validation Loss: 0.6107271909713745\n",
      "Epoch 9482/10000, Training Loss: 0.6464105248451233, Validation Loss: 0.8182864189147949\n",
      "Epoch 9483/10000, Training Loss: 0.6762376427650452, Validation Loss: 0.653994619846344\n",
      "Epoch 9484/10000, Training Loss: 0.6511410474777222, Validation Loss: 0.5235068202018738\n",
      "Epoch 9485/10000, Training Loss: 0.6069080829620361, Validation Loss: 0.5506461262702942\n",
      "Epoch 9486/10000, Training Loss: 0.5948933959007263, Validation Loss: 0.5489286780357361\n",
      "Epoch 9487/10000, Training Loss: 0.63886958360672, Validation Loss: 0.6865558624267578\n",
      "Epoch 9488/10000, Training Loss: 0.6565252542495728, Validation Loss: 0.511533260345459\n",
      "Epoch 9489/10000, Training Loss: 0.6721429228782654, Validation Loss: 0.8160197138786316\n",
      "Epoch 9490/10000, Training Loss: 0.6747729778289795, Validation Loss: 0.6518943905830383\n",
      "Epoch 9491/10000, Training Loss: 0.6743407845497131, Validation Loss: 0.5630064606666565\n",
      "Epoch 9492/10000, Training Loss: 0.6872056722640991, Validation Loss: 0.5095877647399902\n",
      "Epoch 9493/10000, Training Loss: 0.6260408759117126, Validation Loss: 0.704788863658905\n",
      "Epoch 9494/10000, Training Loss: 0.6208730936050415, Validation Loss: 0.8139637112617493\n",
      "Epoch 9495/10000, Training Loss: 0.6145106554031372, Validation Loss: 0.7144368290901184\n",
      "Epoch 9496/10000, Training Loss: 0.6649585962295532, Validation Loss: 0.7105929851531982\n",
      "Epoch 9497/10000, Training Loss: 0.633811891078949, Validation Loss: 0.7082244753837585\n",
      "Epoch 9498/10000, Training Loss: 0.6470050811767578, Validation Loss: 0.44172680377960205\n",
      "Epoch 9499/10000, Training Loss: 0.6437785029411316, Validation Loss: 0.6453139185905457\n",
      "Epoch 9500/10000, Training Loss: 0.6304981708526611, Validation Loss: 0.519385576248169\n",
      "Epoch 9501/10000, Training Loss: 0.6441014409065247, Validation Loss: 0.775463342666626\n",
      "Epoch 9502/10000, Training Loss: 0.6323090195655823, Validation Loss: 0.6371788382530212\n",
      "Epoch 9503/10000, Training Loss: 0.600403368473053, Validation Loss: 0.6730530261993408\n",
      "Epoch 9504/10000, Training Loss: 0.6714087128639221, Validation Loss: 0.5649400353431702\n",
      "Epoch 9505/10000, Training Loss: 0.6211637854576111, Validation Loss: 0.6551156640052795\n",
      "Epoch 9506/10000, Training Loss: 0.6488974690437317, Validation Loss: 0.7160181999206543\n",
      "Epoch 9507/10000, Training Loss: 0.6471680998802185, Validation Loss: 0.7237089276313782\n",
      "Epoch 9508/10000, Training Loss: 0.6415258646011353, Validation Loss: 0.5550382733345032\n",
      "Epoch 9509/10000, Training Loss: 0.6279764771461487, Validation Loss: 0.7888672947883606\n",
      "Epoch 9510/10000, Training Loss: 0.6352431774139404, Validation Loss: 0.7234147191047668\n",
      "Epoch 9511/10000, Training Loss: 0.6446587443351746, Validation Loss: 0.8454219698905945\n",
      "Epoch 9512/10000, Training Loss: 0.6220966577529907, Validation Loss: 0.8024716973304749\n",
      "Epoch 9513/10000, Training Loss: 0.6510539650917053, Validation Loss: 0.5990107655525208\n",
      "Epoch 9514/10000, Training Loss: 0.6326929330825806, Validation Loss: 0.7414777874946594\n",
      "Epoch 9515/10000, Training Loss: 0.6615713834762573, Validation Loss: 0.6522995829582214\n",
      "Epoch 9516/10000, Training Loss: 0.6029505133628845, Validation Loss: 0.626436173915863\n",
      "Epoch 9517/10000, Training Loss: 0.6309981942176819, Validation Loss: 0.8155694007873535\n",
      "Epoch 9518/10000, Training Loss: 0.6474745273590088, Validation Loss: 0.7384688258171082\n",
      "Epoch 9519/10000, Training Loss: 0.6212357878684998, Validation Loss: 0.8898136019706726\n",
      "Epoch 9520/10000, Training Loss: 0.6624730825424194, Validation Loss: 0.6752050518989563\n",
      "Epoch 9521/10000, Training Loss: 0.6510581374168396, Validation Loss: 0.6823813915252686\n",
      "Epoch 9522/10000, Training Loss: 0.6432879567146301, Validation Loss: 0.6210616230964661\n",
      "Epoch 9523/10000, Training Loss: 0.6005157232284546, Validation Loss: 0.792202889919281\n",
      "Epoch 9524/10000, Training Loss: 0.6527025699615479, Validation Loss: 0.6071299314498901\n",
      "Epoch 9525/10000, Training Loss: 0.6801537871360779, Validation Loss: 0.8702242374420166\n",
      "Epoch 9526/10000, Training Loss: 0.625884473323822, Validation Loss: 0.6450396180152893\n",
      "Epoch 9527/10000, Training Loss: 0.6439872980117798, Validation Loss: 0.6846031546592712\n",
      "Epoch 9528/10000, Training Loss: 0.588506281375885, Validation Loss: 0.5065550208091736\n",
      "Epoch 9529/10000, Training Loss: 0.6627064347267151, Validation Loss: 0.6590721607208252\n",
      "Epoch 9530/10000, Training Loss: 0.6425918340682983, Validation Loss: 1.2601662874221802\n",
      "Epoch 9531/10000, Training Loss: 0.6441316604614258, Validation Loss: 0.7320425510406494\n",
      "Epoch 9532/10000, Training Loss: 0.6330358982086182, Validation Loss: 0.7520803809165955\n",
      "Epoch 9533/10000, Training Loss: 0.678587019443512, Validation Loss: 0.6367589831352234\n",
      "Epoch 9534/10000, Training Loss: 0.6319559812545776, Validation Loss: 0.8171439170837402\n",
      "Epoch 9535/10000, Training Loss: 0.5888085961341858, Validation Loss: 0.7496334910392761\n",
      "Epoch 9536/10000, Training Loss: 0.6250415444374084, Validation Loss: 0.8101223111152649\n",
      "Epoch 9537/10000, Training Loss: 0.6524407863616943, Validation Loss: 0.7788105010986328\n",
      "Epoch 9538/10000, Training Loss: 0.6296486258506775, Validation Loss: 0.7397676110267639\n",
      "Epoch 9539/10000, Training Loss: 0.6392238140106201, Validation Loss: 0.7917537093162537\n",
      "Epoch 9540/10000, Training Loss: 0.637488603591919, Validation Loss: 0.6134920120239258\n",
      "Epoch 9541/10000, Training Loss: 0.6695899963378906, Validation Loss: 0.6016121506690979\n",
      "Epoch 9542/10000, Training Loss: 0.6467903852462769, Validation Loss: 0.7934325337409973\n",
      "Epoch 9543/10000, Training Loss: 0.6488564610481262, Validation Loss: 0.59112548828125\n",
      "Epoch 9544/10000, Training Loss: 0.6669635772705078, Validation Loss: 0.7081028819084167\n",
      "Epoch 9545/10000, Training Loss: 0.6713300347328186, Validation Loss: 0.6385601162910461\n",
      "Epoch 9546/10000, Training Loss: 0.6537120342254639, Validation Loss: 0.684592068195343\n",
      "Epoch 9547/10000, Training Loss: 0.6235723495483398, Validation Loss: 0.7634761333465576\n",
      "Epoch 9548/10000, Training Loss: 0.6282739639282227, Validation Loss: 0.810523271560669\n",
      "Epoch 9549/10000, Training Loss: 0.6915994882583618, Validation Loss: 0.6506443023681641\n",
      "Epoch 9550/10000, Training Loss: 0.6415270566940308, Validation Loss: 0.7322508692741394\n",
      "Epoch 9551/10000, Training Loss: 0.6205994486808777, Validation Loss: 0.7055699229240417\n",
      "Epoch 9552/10000, Training Loss: 0.6366522908210754, Validation Loss: 0.7187573313713074\n",
      "Epoch 9553/10000, Training Loss: 0.6397925615310669, Validation Loss: 0.7111937999725342\n",
      "Epoch 9554/10000, Training Loss: 0.6814542412757874, Validation Loss: 0.5572354197502136\n",
      "Epoch 9555/10000, Training Loss: 0.6449975371360779, Validation Loss: 0.5875388383865356\n",
      "Epoch 9556/10000, Training Loss: 0.6474527716636658, Validation Loss: 0.837949275970459\n",
      "Epoch 9557/10000, Training Loss: 0.6664108037948608, Validation Loss: 0.569548487663269\n",
      "Epoch 9558/10000, Training Loss: 0.6396840214729309, Validation Loss: 0.6272733807563782\n",
      "Epoch 9559/10000, Training Loss: 0.6159666776657104, Validation Loss: 0.6308577656745911\n",
      "Epoch 9560/10000, Training Loss: 0.682774007320404, Validation Loss: 0.6079235672950745\n",
      "Epoch 9561/10000, Training Loss: 0.6089867949485779, Validation Loss: 0.6321646571159363\n",
      "Epoch 9562/10000, Training Loss: 0.6446611881256104, Validation Loss: 0.8227943778038025\n",
      "Epoch 9563/10000, Training Loss: 0.6170293092727661, Validation Loss: 0.620415985584259\n",
      "Epoch 9564/10000, Training Loss: 0.609308123588562, Validation Loss: 0.8308050036430359\n",
      "Epoch 9565/10000, Training Loss: 0.6326714158058167, Validation Loss: 0.7843998074531555\n",
      "Epoch 9566/10000, Training Loss: 0.6668196320533752, Validation Loss: 0.6859098076820374\n",
      "Epoch 9567/10000, Training Loss: 0.6671713590621948, Validation Loss: 0.7849728465080261\n",
      "Epoch 9568/10000, Training Loss: 0.6338788270950317, Validation Loss: 0.6510192155838013\n",
      "Epoch 9569/10000, Training Loss: 0.6160293817520142, Validation Loss: 1.0068159103393555\n",
      "Epoch 9570/10000, Training Loss: 0.6164301633834839, Validation Loss: 0.6649724841117859\n",
      "Epoch 9571/10000, Training Loss: 0.6094900369644165, Validation Loss: 0.6552281379699707\n",
      "Epoch 9572/10000, Training Loss: 0.617904782295227, Validation Loss: 0.4240913391113281\n",
      "Epoch 9573/10000, Training Loss: 0.6353583931922913, Validation Loss: 0.7956180572509766\n",
      "Epoch 9574/10000, Training Loss: 0.6313554048538208, Validation Loss: 1.215211272239685\n",
      "Epoch 9575/10000, Training Loss: 0.6330969929695129, Validation Loss: 0.6834297776222229\n",
      "Epoch 9576/10000, Training Loss: 0.6609745621681213, Validation Loss: 0.47859987616539\n",
      "Epoch 9577/10000, Training Loss: 0.6599898934364319, Validation Loss: 0.7550177574157715\n",
      "Epoch 9578/10000, Training Loss: 0.6399725675582886, Validation Loss: 0.625038206577301\n",
      "Epoch 9579/10000, Training Loss: 0.6043168306350708, Validation Loss: 0.7003018856048584\n",
      "Epoch 9580/10000, Training Loss: 0.5829188227653503, Validation Loss: 0.7256497740745544\n",
      "Epoch 9581/10000, Training Loss: 0.6297016143798828, Validation Loss: 0.7974074482917786\n",
      "Epoch 9582/10000, Training Loss: 0.6691843867301941, Validation Loss: 0.9509314894676208\n",
      "Epoch 9583/10000, Training Loss: 0.6424511671066284, Validation Loss: 0.6712461113929749\n",
      "Epoch 9584/10000, Training Loss: 0.654462456703186, Validation Loss: 0.6661192178726196\n",
      "Epoch 9585/10000, Training Loss: 0.6251395344734192, Validation Loss: 0.5707092881202698\n",
      "Epoch 9586/10000, Training Loss: 0.7043358087539673, Validation Loss: 0.6072642803192139\n",
      "Epoch 9587/10000, Training Loss: 0.6290196776390076, Validation Loss: 0.632171094417572\n",
      "Epoch 9588/10000, Training Loss: 0.6503457427024841, Validation Loss: 0.5363662838935852\n",
      "Epoch 9589/10000, Training Loss: 0.5853549242019653, Validation Loss: 0.6214727759361267\n",
      "Epoch 9590/10000, Training Loss: 0.6111072301864624, Validation Loss: 0.66151362657547\n",
      "Epoch 9591/10000, Training Loss: 0.6415476202964783, Validation Loss: 0.691062867641449\n",
      "Epoch 9592/10000, Training Loss: 0.6230623722076416, Validation Loss: 0.648436963558197\n",
      "Epoch 9593/10000, Training Loss: 0.6410093307495117, Validation Loss: 0.6934451460838318\n",
      "Epoch 9594/10000, Training Loss: 0.6456512808799744, Validation Loss: 0.6642916798591614\n",
      "Epoch 9595/10000, Training Loss: 0.6498669385910034, Validation Loss: 0.5634430050849915\n",
      "Epoch 9596/10000, Training Loss: 0.611236035823822, Validation Loss: 0.4861721098423004\n",
      "Epoch 9597/10000, Training Loss: 0.657073974609375, Validation Loss: 0.7550306916236877\n",
      "Epoch 9598/10000, Training Loss: 0.6426792144775391, Validation Loss: 0.758974015712738\n",
      "Epoch 9599/10000, Training Loss: 0.6396821141242981, Validation Loss: 0.7303409576416016\n",
      "Epoch 9600/10000, Training Loss: 0.6308969855308533, Validation Loss: 0.8925072550773621\n",
      "Epoch 9601/10000, Training Loss: 0.6716470718383789, Validation Loss: 0.39545440673828125\n",
      "Epoch 9602/10000, Training Loss: 0.682775616645813, Validation Loss: 0.6107211709022522\n",
      "Epoch 9603/10000, Training Loss: 0.6630387902259827, Validation Loss: 0.9056921601295471\n",
      "Epoch 9604/10000, Training Loss: 0.6081733107566833, Validation Loss: 0.7785673141479492\n",
      "Epoch 9605/10000, Training Loss: 0.647563636302948, Validation Loss: 0.7070820331573486\n",
      "Epoch 9606/10000, Training Loss: 0.6622510552406311, Validation Loss: 0.6986410617828369\n",
      "Epoch 9607/10000, Training Loss: 0.6551278233528137, Validation Loss: 0.5627821683883667\n",
      "Epoch 9608/10000, Training Loss: 0.617531418800354, Validation Loss: 0.580423891544342\n",
      "Epoch 9609/10000, Training Loss: 0.6359908580780029, Validation Loss: 0.7107934355735779\n",
      "Epoch 9610/10000, Training Loss: 0.6327062845230103, Validation Loss: 0.6999581456184387\n",
      "Epoch 9611/10000, Training Loss: 0.6568523645401001, Validation Loss: 0.6857090592384338\n",
      "Epoch 9612/10000, Training Loss: 0.626636803150177, Validation Loss: 0.6793996691703796\n",
      "Epoch 9613/10000, Training Loss: 0.6042189002037048, Validation Loss: 0.5740329623222351\n",
      "Epoch 9614/10000, Training Loss: 0.641442596912384, Validation Loss: 0.6647803783416748\n",
      "Epoch 9615/10000, Training Loss: 0.6543991565704346, Validation Loss: 0.7158544659614563\n",
      "Epoch 9616/10000, Training Loss: 0.6526695489883423, Validation Loss: 0.5851135849952698\n",
      "Epoch 9617/10000, Training Loss: 0.6238995790481567, Validation Loss: 1.0712329149246216\n",
      "Epoch 9618/10000, Training Loss: 0.6388407945632935, Validation Loss: 0.6677277684211731\n",
      "Epoch 9619/10000, Training Loss: 0.639297604560852, Validation Loss: 0.6937577128410339\n",
      "Epoch 9620/10000, Training Loss: 0.6415964365005493, Validation Loss: 0.534890353679657\n",
      "Epoch 9621/10000, Training Loss: 0.6659901738166809, Validation Loss: 0.6082062125205994\n",
      "Epoch 9622/10000, Training Loss: 0.6580118536949158, Validation Loss: 0.6882715821266174\n",
      "Epoch 9623/10000, Training Loss: 0.641728401184082, Validation Loss: 0.6777109503746033\n",
      "Epoch 9624/10000, Training Loss: 0.6440090537071228, Validation Loss: 0.6324650645256042\n",
      "Epoch 9625/10000, Training Loss: 0.6473543643951416, Validation Loss: 0.7407206892967224\n",
      "Epoch 9626/10000, Training Loss: 0.6200304627418518, Validation Loss: 0.6002625226974487\n",
      "Epoch 9627/10000, Training Loss: 0.6595855951309204, Validation Loss: 0.7784433960914612\n",
      "Epoch 9628/10000, Training Loss: 0.6368259787559509, Validation Loss: 0.6714851260185242\n",
      "Epoch 9629/10000, Training Loss: 0.7224124073982239, Validation Loss: 0.7928114533424377\n",
      "Epoch 9630/10000, Training Loss: 0.6492270231246948, Validation Loss: 0.5835674405097961\n",
      "Epoch 9631/10000, Training Loss: 0.6607265472412109, Validation Loss: 0.7105474472045898\n",
      "Epoch 9632/10000, Training Loss: 0.6743897795677185, Validation Loss: 0.8058152794837952\n",
      "Epoch 9633/10000, Training Loss: 0.6409872770309448, Validation Loss: 0.6160438060760498\n",
      "Epoch 9634/10000, Training Loss: 0.6570977568626404, Validation Loss: 0.6586520075798035\n",
      "Epoch 9635/10000, Training Loss: 0.6286742091178894, Validation Loss: 0.6742460131645203\n",
      "Epoch 9636/10000, Training Loss: 0.6535387635231018, Validation Loss: 0.5369681715965271\n",
      "Epoch 9637/10000, Training Loss: 0.6033828258514404, Validation Loss: 0.6961649060249329\n",
      "Epoch 9638/10000, Training Loss: 0.6446120738983154, Validation Loss: 0.8519006371498108\n",
      "Epoch 9639/10000, Training Loss: 0.7048909664154053, Validation Loss: 0.7720178961753845\n",
      "Epoch 9640/10000, Training Loss: 0.642912745475769, Validation Loss: 0.7323401570320129\n",
      "Epoch 9641/10000, Training Loss: 0.6277787685394287, Validation Loss: 1.1051629781723022\n",
      "Epoch 9642/10000, Training Loss: 0.6428838968276978, Validation Loss: 0.5851433277130127\n",
      "Epoch 9643/10000, Training Loss: 0.6282209157943726, Validation Loss: 0.6669099926948547\n",
      "Epoch 9644/10000, Training Loss: 0.6538426280021667, Validation Loss: 0.5432557463645935\n",
      "Epoch 9645/10000, Training Loss: 0.6806862354278564, Validation Loss: 0.6426336765289307\n",
      "Epoch 9646/10000, Training Loss: 0.6540807485580444, Validation Loss: 0.5586487650871277\n",
      "Epoch 9647/10000, Training Loss: 0.6436614990234375, Validation Loss: 0.6633116602897644\n",
      "Epoch 9648/10000, Training Loss: 0.6809558868408203, Validation Loss: 0.7600292563438416\n",
      "Epoch 9649/10000, Training Loss: 0.6417958736419678, Validation Loss: 0.7524241805076599\n",
      "Epoch 9650/10000, Training Loss: 0.6415121555328369, Validation Loss: 0.7059104442596436\n",
      "Epoch 9651/10000, Training Loss: 0.6448256373405457, Validation Loss: 0.7782738208770752\n",
      "Epoch 9652/10000, Training Loss: 0.6499621272087097, Validation Loss: 0.9269728064537048\n",
      "Epoch 9653/10000, Training Loss: 0.5868640542030334, Validation Loss: 0.7067255973815918\n",
      "Epoch 9654/10000, Training Loss: 0.6232731938362122, Validation Loss: 0.6564813256263733\n",
      "Epoch 9655/10000, Training Loss: 0.5986852645874023, Validation Loss: 0.616240382194519\n",
      "Epoch 9656/10000, Training Loss: 0.6574585437774658, Validation Loss: 0.8055451512336731\n",
      "Epoch 9657/10000, Training Loss: 0.6433655619621277, Validation Loss: 0.6685664653778076\n",
      "Epoch 9658/10000, Training Loss: 0.6535018086433411, Validation Loss: 0.6761241555213928\n",
      "Epoch 9659/10000, Training Loss: 0.663142204284668, Validation Loss: 0.6700279712677002\n",
      "Epoch 9660/10000, Training Loss: 0.5929118990898132, Validation Loss: 0.6834404468536377\n",
      "Epoch 9661/10000, Training Loss: 0.645269513130188, Validation Loss: 0.7395368218421936\n",
      "Epoch 9662/10000, Training Loss: 0.6647332310676575, Validation Loss: 0.6402584910392761\n",
      "Epoch 9663/10000, Training Loss: 0.654815673828125, Validation Loss: 0.5105539560317993\n",
      "Epoch 9664/10000, Training Loss: 0.6218979358673096, Validation Loss: 0.6872521042823792\n",
      "Epoch 9665/10000, Training Loss: 0.6435498595237732, Validation Loss: 0.923306941986084\n",
      "Epoch 9666/10000, Training Loss: 0.6656287908554077, Validation Loss: 0.7105355262756348\n",
      "Epoch 9667/10000, Training Loss: 0.6368312239646912, Validation Loss: 0.6116572022438049\n",
      "Epoch 9668/10000, Training Loss: 0.7129002213478088, Validation Loss: 0.6149148941040039\n",
      "Epoch 9669/10000, Training Loss: 0.6263911128044128, Validation Loss: 0.7335474491119385\n",
      "Epoch 9670/10000, Training Loss: 0.5987871885299683, Validation Loss: 0.6631042957305908\n",
      "Epoch 9671/10000, Training Loss: 0.6530448198318481, Validation Loss: 0.960702121257782\n",
      "Epoch 9672/10000, Training Loss: 0.6504798531532288, Validation Loss: 0.7477690577507019\n",
      "Epoch 9673/10000, Training Loss: 0.59732586145401, Validation Loss: 1.0433224439620972\n",
      "Epoch 9674/10000, Training Loss: 0.6302370429039001, Validation Loss: 0.8161240220069885\n",
      "Epoch 9675/10000, Training Loss: 0.6473541855812073, Validation Loss: 0.596132755279541\n",
      "Epoch 9676/10000, Training Loss: 0.6436755061149597, Validation Loss: 0.6165946125984192\n",
      "Epoch 9677/10000, Training Loss: 0.6165530681610107, Validation Loss: 0.6383821368217468\n",
      "Epoch 9678/10000, Training Loss: 0.6172272562980652, Validation Loss: 0.6611560583114624\n",
      "Epoch 9679/10000, Training Loss: 0.6647301912307739, Validation Loss: 0.5643866658210754\n",
      "Epoch 9680/10000, Training Loss: 0.6616945862770081, Validation Loss: 1.1441391706466675\n",
      "Epoch 9681/10000, Training Loss: 0.6472775936126709, Validation Loss: 0.61031574010849\n",
      "Epoch 9682/10000, Training Loss: 0.6607224345207214, Validation Loss: 0.6227073073387146\n",
      "Epoch 9683/10000, Training Loss: 0.6177876591682434, Validation Loss: 0.5941764116287231\n",
      "Epoch 9684/10000, Training Loss: 0.7468522787094116, Validation Loss: 0.7392376065254211\n",
      "Epoch 9685/10000, Training Loss: 0.6513901948928833, Validation Loss: 0.7572421431541443\n",
      "Epoch 9686/10000, Training Loss: 0.6240400075912476, Validation Loss: 0.7735816836357117\n",
      "Epoch 9687/10000, Training Loss: 0.636824369430542, Validation Loss: 0.7273862361907959\n",
      "Epoch 9688/10000, Training Loss: 0.6649188995361328, Validation Loss: 0.5908896327018738\n",
      "Epoch 9689/10000, Training Loss: 0.6480341553688049, Validation Loss: 0.6631842255592346\n",
      "Epoch 9690/10000, Training Loss: 0.6450747847557068, Validation Loss: 0.5551312565803528\n",
      "Epoch 9691/10000, Training Loss: 0.6584168672561646, Validation Loss: 0.6585060954093933\n",
      "Epoch 9692/10000, Training Loss: 0.6232856512069702, Validation Loss: 0.6171625256538391\n",
      "Epoch 9693/10000, Training Loss: 0.5596275925636292, Validation Loss: 0.4857580363750458\n",
      "Epoch 9694/10000, Training Loss: 0.6430267691612244, Validation Loss: 0.6989138722419739\n",
      "Epoch 9695/10000, Training Loss: 0.6184230446815491, Validation Loss: 0.7155291438102722\n",
      "Epoch 9696/10000, Training Loss: 0.6625875234603882, Validation Loss: 0.7978947758674622\n",
      "Epoch 9697/10000, Training Loss: 0.6236056685447693, Validation Loss: 0.6337241530418396\n",
      "Epoch 9698/10000, Training Loss: 0.6685448884963989, Validation Loss: 0.5593810081481934\n",
      "Epoch 9699/10000, Training Loss: 0.6528085470199585, Validation Loss: 0.6529111266136169\n",
      "Epoch 9700/10000, Training Loss: 0.6327753663063049, Validation Loss: 0.5760637521743774\n",
      "Epoch 9701/10000, Training Loss: 0.642572820186615, Validation Loss: 0.5849323868751526\n",
      "Epoch 9702/10000, Training Loss: 0.6453773975372314, Validation Loss: 0.689413845539093\n",
      "Epoch 9703/10000, Training Loss: 0.6629685163497925, Validation Loss: 0.6261556148529053\n",
      "Epoch 9704/10000, Training Loss: 0.6223306655883789, Validation Loss: 0.6340151429176331\n",
      "Epoch 9705/10000, Training Loss: 0.6197997331619263, Validation Loss: 0.8322350382804871\n",
      "Epoch 9706/10000, Training Loss: 0.6392210721969604, Validation Loss: 0.6952791810035706\n",
      "Epoch 9707/10000, Training Loss: 0.6262425184249878, Validation Loss: 0.6399405598640442\n",
      "Epoch 9708/10000, Training Loss: 0.6343383193016052, Validation Loss: 1.1878708600997925\n",
      "Epoch 9709/10000, Training Loss: 0.6546984314918518, Validation Loss: 0.40676355361938477\n",
      "Epoch 9710/10000, Training Loss: 0.6139477491378784, Validation Loss: 0.7127423286437988\n",
      "Epoch 9711/10000, Training Loss: 0.6663486957550049, Validation Loss: 0.6389160752296448\n",
      "Epoch 9712/10000, Training Loss: 0.6706399321556091, Validation Loss: 0.706616222858429\n",
      "Epoch 9713/10000, Training Loss: 0.6597384214401245, Validation Loss: 0.6339123249053955\n",
      "Epoch 9714/10000, Training Loss: 0.6278051137924194, Validation Loss: 0.5564787983894348\n",
      "Epoch 9715/10000, Training Loss: 0.5934222936630249, Validation Loss: 0.7450128197669983\n",
      "Epoch 9716/10000, Training Loss: 0.6819061040878296, Validation Loss: 0.5053896307945251\n",
      "Epoch 9717/10000, Training Loss: 0.6278168559074402, Validation Loss: 0.8070411086082458\n",
      "Epoch 9718/10000, Training Loss: 0.6454325318336487, Validation Loss: 0.452482134103775\n",
      "Epoch 9719/10000, Training Loss: 0.6468955278396606, Validation Loss: 0.9774115085601807\n",
      "Epoch 9720/10000, Training Loss: 0.6527458429336548, Validation Loss: 0.6712493896484375\n",
      "Epoch 9721/10000, Training Loss: 0.6426159739494324, Validation Loss: 0.7694349884986877\n",
      "Epoch 9722/10000, Training Loss: 0.6570863127708435, Validation Loss: 0.8174356818199158\n",
      "Epoch 9723/10000, Training Loss: 0.6260858774185181, Validation Loss: 0.6938808560371399\n",
      "Epoch 9724/10000, Training Loss: 0.6799144744873047, Validation Loss: 0.7414171099662781\n",
      "Epoch 9725/10000, Training Loss: 0.6187598705291748, Validation Loss: 0.9706099629402161\n",
      "Epoch 9726/10000, Training Loss: 0.6255908608436584, Validation Loss: 0.9695677161216736\n",
      "Epoch 9727/10000, Training Loss: 0.629859983921051, Validation Loss: 0.838778018951416\n",
      "Epoch 9728/10000, Training Loss: 0.7038834095001221, Validation Loss: 0.9066866040229797\n",
      "Epoch 9729/10000, Training Loss: 0.6269607543945312, Validation Loss: 0.952521800994873\n",
      "Epoch 9730/10000, Training Loss: 0.6302887201309204, Validation Loss: 0.7334395051002502\n",
      "Epoch 9731/10000, Training Loss: 0.6334549784660339, Validation Loss: 0.5700533986091614\n",
      "Epoch 9732/10000, Training Loss: 0.6295795440673828, Validation Loss: 0.6387365460395813\n",
      "Epoch 9733/10000, Training Loss: 0.6139931678771973, Validation Loss: 0.5283727049827576\n",
      "Epoch 9734/10000, Training Loss: 0.7257123589515686, Validation Loss: 0.786317765712738\n",
      "Epoch 9735/10000, Training Loss: 0.6328367590904236, Validation Loss: 0.6338532567024231\n",
      "Epoch 9736/10000, Training Loss: 0.6796742677688599, Validation Loss: 0.8127608895301819\n",
      "Epoch 9737/10000, Training Loss: 0.6377792954444885, Validation Loss: 0.6854474544525146\n",
      "Epoch 9738/10000, Training Loss: 0.6096982359886169, Validation Loss: 0.6913228631019592\n",
      "Epoch 9739/10000, Training Loss: 0.6723998188972473, Validation Loss: 1.024895191192627\n",
      "Epoch 9740/10000, Training Loss: 0.6293358206748962, Validation Loss: 0.9157342910766602\n",
      "Epoch 9741/10000, Training Loss: 0.6522780060768127, Validation Loss: 0.7055605053901672\n",
      "Epoch 9742/10000, Training Loss: 0.6629002690315247, Validation Loss: 0.6514506936073303\n",
      "Epoch 9743/10000, Training Loss: 0.6511797308921814, Validation Loss: 0.72861647605896\n",
      "Epoch 9744/10000, Training Loss: 0.6706225872039795, Validation Loss: 0.6818244457244873\n",
      "Epoch 9745/10000, Training Loss: 0.6169409155845642, Validation Loss: 0.4429830014705658\n",
      "Epoch 9746/10000, Training Loss: 0.6043088436126709, Validation Loss: 0.6891109943389893\n",
      "Epoch 9747/10000, Training Loss: 0.6233370304107666, Validation Loss: 0.9844352602958679\n",
      "Epoch 9748/10000, Training Loss: 0.6558994054794312, Validation Loss: 0.6483842730522156\n",
      "Epoch 9749/10000, Training Loss: 0.6304803490638733, Validation Loss: 0.3882457911968231\n",
      "Epoch 9750/10000, Training Loss: 0.6333130598068237, Validation Loss: 1.1280521154403687\n",
      "Epoch 9751/10000, Training Loss: 0.641414999961853, Validation Loss: 0.6294236183166504\n",
      "Epoch 9752/10000, Training Loss: 0.6446024179458618, Validation Loss: 0.7413157820701599\n",
      "Epoch 9753/10000, Training Loss: 0.6566281914710999, Validation Loss: 0.7090827822685242\n",
      "Epoch 9754/10000, Training Loss: 0.6416114568710327, Validation Loss: 0.9072405695915222\n",
      "Epoch 9755/10000, Training Loss: 0.6512370705604553, Validation Loss: 1.1681500673294067\n",
      "Epoch 9756/10000, Training Loss: 0.6361151337623596, Validation Loss: 0.8204144835472107\n",
      "Epoch 9757/10000, Training Loss: 0.6644836664199829, Validation Loss: 0.5817716121673584\n",
      "Epoch 9758/10000, Training Loss: 0.6551722884178162, Validation Loss: 0.6956837773323059\n",
      "Epoch 9759/10000, Training Loss: 0.6755595803260803, Validation Loss: 0.7122588157653809\n",
      "Epoch 9760/10000, Training Loss: 0.6461676955223083, Validation Loss: 0.9682145714759827\n",
      "Epoch 9761/10000, Training Loss: 0.6409000158309937, Validation Loss: 0.7633953094482422\n",
      "Epoch 9762/10000, Training Loss: 0.6359671354293823, Validation Loss: 0.8156511187553406\n",
      "Epoch 9763/10000, Training Loss: 0.6317763924598694, Validation Loss: 0.6618261933326721\n",
      "Epoch 9764/10000, Training Loss: 0.6464688777923584, Validation Loss: 0.6305964589118958\n",
      "Epoch 9765/10000, Training Loss: 0.6277030110359192, Validation Loss: 0.6964611411094666\n",
      "Epoch 9766/10000, Training Loss: 0.610201895236969, Validation Loss: 0.9630013108253479\n",
      "Epoch 9767/10000, Training Loss: 0.6474295258522034, Validation Loss: 0.6083444952964783\n",
      "Epoch 9768/10000, Training Loss: 0.6429997682571411, Validation Loss: 0.7975013256072998\n",
      "Epoch 9769/10000, Training Loss: 0.614382266998291, Validation Loss: 0.8034414649009705\n",
      "Epoch 9770/10000, Training Loss: 0.6478142142295837, Validation Loss: 0.5809915661811829\n",
      "Epoch 9771/10000, Training Loss: 0.6499656438827515, Validation Loss: 0.6216812133789062\n",
      "Epoch 9772/10000, Training Loss: 0.6766391396522522, Validation Loss: 0.6984087824821472\n",
      "Epoch 9773/10000, Training Loss: 0.6405951976776123, Validation Loss: 1.027012586593628\n",
      "Epoch 9774/10000, Training Loss: 0.6427446007728577, Validation Loss: 0.5868189930915833\n",
      "Epoch 9775/10000, Training Loss: 0.6245101094245911, Validation Loss: 0.5966998934745789\n",
      "Epoch 9776/10000, Training Loss: 0.6672573089599609, Validation Loss: 0.8562316298484802\n",
      "Epoch 9777/10000, Training Loss: 0.6260599493980408, Validation Loss: 1.6104680299758911\n",
      "Epoch 9778/10000, Training Loss: 0.6660887002944946, Validation Loss: 0.5161705613136292\n",
      "Epoch 9779/10000, Training Loss: 0.6484461426734924, Validation Loss: 0.8010377883911133\n",
      "Epoch 9780/10000, Training Loss: 0.690205454826355, Validation Loss: 1.0124109983444214\n",
      "Epoch 9781/10000, Training Loss: 0.7054333090782166, Validation Loss: 0.7300198078155518\n",
      "Epoch 9782/10000, Training Loss: 0.6453132629394531, Validation Loss: 0.8273388743400574\n",
      "Epoch 9783/10000, Training Loss: 0.6123523116111755, Validation Loss: 0.6864745020866394\n",
      "Epoch 9784/10000, Training Loss: 0.6544612050056458, Validation Loss: 0.728133499622345\n",
      "Epoch 9785/10000, Training Loss: 0.6087175011634827, Validation Loss: 0.5988735556602478\n",
      "Epoch 9786/10000, Training Loss: 0.7008976340293884, Validation Loss: 0.8363574147224426\n",
      "Epoch 9787/10000, Training Loss: 0.6561620235443115, Validation Loss: 0.821304976940155\n",
      "Epoch 9788/10000, Training Loss: 0.6416118144989014, Validation Loss: 0.829992949962616\n",
      "Epoch 9789/10000, Training Loss: 0.6415919661521912, Validation Loss: 0.8027195930480957\n",
      "Epoch 9790/10000, Training Loss: 0.6402701735496521, Validation Loss: 0.6488295197486877\n",
      "Epoch 9791/10000, Training Loss: 0.6357240080833435, Validation Loss: 0.6664180755615234\n",
      "Epoch 9792/10000, Training Loss: 0.6212567687034607, Validation Loss: 0.6098107695579529\n",
      "Epoch 9793/10000, Training Loss: 0.6401354074478149, Validation Loss: 0.6727492809295654\n",
      "Epoch 9794/10000, Training Loss: 0.6016557812690735, Validation Loss: 0.3888213336467743\n",
      "Epoch 9795/10000, Training Loss: 0.6979749202728271, Validation Loss: 0.9088663458824158\n",
      "Epoch 9796/10000, Training Loss: 0.6382007002830505, Validation Loss: 0.573908269405365\n",
      "Epoch 9797/10000, Training Loss: 0.6245816946029663, Validation Loss: 0.8174128532409668\n",
      "Epoch 9798/10000, Training Loss: 0.6584964394569397, Validation Loss: 0.7260944247245789\n",
      "Epoch 9799/10000, Training Loss: 0.6715093851089478, Validation Loss: 0.5981714725494385\n",
      "Epoch 9800/10000, Training Loss: 0.6442898511886597, Validation Loss: 0.55161452293396\n",
      "Epoch 9801/10000, Training Loss: 0.6798151731491089, Validation Loss: 0.6608638167381287\n",
      "Epoch 9802/10000, Training Loss: 0.6561692953109741, Validation Loss: 0.6623213291168213\n",
      "Epoch 9803/10000, Training Loss: 0.6655515432357788, Validation Loss: 0.7741778492927551\n",
      "Epoch 9804/10000, Training Loss: 0.6628617644309998, Validation Loss: 0.4920206069946289\n",
      "Epoch 9805/10000, Training Loss: 0.5991334915161133, Validation Loss: 0.6492965817451477\n",
      "Epoch 9806/10000, Training Loss: 0.6329523921012878, Validation Loss: 0.7619919180870056\n",
      "Epoch 9807/10000, Training Loss: 0.6194167733192444, Validation Loss: 0.6648719906806946\n",
      "Epoch 9808/10000, Training Loss: 0.6025947332382202, Validation Loss: 0.7566883563995361\n",
      "Epoch 9809/10000, Training Loss: 0.6299828290939331, Validation Loss: 0.632800817489624\n",
      "Epoch 9810/10000, Training Loss: 0.6084453463554382, Validation Loss: 0.7275509238243103\n",
      "Epoch 9811/10000, Training Loss: 0.6282759308815002, Validation Loss: 0.6401108503341675\n",
      "Epoch 9812/10000, Training Loss: 0.6623164415359497, Validation Loss: 0.7112049460411072\n",
      "Epoch 9813/10000, Training Loss: 0.6525664925575256, Validation Loss: 0.718733012676239\n",
      "Epoch 9814/10000, Training Loss: 0.6334545612335205, Validation Loss: 0.697360098361969\n",
      "Epoch 9815/10000, Training Loss: 0.6466855406761169, Validation Loss: 0.654142439365387\n",
      "Epoch 9816/10000, Training Loss: 0.6202050447463989, Validation Loss: 0.7457146048545837\n",
      "Epoch 9817/10000, Training Loss: 0.6575090885162354, Validation Loss: 0.8066838383674622\n",
      "Epoch 9818/10000, Training Loss: 0.6581957936286926, Validation Loss: 0.7092738747596741\n",
      "Epoch 9819/10000, Training Loss: 0.6807798743247986, Validation Loss: 0.5112348794937134\n",
      "Epoch 9820/10000, Training Loss: 0.6272462606430054, Validation Loss: 0.730846643447876\n",
      "Epoch 9821/10000, Training Loss: 0.7186330556869507, Validation Loss: 0.445917010307312\n",
      "Epoch 9822/10000, Training Loss: 0.6361649036407471, Validation Loss: 0.7171667218208313\n",
      "Epoch 9823/10000, Training Loss: 0.6267353296279907, Validation Loss: 0.6317895650863647\n",
      "Epoch 9824/10000, Training Loss: 0.6267623901367188, Validation Loss: 0.6592714190483093\n",
      "Epoch 9825/10000, Training Loss: 0.6406036019325256, Validation Loss: 0.8474746346473694\n",
      "Epoch 9826/10000, Training Loss: 0.6536790132522583, Validation Loss: 0.6597203612327576\n",
      "Epoch 9827/10000, Training Loss: 0.6293097138404846, Validation Loss: 0.6968479156494141\n",
      "Epoch 9828/10000, Training Loss: 0.6536409854888916, Validation Loss: 0.7297172546386719\n",
      "Epoch 9829/10000, Training Loss: 0.6380153298377991, Validation Loss: 0.5596714019775391\n",
      "Epoch 9830/10000, Training Loss: 0.6439988017082214, Validation Loss: 0.7715250849723816\n",
      "Epoch 9831/10000, Training Loss: 0.6422539949417114, Validation Loss: 0.6802119612693787\n",
      "Epoch 9832/10000, Training Loss: 0.6707985401153564, Validation Loss: 0.616243839263916\n",
      "Epoch 9833/10000, Training Loss: 0.661636233329773, Validation Loss: 0.8430760502815247\n",
      "Epoch 9834/10000, Training Loss: 0.6257448792457581, Validation Loss: 0.6247512698173523\n",
      "Epoch 9835/10000, Training Loss: 0.6552069187164307, Validation Loss: 0.6937443614006042\n",
      "Epoch 9836/10000, Training Loss: 0.6442956328392029, Validation Loss: 0.6747176647186279\n",
      "Epoch 9837/10000, Training Loss: 0.6519813537597656, Validation Loss: 0.6904375553131104\n",
      "Epoch 9838/10000, Training Loss: 0.6373674869537354, Validation Loss: 0.649084210395813\n",
      "Epoch 9839/10000, Training Loss: 0.6269079446792603, Validation Loss: 0.6221626400947571\n",
      "Epoch 9840/10000, Training Loss: 0.6522300243377686, Validation Loss: 0.620276153087616\n",
      "Epoch 9841/10000, Training Loss: 0.6184966564178467, Validation Loss: 0.8728575706481934\n",
      "Epoch 9842/10000, Training Loss: 0.6910995244979858, Validation Loss: 0.9457416534423828\n",
      "Epoch 9843/10000, Training Loss: 0.6289364099502563, Validation Loss: 0.626765787601471\n",
      "Epoch 9844/10000, Training Loss: 0.6441004872322083, Validation Loss: 0.8583090901374817\n",
      "Epoch 9845/10000, Training Loss: 0.655738115310669, Validation Loss: 0.7352963089942932\n",
      "Epoch 9846/10000, Training Loss: 0.6495205760002136, Validation Loss: 0.7271165251731873\n",
      "Epoch 9847/10000, Training Loss: 0.6606603860855103, Validation Loss: 0.7650638222694397\n",
      "Epoch 9848/10000, Training Loss: 0.6332311630249023, Validation Loss: 0.8503546714782715\n",
      "Epoch 9849/10000, Training Loss: 0.6197912693023682, Validation Loss: 0.7172410488128662\n",
      "Epoch 9850/10000, Training Loss: 0.6825588941574097, Validation Loss: 0.5990728139877319\n",
      "Epoch 9851/10000, Training Loss: 0.6894097924232483, Validation Loss: 0.8633525967597961\n",
      "Epoch 9852/10000, Training Loss: 0.6541364192962646, Validation Loss: 0.6697942614555359\n",
      "Epoch 9853/10000, Training Loss: 0.6240429282188416, Validation Loss: 0.6185763478279114\n",
      "Epoch 9854/10000, Training Loss: 0.6550726294517517, Validation Loss: 0.6734316945075989\n",
      "Epoch 9855/10000, Training Loss: 0.6474522948265076, Validation Loss: 0.5721361637115479\n",
      "Epoch 9856/10000, Training Loss: 0.6495385766029358, Validation Loss: 0.9309008717536926\n",
      "Epoch 9857/10000, Training Loss: 0.6454390287399292, Validation Loss: 0.5883603692054749\n",
      "Epoch 9858/10000, Training Loss: 0.6760641932487488, Validation Loss: 0.739861786365509\n",
      "Epoch 9859/10000, Training Loss: 0.673447847366333, Validation Loss: 0.669873058795929\n",
      "Epoch 9860/10000, Training Loss: 0.659511923789978, Validation Loss: 0.6877616047859192\n",
      "Epoch 9861/10000, Training Loss: 0.6431289315223694, Validation Loss: 0.8206636309623718\n",
      "Epoch 9862/10000, Training Loss: 0.6426395177841187, Validation Loss: 0.7411260604858398\n",
      "Epoch 9863/10000, Training Loss: 0.6937724351882935, Validation Loss: 0.7565166354179382\n",
      "Epoch 9864/10000, Training Loss: 0.6277948617935181, Validation Loss: 0.6782152652740479\n",
      "Epoch 9865/10000, Training Loss: 0.6357880234718323, Validation Loss: 0.7196789383888245\n",
      "Epoch 9866/10000, Training Loss: 0.6462084054946899, Validation Loss: 0.7135887742042542\n",
      "Epoch 9867/10000, Training Loss: 0.6012040376663208, Validation Loss: 0.6552562117576599\n",
      "Epoch 9868/10000, Training Loss: 0.6677212119102478, Validation Loss: 0.73459392786026\n",
      "Epoch 9869/10000, Training Loss: 0.6397106647491455, Validation Loss: 0.6665937304496765\n",
      "Epoch 9870/10000, Training Loss: 0.6272324919700623, Validation Loss: 0.8379964828491211\n",
      "Epoch 9871/10000, Training Loss: 0.6107898354530334, Validation Loss: 0.6178156137466431\n",
      "Epoch 9872/10000, Training Loss: 0.6362412571907043, Validation Loss: 0.9017370343208313\n",
      "Epoch 9873/10000, Training Loss: 0.6252292990684509, Validation Loss: 0.5180736780166626\n",
      "Epoch 9874/10000, Training Loss: 0.64949631690979, Validation Loss: 0.7647452354431152\n",
      "Epoch 9875/10000, Training Loss: 0.6368182301521301, Validation Loss: 0.5543044209480286\n",
      "Epoch 9876/10000, Training Loss: 0.7012535929679871, Validation Loss: 0.6356056332588196\n",
      "Epoch 9877/10000, Training Loss: 0.6477053761482239, Validation Loss: 0.6988045573234558\n",
      "Epoch 9878/10000, Training Loss: 0.622626006603241, Validation Loss: 0.6109210848808289\n",
      "Epoch 9879/10000, Training Loss: 0.6443260908126831, Validation Loss: 1.0220154523849487\n",
      "Epoch 9880/10000, Training Loss: 0.6378949284553528, Validation Loss: 0.7836918234825134\n",
      "Epoch 9881/10000, Training Loss: 0.684045135974884, Validation Loss: 0.6736478209495544\n",
      "Epoch 9882/10000, Training Loss: 0.6743319630622864, Validation Loss: 0.6075128316879272\n",
      "Epoch 9883/10000, Training Loss: 0.6314463019371033, Validation Loss: 0.6038900017738342\n",
      "Epoch 9884/10000, Training Loss: 0.6548879146575928, Validation Loss: 0.7036886215209961\n",
      "Epoch 9885/10000, Training Loss: 0.6715915203094482, Validation Loss: 0.8735225200653076\n",
      "Epoch 9886/10000, Training Loss: 0.6204970479011536, Validation Loss: 0.5656946301460266\n",
      "Epoch 9887/10000, Training Loss: 0.6437711715698242, Validation Loss: 0.8670583367347717\n",
      "Epoch 9888/10000, Training Loss: 0.6620663404464722, Validation Loss: 0.4275919497013092\n",
      "Epoch 9889/10000, Training Loss: 0.6865425109863281, Validation Loss: 0.7764167785644531\n",
      "Epoch 9890/10000, Training Loss: 0.6251210570335388, Validation Loss: 0.8114116191864014\n",
      "Epoch 9891/10000, Training Loss: 0.6263551115989685, Validation Loss: 0.6618055701255798\n",
      "Epoch 9892/10000, Training Loss: 0.6466561555862427, Validation Loss: 0.776549756526947\n",
      "Epoch 9893/10000, Training Loss: 0.6976488828659058, Validation Loss: 0.803837776184082\n",
      "Epoch 9894/10000, Training Loss: 0.6885823607444763, Validation Loss: 0.8197439312934875\n",
      "Epoch 9895/10000, Training Loss: 0.6281818151473999, Validation Loss: 0.6164078116416931\n",
      "Epoch 9896/10000, Training Loss: 0.6436281800270081, Validation Loss: 0.7779151797294617\n",
      "Epoch 9897/10000, Training Loss: 0.6446937918663025, Validation Loss: 0.8555169701576233\n",
      "Epoch 9898/10000, Training Loss: 0.6338983774185181, Validation Loss: 0.6760621666908264\n",
      "Epoch 9899/10000, Training Loss: 0.6355406641960144, Validation Loss: 0.7263727188110352\n",
      "Epoch 9900/10000, Training Loss: 0.620415985584259, Validation Loss: 0.7803637981414795\n",
      "Epoch 9901/10000, Training Loss: 0.6248416900634766, Validation Loss: 0.6096551418304443\n",
      "Epoch 9902/10000, Training Loss: 0.6474911570549011, Validation Loss: 0.6038252711296082\n",
      "Epoch 9903/10000, Training Loss: 0.6671507358551025, Validation Loss: 0.6665080189704895\n",
      "Epoch 9904/10000, Training Loss: 0.6445526480674744, Validation Loss: 0.6960799098014832\n",
      "Epoch 9905/10000, Training Loss: 0.6080144643783569, Validation Loss: 0.8086483478546143\n",
      "Epoch 9906/10000, Training Loss: 0.6472227573394775, Validation Loss: 0.6904467940330505\n",
      "Epoch 9907/10000, Training Loss: 0.6329593062400818, Validation Loss: 0.5968340039253235\n",
      "Epoch 9908/10000, Training Loss: 0.6403223276138306, Validation Loss: 0.5810433626174927\n",
      "Epoch 9909/10000, Training Loss: 0.603381872177124, Validation Loss: 0.81705641746521\n",
      "Epoch 9910/10000, Training Loss: 0.5878380537033081, Validation Loss: 0.7453975677490234\n",
      "Epoch 9911/10000, Training Loss: 0.624573826789856, Validation Loss: 0.7009292244911194\n",
      "Epoch 9912/10000, Training Loss: 0.6386098265647888, Validation Loss: 0.7488512396812439\n",
      "Epoch 9913/10000, Training Loss: 0.6503438949584961, Validation Loss: 0.6058104038238525\n",
      "Epoch 9914/10000, Training Loss: 0.6010698676109314, Validation Loss: 0.8589622378349304\n",
      "Epoch 9915/10000, Training Loss: 0.6272310614585876, Validation Loss: 0.5576342940330505\n",
      "Epoch 9916/10000, Training Loss: 0.6237147450447083, Validation Loss: 0.6728340983390808\n",
      "Epoch 9917/10000, Training Loss: 0.6358840465545654, Validation Loss: 0.6651584506034851\n",
      "Epoch 9918/10000, Training Loss: 0.609798789024353, Validation Loss: 0.8062719702720642\n",
      "Epoch 9919/10000, Training Loss: 0.6447533369064331, Validation Loss: 1.0223296880722046\n",
      "Epoch 9920/10000, Training Loss: 0.6714264750480652, Validation Loss: 0.6154350638389587\n",
      "Epoch 9921/10000, Training Loss: 0.6483598351478577, Validation Loss: 0.9573483467102051\n",
      "Epoch 9922/10000, Training Loss: 0.6135438680648804, Validation Loss: 0.9082805514335632\n",
      "Epoch 9923/10000, Training Loss: 0.6047711968421936, Validation Loss: 0.354236364364624\n",
      "Epoch 9924/10000, Training Loss: 0.5858348608016968, Validation Loss: 0.6415944695472717\n",
      "Epoch 9925/10000, Training Loss: 0.6262851357460022, Validation Loss: 0.6485050320625305\n",
      "Epoch 9926/10000, Training Loss: 0.6744813323020935, Validation Loss: 0.6004294753074646\n",
      "Epoch 9927/10000, Training Loss: 0.6338976621627808, Validation Loss: 0.7258090376853943\n",
      "Epoch 9928/10000, Training Loss: 0.6719040274620056, Validation Loss: 0.586905300617218\n",
      "Epoch 9929/10000, Training Loss: 0.5965728759765625, Validation Loss: 0.5961201190948486\n",
      "Epoch 9930/10000, Training Loss: 0.6500154733657837, Validation Loss: 0.421135812997818\n",
      "Epoch 9931/10000, Training Loss: 0.6076239347457886, Validation Loss: 0.7030054926872253\n",
      "Epoch 9932/10000, Training Loss: 0.6557563543319702, Validation Loss: 0.6025311946868896\n",
      "Epoch 9933/10000, Training Loss: 0.6715285778045654, Validation Loss: 0.560131847858429\n",
      "Epoch 9934/10000, Training Loss: 0.6477221250534058, Validation Loss: 0.6246318221092224\n",
      "Epoch 9935/10000, Training Loss: 0.6304035186767578, Validation Loss: 0.7214606404304504\n",
      "Epoch 9936/10000, Training Loss: 0.642432451248169, Validation Loss: 0.6304077506065369\n",
      "Epoch 9937/10000, Training Loss: 0.6577621102333069, Validation Loss: 0.7547638416290283\n",
      "Epoch 9938/10000, Training Loss: 0.6588087677955627, Validation Loss: 0.5896329283714294\n",
      "Epoch 9939/10000, Training Loss: 0.6670331358909607, Validation Loss: 0.814548671245575\n",
      "Epoch 9940/10000, Training Loss: 0.6395335793495178, Validation Loss: 0.7258322834968567\n",
      "Epoch 9941/10000, Training Loss: 0.645907998085022, Validation Loss: 0.653476893901825\n",
      "Epoch 9942/10000, Training Loss: 0.611092209815979, Validation Loss: 0.6536769866943359\n",
      "Epoch 9943/10000, Training Loss: 0.6507672667503357, Validation Loss: 0.6220780611038208\n",
      "Epoch 9944/10000, Training Loss: 0.62934809923172, Validation Loss: 0.4889952838420868\n",
      "Epoch 9945/10000, Training Loss: 0.6473659873008728, Validation Loss: 0.7999624609947205\n",
      "Epoch 9946/10000, Training Loss: 0.6791716814041138, Validation Loss: 0.723267138004303\n",
      "Epoch 9947/10000, Training Loss: 0.6467517614364624, Validation Loss: 0.6935338973999023\n",
      "Epoch 9948/10000, Training Loss: 0.6389920115470886, Validation Loss: 0.7978673577308655\n",
      "Epoch 9949/10000, Training Loss: 0.6215213537216187, Validation Loss: 0.6278239488601685\n",
      "Epoch 9950/10000, Training Loss: 0.585678219795227, Validation Loss: 0.7382488250732422\n",
      "Epoch 9951/10000, Training Loss: 0.661985456943512, Validation Loss: 0.5773059725761414\n",
      "Epoch 9952/10000, Training Loss: 0.6421379446983337, Validation Loss: 0.641162633895874\n",
      "Epoch 9953/10000, Training Loss: 0.5728186964988708, Validation Loss: 0.5453609824180603\n",
      "Epoch 9954/10000, Training Loss: 0.6859143972396851, Validation Loss: 0.7202842831611633\n",
      "Epoch 9955/10000, Training Loss: 0.6119961142539978, Validation Loss: 0.6726416945457458\n",
      "Epoch 9956/10000, Training Loss: 0.5947214961051941, Validation Loss: 0.7014791965484619\n",
      "Epoch 9957/10000, Training Loss: 0.6209919452667236, Validation Loss: 0.5551254749298096\n",
      "Epoch 9958/10000, Training Loss: 0.6478712558746338, Validation Loss: 0.5730583071708679\n",
      "Epoch 9959/10000, Training Loss: 0.6464643478393555, Validation Loss: 0.6577263474464417\n",
      "Epoch 9960/10000, Training Loss: 0.6562627553939819, Validation Loss: 0.6550206542015076\n",
      "Epoch 9961/10000, Training Loss: 0.6568562388420105, Validation Loss: 0.6275163292884827\n",
      "Epoch 9962/10000, Training Loss: 0.6797894239425659, Validation Loss: 0.763913631439209\n",
      "Epoch 9963/10000, Training Loss: 0.6441707015037537, Validation Loss: 0.7019416689872742\n",
      "Epoch 9964/10000, Training Loss: 0.6264194846153259, Validation Loss: 0.6879931092262268\n",
      "Epoch 9965/10000, Training Loss: 0.6524535417556763, Validation Loss: 0.6112629771232605\n",
      "Epoch 9966/10000, Training Loss: 0.6432403922080994, Validation Loss: 0.631536066532135\n",
      "Epoch 9967/10000, Training Loss: 0.6344889402389526, Validation Loss: 0.6749405264854431\n",
      "Epoch 9968/10000, Training Loss: 0.6285180449485779, Validation Loss: 0.7577471733093262\n",
      "Epoch 9969/10000, Training Loss: 0.6532005667686462, Validation Loss: 0.7961270809173584\n",
      "Epoch 9970/10000, Training Loss: 0.6664139032363892, Validation Loss: 0.7464929223060608\n",
      "Epoch 9971/10000, Training Loss: 0.620658278465271, Validation Loss: 0.6886257529258728\n",
      "Epoch 9972/10000, Training Loss: 0.6281524300575256, Validation Loss: 0.6484689116477966\n",
      "Epoch 9973/10000, Training Loss: 0.6231024861335754, Validation Loss: 0.6023781299591064\n",
      "Epoch 9974/10000, Training Loss: 0.6426674723625183, Validation Loss: 0.5884549021720886\n",
      "Epoch 9975/10000, Training Loss: 0.6619272828102112, Validation Loss: 0.7284278273582458\n",
      "Epoch 9976/10000, Training Loss: 0.6941495537757874, Validation Loss: 0.6593031883239746\n",
      "Epoch 9977/10000, Training Loss: 0.6069403290748596, Validation Loss: 0.9342047572135925\n",
      "Epoch 9978/10000, Training Loss: 0.6361077427864075, Validation Loss: 0.6296346783638\n",
      "Epoch 9979/10000, Training Loss: 0.6447754502296448, Validation Loss: 0.6436948776245117\n",
      "Epoch 9980/10000, Training Loss: 0.6721246242523193, Validation Loss: 0.58664470911026\n",
      "Epoch 9981/10000, Training Loss: 0.609750509262085, Validation Loss: 0.6257370710372925\n",
      "Epoch 9982/10000, Training Loss: 0.6228757500648499, Validation Loss: 0.8148801922798157\n",
      "Epoch 9983/10000, Training Loss: 0.5918277502059937, Validation Loss: 0.6861412525177002\n",
      "Epoch 9984/10000, Training Loss: 0.7036561369895935, Validation Loss: 0.6294583678245544\n",
      "Epoch 9985/10000, Training Loss: 0.6264902949333191, Validation Loss: 0.70516437292099\n",
      "Epoch 9986/10000, Training Loss: 0.604095995426178, Validation Loss: 0.7986038327217102\n",
      "Epoch 9987/10000, Training Loss: 0.6955806016921997, Validation Loss: 0.6308047771453857\n",
      "Epoch 9988/10000, Training Loss: 0.6189388036727905, Validation Loss: 0.7088160514831543\n",
      "Epoch 9989/10000, Training Loss: 0.6099837422370911, Validation Loss: 0.5154799818992615\n",
      "Epoch 9990/10000, Training Loss: 0.6824337244033813, Validation Loss: 0.653165876865387\n",
      "Epoch 9991/10000, Training Loss: 0.6849155426025391, Validation Loss: 1.6680513620376587\n",
      "Epoch 9992/10000, Training Loss: 0.647303581237793, Validation Loss: 0.7706223130226135\n",
      "Epoch 9993/10000, Training Loss: 0.6660255193710327, Validation Loss: 0.6790068745613098\n",
      "Epoch 9994/10000, Training Loss: 0.6487607955932617, Validation Loss: 0.70645672082901\n",
      "Epoch 9995/10000, Training Loss: 0.6383374929428101, Validation Loss: 0.8564576506614685\n",
      "Epoch 9996/10000, Training Loss: 0.6461299657821655, Validation Loss: 0.599721372127533\n",
      "Epoch 9997/10000, Training Loss: 0.5874320268630981, Validation Loss: 0.7585483193397522\n",
      "Epoch 9998/10000, Training Loss: 0.6048401594161987, Validation Loss: 0.7023329138755798\n",
      "Epoch 9999/10000, Training Loss: 0.6401999592781067, Validation Loss: 0.7820468544960022\n",
      "Epoch 10000/10000, Training Loss: 0.6246680021286011, Validation Loss: 0.6623102426528931\n",
      "Test Accuracy: 70.83%\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\") + \"_\" + 'GAT'\n",
    "writer = SummaryWriter(log_dir=f'./Graph/{timestr}')\n",
    "epochs = 10000\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 2. Encode nodes using GAT layers\n",
    "    z = model.encode(node_features, data.train_pos_edge_index)\n",
    "    \n",
    "    # 3. Positive and negative samples for training\n",
    "    pos_edge_index = data.train_pos_edge_index  # Positive edges from training set\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=data.train_pos_edge_index,\n",
    "        num_nodes=z.size(0),\n",
    "        num_neg_samples=pos_edge_index.size(1)\n",
    "    )  # Random negative samples\n",
    "    \n",
    "    # 4. Decode edges\n",
    "    pos_pred = model.decode(z, pos_edge_index)\n",
    "    neg_pred = model.decode(z, neg_edge_index)\n",
    "    \n",
    "    # 5. Create labels for positive and negative samples\n",
    "    pos_label = torch.ones(pos_pred.size(0), device=device)\n",
    "    neg_label = torch.zeros(neg_pred.size(0), device=device)\n",
    "    \n",
    "    # 6. Compute loss (binary cross-entropy)\n",
    "    loss = F.binary_cross_entropy_with_logits(torch.cat([pos_pred, neg_pred]), torch.cat([pos_label, neg_label]))\n",
    "    \n",
    "    writer.add_scalar('Loss/train', loss, epoch)\n",
    "    \n",
    "    # validation loss\n",
    "    val_pos_edge_index = data.val_pos_edge_index \n",
    "    val_neg_edge_index = negative_sampling(\n",
    "        edge_index=data.val_pos_edge_index,\n",
    "        num_nodes=z.size(0),\n",
    "        num_neg_samples=val_pos_edge_index.size(1)\n",
    "    )  # Random negative samples\n",
    "    \n",
    "    val_pos_pred = model.decode(z, val_pos_edge_index)\n",
    "    val_neg_pred = model.decode(z, val_neg_edge_index)\n",
    "    \n",
    "    val_pos_label = torch.ones(val_pos_pred.size(0), device=device)\n",
    "    val_neg_label = torch.zeros(val_neg_pred.size(0), device=device)\n",
    "    \n",
    "    val_loss = F.binary_cross_entropy_with_logits(torch.cat([val_pos_pred, val_neg_pred]), torch.cat([val_pos_label, val_neg_label]))\n",
    "    writer.add_scalar('Loss/val', val_loss, epoch)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {loss.item()}, Validation Loss: {val_loss.item()}\")\n",
    "\n",
    "# Evaluation (link prediction on the test set)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    z = model.encode(node_features, data.train_pos_edge_index)\n",
    "    pos_pred = model.decode(z, data.test_pos_edge_index)\n",
    "    neg_pred = model.decode(z, data.test_neg_edge_index)\n",
    "    \n",
    "    pos_label = torch.ones(pos_pred.size(0), device=device)\n",
    "    neg_label = torch.zeros(neg_pred.size(0), device=device)\n",
    "    \n",
    "    # Concatenate predictions and labels for evaluation\n",
    "    pred = torch.cat([pos_pred, neg_pred])\n",
    "    label = torch.cat([pos_label, neg_label])\n",
    "    \n",
    "    # Use a threshold (e.g., 0.5) on the sigmoid output to classify edges\n",
    "    pred_class = (torch.sigmoid(pred) > 0.5).float()\n",
    "    accuracy = (pred_class == label).sum().item() / label.size(0)\n",
    "    \n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
